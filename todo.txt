-- summary of learned best practices so far for creating matching point lists between 2 images:
    -- determining scale:
       (1) use k=2 segmentation and binning to < 300x300 to find the top 10 blobs in each image
           where top 10 is defined by being the longest closed contours.
           (note, may be able to take short cut to use area instead of the full curve and extraction)
       (2) find those same blobs in the same k=2 but for full image size.
           if blobs weren't found in (1), the top 10 are chosen from here.
           (a) for each possible blob pairing:
               compare feature descriptors, that is intensity (and perhaps gradient too).
               if the blobs look similar, use contour matcher to solve for the
               match.  the solution provides cost, scale, and matching points (points are
               inflection points from the contours).
       (3) when finished, may only have one or two solutions.
           from those, the low cost and longest curves are what to choose as solution.
           stdev and mean of weighted solutions and discarding outliers...
    -- find matching points between images using feature matches:
       (1) find corners and use their edge information to determine an orientation for the 
           descriptors.
           (NOTE that the descriptor region sizes in one of the images should be adjusted by scale
           if the scales are not the same.  One needs to consider the pixel size (resolution) also
           for best comparison of similar region sizes).
       (2) extract intensity, gradient (and maybe theta) desciptors around the corners
           and find best matches amond the features by comparing the SSD of descriptors
       (3) from the matches and knowing the scale, determine rotation and translation for each.
           use frequency of parameters to find the best rotation, then translation in X and Y.
           The points that went into the final transformation solutions are then the 
           correspondence list which can be used as input to the epipolar projection solver.

     Most of this as a sequence of operations is present in a temporary form in 
     BinSegmentationHelper but will be refactored soon.

     -- make the wrappers and tests for the above and add it to the cmd line invokers in the dist jar

-- finish the PointMatchers and remove unused experimental code everywhere.

-- make the method to combine 2 skyline masks to decide what is sky and what
   is land when there are differences.

-- move all of debugging code to aspects or test.  jfree should not need to be included
   in the command line jar unless add an output that uses it.

-- improve my canny edge detector w/ some new entries here for computing
    gradient using 6 neighbors
    and the suggested derivations of the 2-layer thresh
    and the line thinner
    https://en.wikipedia.org/wiki/Canny_edge_detector#Finding_the_Intensity_Gradient_of_the_Image

    note that the gradient changes might help with the normalized gradient right away

-- for more feature matching tests,
    see
    http://web.stanford.edu/~bgirod/pdfs/Chandrasekhar_CVPR2009.pdf
    "For evaluating the performance of low-bit-rate descrip- tors, we use two data sets provided by Winder and Brown [10], Trevi Fountain and Notre Dame. Each data set has matching pairs of 64×64 pixel patches. For algorithms that require training, we use matching pairs from the Trevi Fountain. For testing, we randomly select 10,000 matching"
    S. A. Winder and M. Brown, “Learning Local Image Descriptors,”
in Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE
Conference on, 2007, pp. 1–8. 1, 2

-- revisit areas where using CountingSort and compare 
     O(N_max_value) to O(N*lg2N) where the later is the number of points

-- write an epipolar projection test for brown & lowe

-- make plans for a stereoscopic matcher, image rectification, and disparity map maker

-- http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/electrical-engineering/communications-engineering/9780123965028/chapter-20dot-clustering/st010_html_17?query=((ransac))#snippet

-- for a random subset chooser, since
     the x only depends upon the subset size, not on n, the number to choose from,
     could think about a way to calculate the bitstring randomly.
     for k = 7, the lowest bitstring is 0111 1111 which is a decimal value of 127.
     n gives the largest number of bitstring positions with k at the highest value positions.
     To calculate it in this way probably needs to use BigInteger.
     It has some random methods in it, so one could probably work out how to draw
     a number randomly between those values.
     Then might find the next lowest number from that, that has k=7 '1' digits.
     For example, if the random number were 252, that is 1111 1100, the next lowest 
     bitstring with k=7 1's is 1111 1011.      (find lowest set bit, change it to 0 and set the 2 below it to 1)
     If that bitstring has already been selected randomly, then the SubsetChooser's
     Gosper's hack quickly returns the next larger k=7 bitstring.

     This should be compared to the way I'm currently selecting k=7 indexes randomly.
     currently:  not sure actually... I think I select 7 randomly with no repeating digits
     and then sort them and see if the combination has been selected before, and 
     repeat if they have.
        that mean 7 random operations * 7 * lg2(7) for sort operations = 7 random * 20 steps for single iteration.
     
        the random w/ biginteger is 1 or more random operations (see internal implementation)
        plus a find lowest bit operation, plus 3 set bits, plus gospers hack (a half dozen bit operations)
        so the result =  1 (or more internal to BigInteger) rand operation + a dozen or so bit operations.

        The advantage to the BigInteger method is that it is possibly faster, but the draw would be
        within highly uneven space (the universe drawn from is far larger than the number of possible
        permutations, and then one adjusts to find the next lowest matching number).
        The probability that a single number had the k=7 digits set can be estimated using
        the probability of drawing one unique set of all the possible subsets for k=7 and n (with one draw)
        so that's 1./((n!/(k!*(n-k)!)) and then would need to account for the dilution of
        that within a larger set of numbers (nbits with highest k bits set to '1' - kbits all set to '1').
        The adjustment to select the next lowest number with k bits set leads to an uneven
        distribution (not uniform), but that would not necessarily be a bad thing here.

    Following the line of reasoning I must have started with:
       Ideally, one would be randomly choosing between k bit positions whose maximum is n.
       That requires enumerating all combinations and choosing randomly from them.
       Can see a pattern easily within Gosper's hack already, so might be a clever way
       to store them or to recalculate with offsets or derive a new ordering
       from Gosper's Hack that would allow an O(1) access of a bitstring from a random number 
       chosen within the range of the number of possible permutations.
       This ideal implementation would be uniform random draws.

    For either, should be storing "selected" as a bitstring.
    
    For the current implementation, I am not checking for having selected the previous combination
    before because the chance of drawing the same combination should be pretty small.
       The probability of selecting the same 7 objects out of n objects is roughly calculated with
            the presence of unique set of k within n objects is 1./(n!/(k!*(n-k)!)) 
            but that is not considering 'k draws'.   Can't use Bernoulli principle because that
            gives an answer that is a number having any k digits rather than a uniquely ordered
            set of k digits.
            
    ** do a quick plot of the difference of k=7 among n=(some number such that nPermutations < 1<<63)
       drawn by 7 random draws from a max of n numbers
       versus the BigInteger random draw 

-- fix the contour matcher search to use a compare and binary search

-- when sun is found, such as in the NM image, might try to fit a
   radial profile (1/r^2) from it over the image.
   can wait until sky pixels are mostly found and then fit over
   that and extend that pattern to the rest of the image?
   that is, in the NM image, can see that the sky varies smoothly
   but there are clouds that are in the sky too. 
   ** what happens when one knows the sun is in the image,
   one fits a gradient to the least bright pixels and then divides
   the image by that gradient?  presumably, the skyline stands out
   even more.   cie XY are independent of illumination in any case,
   so this step probably isn't useful and not always possible...

-- include other atmospheric optically visible features:  sun dogs and moon dogs.
-- consider more images which are near evening or evening, but there is enough illumination
   to see a skyline.

-- consider simplest ways to find solar reflected light in the images:
     -- already have a method which finds visible sun's photosphere
     -- create a separate method for reflected from water (see earlier code... depending upon sun location,
        the light might be orange to white...)... can use a source function of the sun
        and mie and rayliegh scattering if more formal methods are needed and reflection off of optically
        thick H_2O.
     -- refracted and reflected from clouds that are thick enough to provide a surface of reflected light
        that has a pattern of brightness due to illumination by sun...
     -- consider whether the location of the sun when photosphere is not in image is learnable
        by gradient of brightness.

-- add more tests when have a working solution for skyline extraction
   -- test by resolution too.  the seed sky points probably have some resolution dependence at this point.
   -- find test images for:
         -- structured cloudy sky and smooth white foreground
         -- structured cloudy sky and smooth dark foreground
         Neither of those will be solved as well as the NM test image which had non-sky foreground colors.
         Need additional information to understand what is sky without external sensors
         or assumption of horizontal.  In ambiguous cases, might need to make the assumption of
         skyline being horizontal in the image and sky near the top of the image...would like to avoid
         that assumption if at all possible.

-- one day, put in my algorithms toolbox, deconvolution methods:
   weiner filter deconvolution
   Tikhonov regularization
   Richardson-Lucy

-- find image with naturally occuring skew for tests

-- for ways to reduce the corner list, consider reading:
   Shi, J., and C. Tomasi. "Good Features to Track." 
   Conference on Computer Vision and Pattern Recognition, 1994.

   journals to browse:

   CVGIP  Graphical Models /graphical Models and Image Processing /computer Vision, Graphics, and Image Processing
   CVIU  Computer Vision and Image Understanding
   IJCV  International Journal of Computer Vision
   IVC  Image and Vision Computing
   JMIV  Journal of Mathematical Imaging and Vision
   MVA  Machine Vision and Applications
   TMI - IEEE  Transactions on Medical Imaging

   http://pointclouds.org/documentation/

-- read:
    http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/illustration-and-graphics/9780133373721

-- revisit the matrix math, especially where there's multiplication.
   did I replace dot operation with multiplication anywhere?
   needs tests...

-- reconsider later some properties.
   hue for blue skies.
   eucl dist of color = sqrt((r-r)^2 + (g-g)^2 + (b-b)^2)
   
another test image?
   http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/dataset/images/color/260058.html

-- read more on segmentation... lots of comparisons on the benchmark site.
   note the skyline extractor here is only partial segmentation and specific to a task.

   http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf

-- test datasets
http://sipi.usc.edu/database/database.php?volume=misc
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation

-- finish the method converting from fundamental matrix to essential matrix.

-- consider following the implementation of disparity maps for stereo images
   and 3d modelling.  see notes in the docs directory.
   -- see http://vision.middlebury.edu/stereo/code/

-- consider implementing the LM-ICP from fitz-gibbons?
   did i look at this already and decide otherwise?
   does is handle projective transformations?

-- ** read: https://www.graphics.rwth-aachen.de/person/21/

-- for tests, make sure the projective point matcher can handle images like the
    ones in this tutorial:
    http://www.robots.ox.ac.uk/~az/tutorials/tutorialb.pdf

-- to the corner list makin',
   -- add a removal of redundant points

-- test for degenerate camera conditions:
   -- parallel camera motion w/o rotation 
-- test for degenerate scene structure configurations
   -- all points lying on a plane or nearly lying on a plane (?)
-- test for point sets containing noise
-- error estimate in fundamental matrix:
   6.5.3
   -- Gold Standard algorithm?  (need camera details...)
      summation over i( d(x_i, x_hat_i)^2 + d(x_prime_i, x_prime_hat_i)^2 ) 
   -- Sampson distance?

http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document3_hartley.pdf
-- a hartley paper suggests rectifying images to evaluate point transformations.

