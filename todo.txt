-- merton college test images matching on right side shows needs improvement.

   -- some blobs which largely are same area in other image
      have a different blob shape at a corner (due to segmentation).
      the current corner region orientation is calculated as perp to
      the perimeter, so different shape there will mean the 
      orientations are wrong.
         an example is
         image1: (385, 185), cr_orientation=158
         image2: (401, 203), cr_orientation=23

         a detailed look shows the matching pixel in image2 is 1 pixel
         off in x and y AND the blob perimeters are different there.

         in order to match those 2 pixels, one would need:
            -- a dither pattern of up to k pixels
            -- the calculation of orientation within 45 degree accuracy by 
               looking at the greyscale intensities in the surrounding 24 pixels.
               ** these need to be cached like features for re-use

               |3|3|2|1|1|
               |3|3|2|1|1|
               |4|4| |0|0|
               |5|5|6|7|7|
               |5|5|6|7|7|

            -- a small dither in rotation around the 45 degree angle

      Will try 2 new patterns of image wide matching and see if the fastest is usually successful:
        (1) use the exact CornerRegion coordinates and orientations to try to
            match a curve to a curve.  no dithers in pixel translation or rotation should be used.
            --> if this is more often successful than not, it can be improved
                on a point to point basis.  that is, if a critical number of
                corners are matched, the others can be recalculated using
                the 45 degree orientation resoltion and translation and rotaton dithers.
        (2) instead, for every corner, use the 45 deg orientation and the
            dither in translation and rotation.
            this dither pattern should be a new method in the matcher, because the
            wide range of rotation tries should not be necessary...the orientation range should
            be very small around the 45 degree estimate.
            the existing matching methods should be made obsolete.

-- checkerboard is still not matched correctly. (this may be due to same reason as merton)
   looks like the quantity of points is small for a blob and
   when an extra corner or 2 due to artifacts are present,
   that can lead to a false match with another blob in the
   other image with same additional artifact corners.
   -- need different matching  when a large number of features
      look very similar to one another?  or the number of corners per
      blob is less than some number?
      in that case, the corner matcher3 is possibly a better matcher.
-- consider making corner matcher 3 that sorts the corners and only
   keeps top 4 strongest corners, then feeds them to corner matcher 2.
   -- should be easy to add for testing then remove if not a good decision.
      the goal is to keep the accuracy but make faster matches.

** in ClosedCurveCornerMatcher2, revisit the heapKeyFactor to make
   sure keys will never overflow a long

** -- impl a circle hough transform

**-- use the added half dome test images to assert that matching same
     from other cameras is working

**-- make the feature matcher offset arrays improvements below

-- 
   -- consider comment below on cases where there are no matchable
      blobs in an image, such as test image for half dome.
      -- can treat all corners or corner regions as belonging to 
         the same blob and match that way, but only if can reduce 
         the number of points compared.
         -- portions of edges could be used with distinct patterns
            of concavity or convexity...
            if sillhouettes appear to be present in one image but
            not another, feature matching won't be possible but 
            contour matching would be.

-- more ransac and epipolar solver tests.
   -- when revisiting the books epipolar test, see if evaluation needs to focus
      on dist from epipolar line as much as the number of points within tolerance.
      --> make the combined score
   -- consider places in the code where an M-estimator could be helpful
   -- consider implementing a version of LMedS to use in place of RANSAC as a comparison
   update the open agile story and close it
-- update the command line runners.
-- change any debugging code to not run by default and change log levels where needed
-- finish implementing recursive gaussian and gaussian derivative filters

-- for cases in which there are no discernible blobs or the ones that are present
   might be a changing component (for example, see the test image of stonehenge in sillhouette)
   may need to use a corners only feature matching.
   (can adapt the code to perform same steps, but for a all corners being part of a fake repr blob).
   might need a way to limit the open curves being matched such as a look at convex and concave
   for any 3 or more points?  I thnk BL2003 chose large blocks of cells of pixels to compare between
   images.  edges and groups of corners should be faster...

** -- for the offset arrays in the feature matcher, could make these improvements:
    -- make the math.round changes to the feature offset arrays to speed up process 
       and add documentation that it should only be used to add to integers (not floating point numbers)
    -- for speeding up the transformations of feature descriptor offsets, consider 2 things:
          -- round the numbers into integers.
             knowing descriptor range0, a maximum and minumum possible value can
             be calculated: 
             -- min and max of (-6 and +6?)
                can store 6 in 3 bits.  
                can multiply by 2 and add one for negative numbers (max value=12+1=13 so 4 bits), 
                so an int, being 32 bits could store 8 such numbers.
                the 144 item array would then only be 18 items.
             -- in the case that all 360 degrees were tried, that would be 6480 items
                pre-computing all 360 degrees 
                    would take that much space plus 32 bits for an array reference.
                    = in array of size 6480 items
                    = 207392 bits = 25924 Bytes
                -- if wanted to use symmetry, could reduce that space by 4 but would need
                   to use logic for x *=-1 and/or y *=-1.
                   result is an array that would take 6481 Bytes

-- update aspects if needed and considering replacing them
    -- consider replacing aspectj with something like nanning aop

-- improve skyline extraction code

-- similarity that uses an exponential base?  can transform the numbers and use a distance
   transform to make the calculations faster

-- low priority, changes for KMPP...
   -- use a watershed at line 133
      and use the labeled components throughout code

-- explore a little the biorthogonal wavelet transform 
    in the JPEG-2000 image storage and compression standard

-- text recognition
   -- would like a class to extract and match text
      -- can see from the books test image that a binary segmentation would
         work best.
         -- mean adaptive thresholding might be a good follow up.
         -- blobs can be found as contiguous pixels of same value
         -- for the separated letters, should apply skeletonization possibly
         -- match against fonts.
            -- note font databases such as
               OCR-A, OCR-B, or MICR fonts
    some papers:
      http://www.rfai.li.univ-tours.fr/fr/ressources/_dh/DOC/DocOCR/OCRbinarisation.pdf

-- control flow graph for method called resolve() in DistanceTransform
   and same for resolveIterative to simplify the later

-- considering implementing a max-tree (Meister and Wilkinson)
   -- for a shape filter for blobs for example
       http://www.cs.rug.nl/michael/mt-pres.pdf

-- impl wavelets using "lifted wavelets".  see Sweldens 1996 and 1997

-- begin outline for rectification code

-- revisit kmeansplusplus...

-- make the method to combine 2 skyline masks to decide what is sky and what
   is land when there are differences.  This can happen when there are clouds
   or snow or varying occlusion, etc

-- move all of debugging code to aspects or test.  jfreegraph should not need to be included
   in the command line jar unless add an output that uses it.

-- improve my canny edge detector w/ some new entries here for computing
    gradient using 6 neighbors
    and the suggested derivations of the 2-layer thresh
    and the line thinner
    https://en.wikipedia.org/wiki/Canny_edge_detector#Finding_the_Intensity_Gradient_of_the_Image

    note that the gradient changes might help with the normalized gradient right away

-- for more feature matching tests,
    see
    http://web.stanford.edu/~bgirod/pdfs/Chandrasekhar_CVPR2009.pdf
    "For evaluating the performance of low-bit-rate descrip- tors, we use two data sets provided by Winder and Brown [10], Trevi Fountain and Notre Dame. Each data set has matching pairs of 64×64 pixel patches. For algorithms that require training, we use matching pairs from the Trevi Fountain. For testing, we randomly select 10,000 matching"
    S. A. Winder and M. Brown, “Learning Local Image Descriptors,”
in Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE
Conference on, 2007, pp. 1–8. 1, 2

-- revisit areas where using CountingSort and compare 
     O(N_max_value) to O(N*lg2N) where the later is the number of points

-- make plans for a stereoscopic matcher, image rectification, and disparity map maker

-- http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/electrical-engineering/communications-engineering/9780123965028/chapter-20dot-clustering/st010_html_17?query=((ransac))#snippet

-- for a random subset chooser, since
     the x only depends upon the subset size, not on n, the number to choose from,
     could think about a way to calculate the bitstring randomly.
     for k = 7, the lowest bitstring is 0111 1111 which is a decimal value of 127.
     n gives the largest number of bitstring positions with k at the highest value positions.
     To calculate it in this way probably needs to use BigInteger.
     It has some random methods in it, so one could probably work out how to draw
     a number randomly between those values.
     Then might find the next lowest number from that, that has k=7 '1' digits.
     For example, if the random number were 252, that is 1111 1100, the next lowest 
     bitstring with k=7 1's is 1111 1011.      (find lowest set bit, change it to 0 and set the 2 below it to 1)
     If that bitstring has already been selected randomly, then the SubsetChooser's
     Gosper's hack quickly returns the next larger k=7 bitstring.

     This should be compared to the way I'm currently selecting k=7 indexes randomly.
     currently:  not sure actually... I think I select 7 randomly with no repeating digits
     and then sort them and see if the combination has been selected before, and 
     repeat if they have.
        that mean 7 random operations * 7 * lg2(7) for sort operations = 7 random * 20 steps for single iteration.
     
        the random w/ biginteger is 1 or more random operations (see internal implementation)
        plus a find lowest bit operation, plus 3 set bits, plus gospers hack (a half dozen bit operations)
        so the result =  1 (or more internal to BigInteger) rand operation + a dozen or so bit operations.

        The advantage to the BigInteger method is that it is possibly faster, but the draw would be
        within highly uneven space (the universe drawn from is far larger than the number of possible
        permutations, and then one adjusts to find the next lowest matching number).
        The probability that a single number had the k=7 digits set can be estimated using
        the probability of drawing one unique set of all the possible subsets for k=7 and n (with one draw)
        so that's 1./((n!/(k!*(n-k)!)) and then would need to account for the dilution of
        that within a larger set of numbers (nbits with highest k bits set to '1' - kbits all set to '1').
        The adjustment to select the next lowest number with k bits set leads to an uneven
        distribution (not uniform), but that would not necessarily be a bad thing here.

    Following the line of reasoning I must have started with:
       Ideally, one would be randomly choosing between k bit positions whose maximum is n.
       That requires enumerating all combinations and choosing randomly from them.
       Can see a pattern easily within Gosper's hack already, so might be a clever way
       to store them or to recalculate with offsets or derive a new ordering
       from Gosper's Hack that would allow an O(1) access of a bitstring from a random number 
       chosen within the range of the number of possible permutations.
       This ideal implementation would be uniform random draws.

    For either, should be storing "selected" as a bitstring.
    
    For the current implementation, I am not checking for having selected the previous combination
    before because the chance of drawing the same combination should be pretty small.
       The probability of selecting the same 7 objects out of n objects is roughly calculated with
            the presence of unique set of k within n objects is 1./(n!/(k!*(n-k)!)) 
            but that is not considering 'k draws'.   Can't use Bernoulli principle because that
            gives an answer that is a number having any k digits rather than a uniquely ordered
            set of k digits.
            
    ** do a quick plot of the difference of k=7 among n=(some number such that nPermutations < 1<<63)
       drawn by 7 random draws from a max of n numbers
       versus the BigInteger random draw 

-- fix the contour matcher search to use a compare and binary search

-- when sun is found, such as in the NM image, might try to fit a
   radial profile (1/r^2) from it over the image.
   can wait until sky pixels are mostly found and then fit over
   that and extend that pattern to the rest of the image?
   that is, in the NM image, can see that the sky varies smoothly
   but there are clouds that are in the sky too. 
   ** what happens when one knows the sun is in the image,
   one fits a gradient to the least bright pixels and then divides
   the image by that gradient?  presumably, the skyline stands out
   even more.   cie XY are independent of illumination in any case,
   so this step probably isn't useful and not always possible...

-- include other atmospheric optically visible features:  sun dogs and moon dogs.
-- consider more images which are near evening or evening, but there is enough illumination
   to see a skyline.

-- consider simplest ways to find solar reflected light in the images:
     -- already have a method which finds visible sun's photosphere
     -- create a separate method for reflected from water (see earlier code... depending upon sun location,
        the light might be orange to white...)... can use a source function of the sun
        and mie and rayliegh scattering if more formal methods are needed and reflection off of optically
        thick H_2O.
     -- refracted and reflected from clouds that are thick enough to provide a surface of reflected light
        that has a pattern of brightness due to illumination by sun...
     -- consider whether the location of the sun when photosphere is not in image is learnable
        by gradient of brightness.

-- add more tests when have a working solution for skyline extraction
   -- test by resolution too.  the seed sky points probably have some resolution dependence at this point.
   -- find test images for:
         -- structured cloudy sky and smooth white foreground
         -- structured cloudy sky and smooth dark foreground
         Neither of those will be solved as well as the NM test image which had non-sky foreground colors.
         Need additional information to understand what is sky without external sensors
         or assumption of horizontal.  In ambiguous cases, might need to make the assumption of
         skyline being horizontal in the image and sky near the top of the image...would like to avoid
         that assumption if at all possible.

-- one day, put in my algorithms toolbox, deconvolution methods:
   weiner filter deconvolution
   Tikhonov regularization
   Richardson-Lucy

-- find image with naturally occuring skew for tests

-- for ways to reduce the corner list, consider reading:
   Shi, J., and C. Tomasi. "Good Features to Track." 
   Conference on Computer Vision and Pattern Recognition, 1994.

   journals to browse:

   CVGIP  Graphical Models /graphical Models and Image Processing /computer Vision, Graphics, and Image Processing
   CVIU  Computer Vision and Image Understanding
   IJCV  International Journal of Computer Vision
   IVC  Image and Vision Computing
   JMIV  Journal of Mathematical Imaging and Vision
   MVA  Machine Vision and Applications
   TMI - IEEE  Transactions on Medical Imaging

   http://pointclouds.org/documentation/

-- read:
    http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/illustration-and-graphics/9780133373721

-- revisit the matrix math, especially where there's multiplication.
   did I replace dot operation with multiplication anywhere?
   needs tests...

-- reconsider later some properties.
   hue for blue skies.
   eucl dist of color = sqrt((r-r)^2 + (g-g)^2 + (b-b)^2)
   
another test image?
   http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/dataset/images/color/260058.html

-- read more on segmentation... lots of comparisons on the benchmark site.
   note the skyline extractor here is only partial segmentation and specific to a task.

   http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf

-- test datasets
http://sipi.usc.edu/database/database.php?volume=misc
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation

-- finish the method converting from fundamental matrix to essential matrix.

 /*
    https://github.com/jesolem/PCV
    adapted from code licensed under  BSD license (2-clause "Simplified BSD License").
    private SimpleMatrix calculateCameraMatrixFromFundamentalMatrix(SimpleMatrix fm) {
        SimpleMatrix u = fm.svd().getU();
        SimpleMatrix leftE = u.extractVector(true, 2);
        leftE = leftE.divide(u.get(2, 2));

        double[][] skewSymmetric = new double[3][];
        skewSymmetric[0] = new double[]{0, -leftE.get(0, 2), leftE.get(0, 1)};
        skewSymmetric[1] = new double[]{leftE.get(0, 2), 0, -leftE.get(0, 0)};
        skewSymmetric[2] = new double[]{-leftE.get(0, 1), leftE.get(0, 0), 0};

        SimpleMatrix sMatrix = new SimpleMatrix(skewSymmetric);
        double v = sMatrix.dot( fm.transpose() );


        //essential matrix from F: E = transpose(K2)*F*K1
        SimpleMatrix k = DataForTests.getVenturiCameraIntrinsics()
            .mult(DataForTests.getVenturiRotationMatrixForImage001());
        SimpleMatrix k2 = DataForTests.getVenturiCameraIntrinsics()
            .mult(DataForTests.getVenturiRotationMatrixForImage010());

        SimpleMatrix essentialMatrix = k2.transpose().mult(fm).mult(k);

        int z = 1;
    }*/


-- consider following the implementation of disparity maps for stereo images
   and 3d modelling.  see notes in the docs directory.
   -- see http://vision.middlebury.edu/stereo/code/

-- consider implementing the LM-ICP from fitz-gibbons?
   did i look at this already and decide otherwise?
   does is handle projective transformations?

-- ** read: https://www.graphics.rwth-aachen.de/person/21/

-- for tests, make sure the projective point matcher can handle images like the
    ones in this tutorial:
    http://www.robots.ox.ac.uk/~az/tutorials/tutorialb.pdf

-- to the corner list makin',
   -- add a removal of redundant points

-- test for degenerate camera conditions:
   -- parallel camera motion w/o rotation 
-- test for degenerate scene structure configurations
   -- all points lying on a plane or nearly lying on a plane (?)
-- test for point sets containing noise
-- error estimate in fundamental matrix:
   6.5.3
   -- Gold Standard algorithm?  (need camera details...)
      summation over i( d(x_i, x_hat_i)^2 + d(x_prime_i, x_prime_hat_i)^2 ) 
   -- Sampson distance?

http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document3_hartley.pdf
-- a hartley paper suggests rectifying images to evaluate point transformations.

