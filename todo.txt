-- to speed up the feature matching:

   (0) reduce transcendental functions:
       for the transformations in IntensityFeatures.extractGsIntensityForCells,
       should be able to use hard wired templates of offsets.

       can encode and compress the templates so that extracting values is
       a fast bit shift operation.  should only need to store one
       quadrant and apply reflection around the axes.

   (1) possibly make the descriptor cell coarser (for fewer comparisons, 
       but same spatial extent).
       -- edit the cell block method in intensity features to make it easier
          to change the binning in the descriptor creator
          in IntensityFeatures.extractGsIntensityForCells

       -- then log the answers for a blob like [0] in image 1 of B&L2003.
          (logging the corner's best found center and rotation).
          -- then change the cell binning to much more coarse but same total
             width and log the same region of test.
             --> does the center or rotation get close to finer descriptor's results?

      feature matching:
         4x4 dither * 10 rotations = 160 for a corner region.  
         36 descriptor points for each of the 160 states = 5760 comparisons per corner region.
         up to 20 x 20 blobs to compare, each having >=3 corner regions.
         changes in (1) would reduce the 36 to a smaller value like 9 or 4 if possible, 
         though testing before suggested that the 36 produced the most reliable results.

-- low priority: improve segmentation for scale and common object finding.
   kmpp is best all around, but sometimes results in slightly different results.
      for regions that are largely dark and bright only, such as BL2003, the kmpp works best
      but could possibly be replaced with a polar ciexy that does similar knowing image is
      mostly white and dark grey..

-- consider whether as part of very close corners, such as the result of aliasing 
   at a junction, should average them to make a combined corner and remove the others
   -- the smaller number of corners reduces the runtime

-- update aspects if needed and considering replacing them
    -- consider replacing aspectj with something like nanning aop

-- improve skyline extraction code

-- similarity that uses an exponential base?

-- text recognition
   -- would like a class to extract and match text
      -- can see from the books test image that a binary segmentation would
         work best.
         -- mean adaptive thresholding might be a good follow up.
         -- blobs can be found as contiguous pixels of same value
         -- for the separated letters, should apply skeletonization possibly
         -- match against fonts.
            -- note font databases such as
               OCR-A, OCR-B, or MICR fonts
    some papers:
      http://www.rfai.li.univ-tours.fr/fr/ressources/_dh/DOC/DocOCR/OCRbinarisation.pdf

-- control flow graph for method called resolve() in DistanceTransform
   and same for resolveIterative to simplify the later

-- considering implementing a max-tree (Meister and Wilkinson)
   -- for a shape filter for blobs for example
       http://www.cs.rug.nl/michael/mt-pres.pdf

-- impl wavelets using "lifted wavelets".  see Sweldens 1996 and 1997

-- update the command line runners.
-- begin outline for rectification code

-- finish the PointMatchers and remove unused experimental code everywhere.
-- revisit kmeansplusplus...

-- make the method to combine 2 skyline masks to decide what is sky and what
   is land when there are differences.  This can happen when there are clouds
   are snow or varying occlusion, etc

-- move all of debugging code to aspects or test.  jfree should not need to be included
   in the command line jar unless add an output that uses it.

-- improve my canny edge detector w/ some new entries here for computing
    gradient using 6 neighbors
    and the suggested derivations of the 2-layer thresh
    and the line thinner
    https://en.wikipedia.org/wiki/Canny_edge_detector#Finding_the_Intensity_Gradient_of_the_Image

    note that the gradient changes might help with the normalized gradient right away

-- for more feature matching tests,
    see
    http://web.stanford.edu/~bgirod/pdfs/Chandrasekhar_CVPR2009.pdf
    "For evaluating the performance of low-bit-rate descrip- tors, we use two data sets provided by Winder and Brown [10], Trevi Fountain and Notre Dame. Each data set has matching pairs of 64×64 pixel patches. For algorithms that require training, we use matching pairs from the Trevi Fountain. For testing, we randomly select 10,000 matching"
    S. A. Winder and M. Brown, “Learning Local Image Descriptors,”
in Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE
Conference on, 2007, pp. 1–8. 1, 2

-- revisit areas where using CountingSort and compare 
     O(N_max_value) to O(N*lg2N) where the later is the number of points

-- write an epipolar projection test for brown & lowe

-- make plans for a stereoscopic matcher, image rectification, and disparity map maker

-- http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/electrical-engineering/communications-engineering/9780123965028/chapter-20dot-clustering/st010_html_17?query=((ransac))#snippet

-- for a random subset chooser, since
     the x only depends upon the subset size, not on n, the number to choose from,
     could think about a way to calculate the bitstring randomly.
     for k = 7, the lowest bitstring is 0111 1111 which is a decimal value of 127.
     n gives the largest number of bitstring positions with k at the highest value positions.
     To calculate it in this way probably needs to use BigInteger.
     It has some random methods in it, so one could probably work out how to draw
     a number randomly between those values.
     Then might find the next lowest number from that, that has k=7 '1' digits.
     For example, if the random number were 252, that is 1111 1100, the next lowest 
     bitstring with k=7 1's is 1111 1011.      (find lowest set bit, change it to 0 and set the 2 below it to 1)
     If that bitstring has already been selected randomly, then the SubsetChooser's
     Gosper's hack quickly returns the next larger k=7 bitstring.

     This should be compared to the way I'm currently selecting k=7 indexes randomly.
     currently:  not sure actually... I think I select 7 randomly with no repeating digits
     and then sort them and see if the combination has been selected before, and 
     repeat if they have.
        that mean 7 random operations * 7 * lg2(7) for sort operations = 7 random * 20 steps for single iteration.
     
        the random w/ biginteger is 1 or more random operations (see internal implementation)
        plus a find lowest bit operation, plus 3 set bits, plus gospers hack (a half dozen bit operations)
        so the result =  1 (or more internal to BigInteger) rand operation + a dozen or so bit operations.

        The advantage to the BigInteger method is that it is possibly faster, but the draw would be
        within highly uneven space (the universe drawn from is far larger than the number of possible
        permutations, and then one adjusts to find the next lowest matching number).
        The probability that a single number had the k=7 digits set can be estimated using
        the probability of drawing one unique set of all the possible subsets for k=7 and n (with one draw)
        so that's 1./((n!/(k!*(n-k)!)) and then would need to account for the dilution of
        that within a larger set of numbers (nbits with highest k bits set to '1' - kbits all set to '1').
        The adjustment to select the next lowest number with k bits set leads to an uneven
        distribution (not uniform), but that would not necessarily be a bad thing here.

    Following the line of reasoning I must have started with:
       Ideally, one would be randomly choosing between k bit positions whose maximum is n.
       That requires enumerating all combinations and choosing randomly from them.
       Can see a pattern easily within Gosper's hack already, so might be a clever way
       to store them or to recalculate with offsets or derive a new ordering
       from Gosper's Hack that would allow an O(1) access of a bitstring from a random number 
       chosen within the range of the number of possible permutations.
       This ideal implementation would be uniform random draws.

    For either, should be storing "selected" as a bitstring.
    
    For the current implementation, I am not checking for having selected the previous combination
    before because the chance of drawing the same combination should be pretty small.
       The probability of selecting the same 7 objects out of n objects is roughly calculated with
            the presence of unique set of k within n objects is 1./(n!/(k!*(n-k)!)) 
            but that is not considering 'k draws'.   Can't use Bernoulli principle because that
            gives an answer that is a number having any k digits rather than a uniquely ordered
            set of k digits.
            
    ** do a quick plot of the difference of k=7 among n=(some number such that nPermutations < 1<<63)
       drawn by 7 random draws from a max of n numbers
       versus the BigInteger random draw 

-- fix the contour matcher search to use a compare and binary search

-- when sun is found, such as in the NM image, might try to fit a
   radial profile (1/r^2) from it over the image.
   can wait until sky pixels are mostly found and then fit over
   that and extend that pattern to the rest of the image?
   that is, in the NM image, can see that the sky varies smoothly
   but there are clouds that are in the sky too. 
   ** what happens when one knows the sun is in the image,
   one fits a gradient to the least bright pixels and then divides
   the image by that gradient?  presumably, the skyline stands out
   even more.   cie XY are independent of illumination in any case,
   so this step probably isn't useful and not always possible...

-- include other atmospheric optically visible features:  sun dogs and moon dogs.
-- consider more images which are near evening or evening, but there is enough illumination
   to see a skyline.

-- consider simplest ways to find solar reflected light in the images:
     -- already have a method which finds visible sun's photosphere
     -- create a separate method for reflected from water (see earlier code... depending upon sun location,
        the light might be orange to white...)... can use a source function of the sun
        and mie and rayliegh scattering if more formal methods are needed and reflection off of optically
        thick H_2O.
     -- refracted and reflected from clouds that are thick enough to provide a surface of reflected light
        that has a pattern of brightness due to illumination by sun...
     -- consider whether the location of the sun when photosphere is not in image is learnable
        by gradient of brightness.

-- add more tests when have a working solution for skyline extraction
   -- test by resolution too.  the seed sky points probably have some resolution dependence at this point.
   -- find test images for:
         -- structured cloudy sky and smooth white foreground
         -- structured cloudy sky and smooth dark foreground
         Neither of those will be solved as well as the NM test image which had non-sky foreground colors.
         Need additional information to understand what is sky without external sensors
         or assumption of horizontal.  In ambiguous cases, might need to make the assumption of
         skyline being horizontal in the image and sky near the top of the image...would like to avoid
         that assumption if at all possible.

-- one day, put in my algorithms toolbox, deconvolution methods:
   weiner filter deconvolution
   Tikhonov regularization
   Richardson-Lucy

-- find image with naturally occuring skew for tests

-- for ways to reduce the corner list, consider reading:
   Shi, J., and C. Tomasi. "Good Features to Track." 
   Conference on Computer Vision and Pattern Recognition, 1994.

   journals to browse:

   CVGIP  Graphical Models /graphical Models and Image Processing /computer Vision, Graphics, and Image Processing
   CVIU  Computer Vision and Image Understanding
   IJCV  International Journal of Computer Vision
   IVC  Image and Vision Computing
   JMIV  Journal of Mathematical Imaging and Vision
   MVA  Machine Vision and Applications
   TMI - IEEE  Transactions on Medical Imaging

   http://pointclouds.org/documentation/

-- read:
    http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/illustration-and-graphics/9780133373721

-- revisit the matrix math, especially where there's multiplication.
   did I replace dot operation with multiplication anywhere?
   needs tests...

-- reconsider later some properties.
   hue for blue skies.
   eucl dist of color = sqrt((r-r)^2 + (g-g)^2 + (b-b)^2)
   
another test image?
   http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/dataset/images/color/260058.html

-- read more on segmentation... lots of comparisons on the benchmark site.
   note the skyline extractor here is only partial segmentation and specific to a task.

   http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf

-- test datasets
http://sipi.usc.edu/database/database.php?volume=misc
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation

-- finish the method converting from fundamental matrix to essential matrix.

-- consider following the implementation of disparity maps for stereo images
   and 3d modelling.  see notes in the docs directory.
   -- see http://vision.middlebury.edu/stereo/code/

-- consider implementing the LM-ICP from fitz-gibbons?
   did i look at this already and decide otherwise?
   does is handle projective transformations?

-- ** read: https://www.graphics.rwth-aachen.de/person/21/

-- for tests, make sure the projective point matcher can handle images like the
    ones in this tutorial:
    http://www.robots.ox.ac.uk/~az/tutorials/tutorialb.pdf

-- to the corner list makin',
   -- add a removal of redundant points

-- test for degenerate camera conditions:
   -- parallel camera motion w/o rotation 
-- test for degenerate scene structure configurations
   -- all points lying on a plane or nearly lying on a plane (?)
-- test for point sets containing noise
-- error estimate in fundamental matrix:
   6.5.3
   -- Gold Standard algorithm?  (need camera details...)
      summation over i( d(x_i, x_hat_i)^2 + d(x_prime_i, x_prime_hat_i)^2 ) 
   -- Sampson distance?

http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document3_hartley.pdf
-- a hartley paper suggests rectifying images to evaluate point transformations.

