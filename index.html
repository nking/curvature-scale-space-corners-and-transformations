<!DOCTYPE html>
<html lang="en-us">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-11146214-6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-11146214-6');
    </script>

    <meta charset="UTF-8">
    <title>Curvature-scale-space-corners-and-transformations by nking</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <meta name="keywords" content="software, computer vision, Climb With Your Feet" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" />
    <meta name="description" content="Computer Vision software that started with curvature scale space" />

  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Curvature-scale-space-corners-and-transformations</h1>
      <h2 class="project-tagline">Machine Learning and Computer Vision Programs</h2>
      <a href="https://github.com/nking/curvature-scale-space-corners-and-transformations" class="btn">View on GitHub</a>
      <a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>This holds a few summary notes and snapshots.</p>

<p>Object Detection and Machine Learning Notebooks:</p>
        <ul>
            <li> Object detection with the
                <a href="https://docs.pytorch.org/vision/main/models/faster_rcnn.html">(Faster RCNN)</a>
                two-stage object detection algorithm. The backbone is a ResNet 50 CNN w/ 
                a Feature Pyramid Network pretrained on the COCO dataset.
                A transfer-learning technique called fine-tuning is used to replace the last layer,
                that is, the classification layer to adapt the model to the small, specific dataset
                of android statue images.  The fine-tuned model is then used on unseen video frames
                from a YouTube video.
                <table>
                    <tr>
                        <td>
                            <a href="https://raw.githubusercontent.com/nking/curvature-scale-space-corners-and-transformations/master/src/FineTunePretrainedObjectDetector2.ipynb">The jupyter notebook.</a>
                        </td>
                        <td>
                            <a href="images/android_statues_obj_det_fasterrcnn_01.png">
                                <img alt="snapshots of object detection with android lawn statues"
                                     src="images/android_statues_obj_det_fasterrcnn_01.png"
                                     width="30" height="30"/>
                            </a>
                        </td>
                        <td>
                            <a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/raw/master/doc/statues_transfer_learning_object_detection.pdf">Summary snapshots and statistics.</a>
                        </td>
                    </tr>
                 </table>
                            <button onclick="x=document.getElementById('concl'); if (window.getComputedStyle(x).display === 'none') {x.style.display='block';} else {x.style.display='none'}">Conclusion</button>
                            <div id="concl" style="display:none">
                            <ul>
                            <li>add more training data
                            <li>try other pre-trained models
                            <li>try other variants of transfer-learning with object detection models.
                            <li>for the test frames from the video, could consider applying HMM inference to the best scoring identifications for “smoothing” inference where missing data. can use “filtering” queries for the next observations… can use particle filtering for similar reasons.  can build a better training set by adding the inferred missing identifications (bounding boxes and new ground truth labels) to the existing training dataset and then get a new test dataset.
                            </ul>
                            </div>
            </li>
            <!-- add the multiclass classification notes
            <li>
            </li>
            -->
            <!-- add the transformer notes
            <li>
            </li>
            -->
        </ul>

<p>Highlights of computer vision algorithms (without machine learning):</p>
    <ul>
        <li> Finding an object in a segmented image by shape alone. 
        PartialShapeMatcher can find a matching articulated, occluded object in an
        image and MultiPartialShapeMatcher can find the same object among a 
        database of candidate objects.
        The quality of the closed curve contours matters.  Here are shapshots from
        using MultiPartialShapeMatcher on a database of contours constructed using
        Segment Anything Model-2 (SAM-2) and using the gingerbread man as a query object.
        Also shown are the results when the contours are formed using a quick custom
        segmentation method.
            <table>
            <tr>
            <td>
            <a href="images/multipartialshapematcher_with_sam2_seg.png">
            <img alt="snapshots of shape matching results for gingerbread man query on SAM2 contours"
                src="images/multipartialshapematcher_with_sam2_seg.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            <tr>
            <td>
            <a href="images/multipartialshapematcher_with_quick_seg.png">
            <img alt="snapshots of shape matching results for gingerbread man query on quick segmentatin contours"
                src="images/multipartialshapematcher_with_quick_seg.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            </table>
        </li>
        <li>
             A correspondence list can be made quickly with 
             keypoints (e.g. Harris corners, inflection points, peaks in aTrous wavelets) and
             the rotationally invariant ORB descriptors.  RANSAC is also used to further remove outliers
             through epipolar fits.
            <table>
            <tr>
            <td>
            <a href="images/image_correspondence.png">
            <img alt="snapshots of image correspondence"
                src="images/image_correspondence.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            </table>
        </li>
        <li>
             An improved Canny Edges Filter can be made for color images by combining the greyscale
             Canny edges with the same made with the "C" image of the LCH colorspace.
            <table>
            <tr>
            <td>
            <a href="images/canny_color_edges.png">
            <img alt="Canny edges using color"
                src="images/canny_color_edges.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            </table>
        </li>
        <li>
            MSER (Maximally Stable Extremal Regions) are a very fast algorithm that can be used
            where level sets are needed.  Using MSER on greyscale and on the "H" image of
            LCH color space can find potential objects.  
            I combined the Canny color edges with the boundaries of MSER regions to create
            edges that are complete contours (== segmentation).
            The current version has some color filter rules that are potentially fragile and
            the use of MSER is resolution dependent so a wrapper class is provided.
            The results look promising.
            <table>
            <tr>
            <td>
            <a href="images/seattle_edges.png">
            <img alt="MSEREdges run on image of Seattle"
                src="images/seattle_edges.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/andr_01_edges.png">
            <img alt="MSEREdges run on image of android statues"
                src="images/andr_01_edges.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/andr_03_edges.png">
            <img alt="MSEREdges run on image of android statues"
                src="images/andr_03_edges.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/checkerboard_edges.png">
            <img alt="MSEREdges run on image of checkerboard"
                src="images/checkerboard_edges.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            </table>
        </li>
        <li>
            Finding an object in another image in which it has changed location, lighting,
            and pose is not easy, but can be done with MSER and HOGs.
            The current version has wrapper classes which help pre-prepare the images.
            <table>
            <tr>
            <td>
            <a href="images/gingerbreadman_corres.png">
            <img alt="image of android statues showing gingerbreadman match"
                src="images/gingerbreadman_corres.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/icecream_corres.png">
            <img alt="image of android statues showing icecream match"
                src="images/icecream_corres.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/cupcake_corres.png">
            <img alt="image of android statues showing cupcake match"
                src="images/cupcake_corres.png"
                width="30" height="30"/>
            </a>
            </td>
            <td>
            <a href="images/honeycomb_corres.png">
            <img alt="image of android statues showing honeycomb match"
                src="images/honeycomb_corres.png"
                width="30" height="30"/>
            </a>
            </td>
            </tr>
            </table>
        </li>
    </ul>

<p>Some of the notes while implementing comp vis w/o machine learning:</p>

<p><a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/raw/master/doc/colorSegmentation3.pdf">segmentation and edges</a></p>

<p><a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/raw/master/doc/contours.pdf">misc scale space contour notes</a></p>

<p><a href="https://github.com/nking/curvature-scale-space-corners-and-transformations/raw/master/doc/projection.pdf">projection</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/nking/curvature-scale-space-corners-and-transformations">Curvature-scale-space-corners-and-transformations</a> is maintained by <a href="https://github.com/nking">nking</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
