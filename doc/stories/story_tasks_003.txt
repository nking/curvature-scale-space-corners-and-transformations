========================================================================
Story: Determine scale w/ blob perimeters and corners
 
    summary: The current implementation of scale calculation uses scale 
      space image contours of the blob perimeters to find matching 
      points and hence scale, but it is vulnerable to differences in the 
      contours between images, for example, due to a large bump or 
      occulusion.  An alternate method is needed to use the same blob 
      perimeters, but corners instead of inflection points and contours.  
      Both methods should be available for use after the changes, but 
      the new algorithm will be the default.
      (see the end of docs/contours.pdf)

    time estimate: 
        minimum: 3 days
        maximum: 2 weeks

    amount of time used: 

    state of completion: in design phase

    comments regarding state:

    comments:
        Refactoring of code associated with the current calculations is
        necessary before the new algorithm to assert that the 
        abstraction is correct.
        Much of the logic needed is already known from the experience
        of implementing the scale calculations with contour matching
        so the more detailed specification for Task 3 and Task 4 
        should not take long before implementation.

---------------------------------------------------------------------
Task 1:
    goal: Refactor the current BlobScaleFinder and associated classes
       to make the current algorithm available as an alternative choice,
       but the default will be the current algorithm when implemented.
      
    details:  

       BlobScaleFinder:
           should be abstracted to IBlobScaleFinder
               public TransformationParameters solveForScale(
                   ISegmentedImageHelper img1Helper, SegmentationType type1,
                   boolean useBinned1,
                   ISegmentedImageHelper img2Helper, SegmentationType type2,
                   boolean useBinned2,
                   float[] outputScaleRotTransXYStDev)
               public void setToDebug()
           with concrete implmentations:
               BlobContoursScaleFinder
                  is given all content from BlobScaleFinder
               BlobCornersScaleFinder
       
       SegmentedImageHelper:
          should be abstracted to an interface ISegmentedImageHelper, and AbstractSegmentedImageHelper
              
          ISegmentedImageHelper:
              public void applySegmentation(SegmentationType type, boolean applyToBinnedImage) 
                  throws IOException, NoSuchAlgorithmException;
              public void generatePerimeterPointsOfInterest(
                  SegmentationType type, boolean applyToBinnedImage)

          AbstractSegmentedImageHelper:
              should contain all except contour methods and members from existing SegmentedImageHelper

          SegmentedImageBlobContourHelper
          SegmentedImageBlobCornerHelper

       BlobScaleFinderWrapper:
          should be changed to use contours or corners.
          if choice is contours, SegmentedImageBlobContourHelper is constructed and BlobContoursScaleFinder is
              used, else the corner implementations.

       Adapt tests to the changes.

    time estimate: < half day
  
    amount of time used: 

    state of completion:

    comments:
       Later, could consider to implement the service provider interface for contour inflection
       points or corners as choices... It's not the goal of the overall project currently to 
           always provide a choice.
       see $JDK_HOME/docs/technotes/guides/jar/jar.html#Service_Provider 
              
---------------------------------------------------------------------
Task 2:
    goal: Need ability to make corners given just a curve rather than  
        an image.

    details:  
       The existing class CurvatureScaleSpaceCornerDetector should
           move the corner making portion to a separate class.
           The content that needs to be encapsulated is all
           within method 
               Map<PairIntArray, Map<SIGMA, ScaleSpaceCurve> > maps =
                   findCornersInScaleSpaceMaps(edges, useOutdoorMode, corners);

           changes in method:
              member variables need to be passed in as arguments:
                  enableJaggedLineCorrections
                  doStoreCornerRegions
                  edgeCornerRegionMap
                  factorIncreaseForCurvatureMinimum

           new class for moved method and member variable settings: 
               CSSCornerMaker

        Adapt tests to the changes.

    time estimate:  minimum couple of hours, maximum 1 day.
  
    amount of time used: 

    state of completion:

    comments:
              
---------------------------------------------------------------------
Task 3:
    goal: Implement the content for classes for corners specified in 
        Task 1 and add new tests for them, excepting the corner
        matching algorithm which has its own task 4.

    details:  
        Implement:
           BlobCornersScaleFinder
               public TransformationParameters solveForScale(
                   ISegmentedImageHelper img1Helper, SegmentationType type1,
                   boolean useBinned1,
                   ISegmentedImageHelper img2Helper, SegmentationType type2,
                   boolean useBinned2,
                   float[] outputScaleRotTransXYStDev)

           SegmentedImageBlobCornerHelper
               public void generatePerimeterPointsOfInterest(
                   SegmentationType type, boolean applyToBinnedImage)

       BlobScaleFinderWrapper:
          should be changed to use contours by default

       ClosedCurveCornerMatcher:    
       Implement combining the blob perimeter corner matching results
        to find the best points that should be used for the euclidean
        transformation calculation.

           (1) compare each from list of blob perimeters of image 1 
               to those in image 2
               -- if the number of corners in current curve1 is very different
                  in the current curve2, log them and discard, do not try to match
               -- use ClosedCurveCornerMatcher to match the points in curve1 w/ curve2.
                  -- the results should be comparable to all other for the same
                     curve1 to find the best 2 solutions for curve1
           (2) with a map of the top 2 best solutions for each curve1, find the
               best consistent answers among the solutions.

               with the contours, was able to adjust all costs to same
               reference frame (which was the largest sigm from contour peaks). 
               Here, the cost is now distance from predicted points so should 
               be in the same frame already to compare against other curve results.

               Then the contour algorithm uses bipartite matching between the 
               curve1, curve2 pairings for the highest number of matchings for lowest cost.
           
               Then the algorithm removes solutions that have rotation very different
               from the best solution(s).

               Similar solutions to the best solution are then gathered to make the
               largest set of matched points.
               Those points are then given to the class which pairs the points and 
               solves for euclidean transformation.

        Tests are needed too.  First can use the real data while tuning the method.

    time estimate: minimum 1 day, maximum 1 week, expecting ~2 days w/ minimum testing
                   but another may be needed for changes coincident with matching algorithm.
  
    amount of time used: 

    state of completion:

    comments:
        When have test images that have textures of different size, may need to provide
        a way to recognize that and process the results from each curve's
        ClosedCurveCornerMatcher use for that specific (rare?) use case.
          A new task should be made for that in the future.
              
---------------------------------------------------------------------
Task 4:
    goal: Implement the blob perimeter corner matching algorithm.

    details:  
        Create new class ClosedCurveCornerMatcher.

        The class will have an algorithm similar to the Mokhtarian and
        Macworth min cost search for matching contour peaks, but 
        instead of scale space contours, it will use the closed
        curve's corners and feature descriptors.

        The cost and penalty have to be adjusted to using SSD of feature
        instead of sigma of peak.

        The contour algorithm used as a reference, the properties of
        strongest peak in the contours of an edge.  That sigma was
        used to calculate a penalty for matches of solutions that
        start with smaller sigma contours.
           For corners and feature descriptors, the equivalent should
           be some measure of best matched singly or best matchable
             -- considering the smallest error for a feature (where 
                error is the descriptor's auto-correlation SSD).
             -- considering the corner w/ the largest curvature magnitude.
                
                  X_dot(t,o~) * Y_dot_dot(t,o~) - Y_dot(t,o~) * X_dot_dot(t,o~) 
        k(t,o~) = -------------------------------------------------------------
                                 (X_dot^2(t,o~) + Y_dot^2(t,o~))^1.5


           Then a search pattern is needed.  The following is for comparison
           of one curve's corners in image 1 to another curve's corners in
           image 2.

           (1) Find the best match by feature descriptor SSD of each corner in
               curve 1 to curve 2.
               For the first corner in curve1, that search is O(N_curves2).
               For all corner1 points, the runtime for this stage is then
                  N_curves1 * O(N_curves2).

               This is kept to use in the remaining search.

           (2) Skipping ahead to define the cost:
               When a corner in curve1 is predicted to be located at (x2, y2) in
               image2, the smallest matching SSD within a tolerance, that
               is radius of distance from (x2, y2) is the found match.
                  The cost when there is such a point is the disance
                     between the found and predicted.

                  The cost when no point is found needs to be determined.
                     If the point were in reality not truly matchable, then
                     the cost affects all attempted matches similarly, so 
                     the cost should be less than infinity (else all solutions
                     would have infinite cost).
                     The cost for non-matched cannot be 0, that is ignored,
                     because that allows solutions with fewer or no matches
                     to have better total costs.
                     -> so the cost for a non-match should be at least as
                        large as the error for the point.
                        The cost may need to have an inverse relationship with
                        that error, so that small errors produce larger costs
                        for not finding a match.
                        Also, the cost needs to be in the reference frame of 
                        the distance.
                        Therefore, the maximum error is the tolerance in distance.
                        A normalized inverse of the SSD error could be used as
                        a factor to increase the cost past the tolerance.
                        ==> the cost for a non-matching point will be the tolerance
                            radius.
                            ---> a factor that includes inverse of SSD (statistically defined)
                                 should be experimented with to see if it improves the results.
 
           (3) The min cost heap needs to be initialized with starter solutions
               from pairs of matched points applied to one further point on the
               curves 
               (pairs of points are needed to determine the euclidean transformation and the
               evaluation is use of that transformation on other points).

               Considering patterns for trying pairs of points:

               If one knew that that best SSD match of a point in curve1 to
               point in curve2 were true for at least 2 points in the curves,
               then one could use a pattern of solution starter points of
                  pt 1 = curve1[0] w/ best SSD match in curve2
                  pt 2 = curve1[1] w/ best SSD match in curve2
                  written as (pt1, pt2) for one solution starter
                             (pt1, pt3) for another solution starter
                             (pt1, pt4)  ""
                             (pt2, pt3)  ""
                             (pt2, pt4)  ""
                             (pt3, pt4)  ""
                             which is n!/(k!(n-k)!) since k is always 2, can rewrite as n*(n-1)/2.
                                 for a curve with 4 corners, the heap would have 6 solution starter nodes

                An improvement to the  n!/(k!(n-k)!) would be to try all matches just for
                the first point in the two points needed in the solution.
                    The number of starter solutions would be  n*n*(n-1)/2 
                    The chances of finding the correct solution are much higher.
                    It requires that only one point in the corners common to both curves
                    be a true match for it's best SSD match in curve 2.
                       for a curve with 4 corners, the heap would have 24 solution starter nodes

                A more thorough pattern guaranteed to find the best solution
                would try every possible combination of curve1[0] with curve2[i] and the
                same for the 2nd point, trying all combinations of pairs.
                That would be n^3*(n-1)/2 number of solutions which becomes large
                very quickly even for a small number of points and has redundant solutions
                among them.  
                       for a curve with 4 corners, the heap would have 96 solution starter nodes
               
                It looks like a limit decided by error and runtime should be chosen to
                decide between the n*(n-1)/2. starter points and the n*n*(n-1)/2. starter points.
                      for n = 5  -->  10 vs 50
                      for n = 10 -->  45 vs 450
                      for n = 15 -->  105 vs 1575
                      for n = 25 -->  300 vs 7500
                Since the corners are already high gradient in intensities, can expect that the
                feature descriptors are somewhat unique so an assumption that the best match
                is correct for at least 2 out of n is not bad for n > 10.
       
                Note, that if needed, an improvement on the minimum approach of n*(n-1)/2 would
                be a set number of best SSD matches for a point, that is the top 2 SSD matches
                are tried instead of the top 1.  The factor would be applied to both points in
                the starter points, so for top 2, would have 4 times more, which is 2 * n*(n-1).

                It looks like a good approach to the first draft would be to use the
                n*(n-1)/2 starter pattern when both curves have number of corners > 10,
                else if number of corners is <= 10, should use the n*n*(n-1)/2 pattern.

                Whichever pattern is used, the starter solution is then applied to a third
                point in curve1 and the cost is determined.
                The starter solution and the cost are used to create a node to be inserted into
                the min cost heap.  for each solution, the corners thus far used and not used
                are kept track of.

           (4) Once the heap is initialized, the solution for successive extractions of
               the min cost node are built up one corner at a time until the min cost
               node drawn has no more untried points.

               More specifically,
                  the min cost node is extracted from the heap.
                  that solution is applied to the next strongest corner not yet selected in
                  the solution.
                  the cost is determined for the found match and a new heap node is inserted
                  back into the min cost heap.

               At some point, the extracted min cost node will have no more available matches
                  to evaluate, and that node is returned as the answer.

            Note that like the contour matching code, the invoking code needs to handle reversing
            the arguments when the scale is less than one.

    time estimate: minimum one day, maximum 1 week.
  
    amount of time used: 

    state of completion:

    comments:
              
---------------------------------------------------------------------
