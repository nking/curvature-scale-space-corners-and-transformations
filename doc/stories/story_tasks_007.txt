Lacking an app to keep these in, making a temporary file while
implementing...

=====================================================================
Story: Implementing a camera self-calibration algorithm by Ma, Chen, & Moore 2003
       "Camera Calibration: a USU Implementation"
 
    summary: given features from N images in image pixel coordinates and
      given the features in the world coordinate system, estimates
      the camera intrinsic and extrinsic parameters.

    time estimate: 
        minimum: 1 day
        maximum: 2 days

    amount of time used: 

    state of completion: at requirements stage

    comments regarding state:

    comments:

---------------------------------------------------------------------
Task 1: Write the stub for the main class CameraCalibration and the
        unit test for it.

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  (began near noon 4/7)

   state of completion: currently implementing

   comments:
              
   details:  
       Camera Calibration main method:
          input:  n, coordsI, coordsW
            n is the number of points in each image which is the
              same for all images.
            coordsI holds the image coordinates in pixels of 
               features present in all images ordered in the same
               manner and paired with features in coordsW.  
               It is a 2 dimensional double array of format
               3 X (N*n) where N is the number of images.
               the first row is the x coordinates, the second row
               is the y coordinates, and the third row is "1"'s.
            coordsW holds the world coordinates of features, ordered
               by the same features in the images.
               the first row is the X coordinates, the second row
               is the Y coordinates, and the third row is assumed to
               be zero as the scale factor is lost in the homography..
          output: camera intrinsic parameters, camera extrinsic parameters
          
          responsibilities:
             - asserts arguments: 
                N >= 3 where N is the number of images.
                input are consistent
             - for each image: invokes homography solution with a solver using SVD
             - given all homographies: estimates intrinsic parameters with a solver using SVD
             - for each image: estimates extrinsic parameters and then corrects the rotation
                matrix so that it is an orthogonal matrix (unitary matrix) using SVD products
             - for all coords and homographies: estimates radial distortion coefficients 
                with a solver using linear least squares (SVD...DLT)
             - improves the intrinsic and extrinsic estimates by optimization:
                  minimum residual of the given image points and the projection of the
                  world coordinates using current camera estimates.
                  -> LBFGS is in this project already and doesn't need the gradient or hessian,
                     can use finite difference.

       CameraCalibrationTest data 
          images are 3042 X 3504 (previously 3042x4032)
          errors likely > 8 pixels
          
                img2       img1         img3
          #1 560, 532     680, 720     744, 808
          #2 2428, 512    2208, 896    2284, 528
          #3 1484, 856    1508, 1100   1416, 1000
          #4 1488, 1708   1520, 1856   1428, 1784
          #5 1228, 1896   1296, 2012   1220, 1952
          #6 2136, 1908   2028, 2000   2012, 1972
          #7 620, 2800    704, 2940    788, 2718
          #8 2364, 2844   2208, 2780   2272, 2928

          features in WCS
          #1 -11, 14, 41.5
          #2  11, 14, 41.5
          #3   0, 9.7, 41.5
          #4   0, 0, 41.5
          #5 -3.7, -3, 41.5
          #6 -8, -3, 41.5
          #7 -11, -14, 41.5
          #8  11, -14, 41.5
          
          expecting 
             focalLength ~ 1604 pixels = 2.245 mm
             no skew
             xc=1521
             yc=1752
             little to no radial distortion (if was present, it is already removed)
             rotation between images = 23.4 degrees
             translation between images = 18 cm
          
          other information:
            pixel width = 1.4e-3mm
            FOV = 77 degrees = 1.344 radians

---------------------------------------------------------------------
Task 2: Implement the method that solves for the homography matrix H

   goal: given the image and world coordinates of features, calculates
       the homography matrix H.

   time estimate: minimum  30 min, maximum 0.5 day
  
   amount of time used:  30 min

   state of completion: implemented, not tested

   comments:
              H =   [ h11 h12 h13 ]
                    [ h21 h22 h23 ]
                    [ h31 h32 h33 ]
              H^T = [ h11 h21 h31 ]
                    [ h12 h22 h32 ]
                    [ h13 h23 h33 ]
          
              Let h_i be the ith column vector of H:
                  h_i = [h_i_1]^T = [h_i_1  h_i_2  h_i_3]
                        [h_i_2]
                        [h_i_3]
              
   details:  
          homography solver method:
            input: coordsI_i, coordsW_i 
               where coordsI_i holds the image coordinates in pixels of features present in image i
               and coordsW_i holds the world coordinates of features present in image 1 corresponding
               to the same features and order of coordsI_i
            output: double array of H, the projection matrix
            responsibilities:
              -creates matrix L (Sect 6.1 of Ma et al. 2003), 
               and finds the solution to x as orthogonal to L by using the SVD(L)
               to find the eigenvector belonging to the smallest eigenvalue.
              -reformats x into 3x3 H to return
              
---------------------------------------------------------------------
Task 3: implement the intrinsic parameters solver

   goal: 

   time estimate: minimum  hour, maximum 1/2 day
  
   amount of time used: 2 hours

   state of completion: implemented but not tested

   comments:
              
   details:  
            input: H as (3*NImages)x3 homography, projection matrices
              where each image homography is stacked row-wise
            output: camera intrinsic parameters
            responsibilities:
              - for each H:
                    form a matrix V_i_j out of the first 2 columns of each H matrix
                    and stack them by rows, into a matrix called V
              - perform SVD(V) to get right singular vector of V associated with the smallest singular value
                as the solution to b.
              - b holds the contents of the upper right triangle of B 
                where B = A^-T * A^-1 known as the absolute conic.
              - the intrinsic parameters are extracted from combinations of the solved 
                for B and other coefficients.
          
---------------------------------------------------------------------
Task 4: implement the extrinsic parameters solver

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used: 1 hour

   state of completion: implemented but not tested

   comments:
              
   details:  
          extrinsic parameters solver method:
            input: intrinsicInverse_i, H_i
              where intrinsicInverse_i is the inverse matrix of the camera intrinsic parameters
              and H_i is the homography for the image.
            output: estimated camera extrinsic parameters
            responsibilities:
              - estimates extrinsic parameters from combinations of A^-1 and columns of H
              - for the rotation matrix, matrix is made into a unitary matrix by
                SVD(R).U * SVD(R)*V^T
---------------------------------------------------------------------
Task 5: implement the method to estimate the radial distortion coefficients

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  1 hour so far

   state of completion: in the design stage

   comments:
              
   details:  
          radial parameters solver method:
            input: coordI, coordsW, Hs
            output: radial distortion parameters k1, k2
            responsibilities:
              - given 
                 the ideal projected undistorted image points (u, v) 
                   calculate from eqn (17) which is derived from eqn (15)
          
                then populate D and d below:
          
                then using linear least squares: k = (D^T*D)^-1 * d
          
                --------
                eqn (17)
                       h11*X_w + h13*Y_W + h13
                   u = -----------------------
                       h31*X_w + h32*Y_W + h33
           
                       h21*X_w + h23*Y_W + h23
                   v = -----------------------
                       h31*X_w + h32*Y_W + h33
           
                eqn(8) below starting with "u + ...":
                 NOTE:it uses equation #4 of Ma et al. 2004 Table 2
                      #4: f_r = 1 + k_1*r^2 + k_2*r^4
                          and ud−u0 = (u−u0)*f_r
                      can edit to use the lower order equation #3:
                      #3: f_r = 1 + k_1*r + k_2*r^2, but would use column 3 of Table 2 of Ma et al. too
                 NOTE: [x,y,1] = A^-1 * [u, v, 1]
                 NOTE: the center subtracted [x,y,1] is [x_c, y_c, 1]
                 NOTE: [x_c, y_c, 1] = -(u-u_0)*[s_x,s_y] where s is the pixel size in mm
                 NOTE: [x_c, y_c] = (f/Z_C)*[X_c,Y_c] if ignoring R and t
                     ==>presumably want to estimate [x,y] from [X_c,Y_c,Z_c] and f.
                        knowing that the radial distortion is applied to the centered
                        camera coordinates, one must assume the x^2 + y^2 are
                        the centered (x_c^2 + y_c^2), else would
                              add [x_0,y_0] determined from A^-1 * [u_0, v_0, 1]
             
                 u + (u-u_0)*[k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2] = u_d
                 v + (v-v_0)*[k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2] = v_d
                
                 [k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2]*(u-u_0) + u = u_d
                 [k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2]*(v-v_0) + v = v_d
                
                 [ [(x^2 + y^2)   (x^2+y^2)^2]*(u-u_0)   u ] * [k1] = u_d
                 [ [(x^2 + y^2)   (x^2+y^2)^2]*(v-v_0)   v ]   [k2] = v_d
                                                               [1 ]
                
                  ==> D = [ [(x^2 + y^2)   (x^2+y^2)^2]*(u-u_0)   u ]
                          [ [(x^2 + y^2)   (x^2+y^2)^2]*(v-v_0)   v ]
                          [ next point on 2 rows                    ]
                          [...                                      ]
          
                  ==> d =[u_d]
                         [v_d]
                         [u_d for next point]
                         [v_d for next point]
                         [...]

---------------------------------------------------------------------
Task 6: implement the method for parameter optimization

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  4 hours so far

   state of completion: in the design stage

   comments:
              
   details:  
          parameter optimization method:
            input: imageC, worldC, intrinsic, extrinsic, radial distortion
            output: intrinsic and exrinsic
            responsibilities:
              - given 
                  minimum residual of the given image points and the projection of the
                  world coordinates using current camera estimates.
                  -> LBFGS is in this project already and doesn't need the gradient or hessian,
                     can use finite difference.
          
                thirdparty.dlib.optimization.Helper.CentralDifferences f2 
                   = new Helper.CentralDifferences(f, 1.e-7);
                double fLower = -10;
                LBFGSOptimization opt = new LBFGSOptimization();
                double min = opt.findMin(searchStrategy, 
                    stopStrategy, f2, init, fLower);
          
                need to create f to implement IFunction.
                An example for a different objective is Helper.FunctionPoly
             - the objective function is
                 J = summation_i=0_to_N-1( summation_j=0_to_n-1( || m_i_j - ~m(A,k1,k2,R_i,t_i,M_j) ||^2 ) )
                   where N is the number of images, and
                         n is the number of features
                         m_i_j is the feature in the image plane, measured
                         M_i = [Xw,Yw,1]T is the feature in world coordinate space
                         ~m is the projection of point Mj in image i

           for the objective function need to predict the image point [u,v,1]T to compare to
           the measured [u,v,1]T, so need to predict the distorted image coordinates
           [u_d,v_d,1]^T

               from: http://16720.courses.cs.cmu.edu/lec/transformations.pdf
               using pose, that is Perspective n-Point (PnP)

               let M be the projection matrix, and [X_C_i, Y_C_i, Z_C_i] be the
                  point (a.k.a. feature) in world coordinate space (a.k.a. world scene)
                
                  and [u_i,v_i,1] is the same as in the Ma et al. papers as image coordinates

               NOTE: the measured feature coordinates in the image may have distortion
                 from the camera, so distortion should be removed at this stage.
                 the pattern used is present already in CameraCalibration.java, but
                 for the coordinates in the camera reference frame, so need to
                    // put x into camera coordinates reference frame:
                    double[][] pix = MatrixUtil.multiply(cameraIntrInv, x);
                    // remove radial distortion:
                    pix = CameraCalibration.removeRadialDistortion(pix, rCoeffs[0], rCoeffs[1]);
                    // transform back into image reference frame:
                    pix = MatrixUtil.multiply(cameraIntr, cc);

               then continue with the projection:
               
               [u_i,v_i,1] = M * [X_C_i,Y_C_i,Z_C_i]

                           = m00 m01 m02 * X_C_i
                             m10 m11 m12   Y_C_i
                             m20 m21 m22   Z_C_i

                        u_i= (m00*X_C_i + m01*Y_C_i + m02*Z_C_i)
                        v_i= (m10*X_C_i + m11*Y_C_i + m12*Z_C_i)
                          1= (m20*X_C_i + m21*Y_C_i + m22*Z_C_i)

               let m_1 be row 0 of M, following the notation in the lecture
                     then u_i = m_1 dot [X_C_i, Y_C_i, Z_C_i]
                      and v_i = m_1 dot [X_C_i, Y_C_i, Z_C_i]
                      and   1 = m_3 dot [X_C_i, Y_C_i, Z_C_i]
                     normalizing by the last row:
                         u_i = (m_1 dot [X_C_i, Y_C_i, Z_C_i]) / (m_3 dot [X_C_i, Y_C_i, Z_C_i])
                         v_i = (m_2 dot [X_C_i, Y_C_i, Z_C_i]) / (m_3 dot [X_C_i, Y_C_i, Z_C_i])

               let X_i be [X_C_i, Y_C_i, Z_C_i]

               then following the notation of the lecture:
                  u_i = m_1^T X_i/(m_3^T X_i)
                  and
                  v_i = m_2^T X_i/(m_3^T X_i)
   
               then m_1^T*X_i - (m_3^T*X_i)*u_i = 0
               and
                    m_2^T*X_i - (m_3^T*X_i)*v_i = 0

               form into L*m=0 (based on noise-free case)
                   [ X_i     0  X_i*u_i ] * [ m_1 ] = 0
                   [   0   X_i  X_i*v_i ]   [ m_2 ]
                                            [ m_3 ]
              can add the other features by continuing to fill L as pairs of rows for each feature. 

              to solve for [m_1, m_2, m_3] can use one of these methods:
              (a) assume noise, and solve for ||L*m||^2 which minimizes ||m||^2=1
                   by SVD(L).v^T[last row] which is the right singular eigenvector belonging
                   to the smallest eigenvalue 
              (b) non-linear optimization to minimize the difference between observed and model:
                   (u_i - ((m_1^T X_i)/(m_3^T X_i)))^2 + (v_i - ((m_2^T X_i)/(m_3^T X_i)))^2

              looking at M contents to consider which parameters in which order in the optimization iterations:

              M as m_1 is K * [R | t ] * X where t is actually -R*translation
                   m_2
                   m_3

                 [alpha  gamma  u_0 ]   [ r11  r12  r13  t_x ]   [ X ]
             M = [  0     beta  v_0 ] * [ r21  r22  r23  t_y ] * [ Y ]
                 [  0      0     1  ]   [ r31  r32  r33  t_z ]   [ 0 ]
                                                                 [ 1 ]
                 [alpha  gamma  u_0 ]   [ r11  r12  t_x ]   [ X ]
               = [  0     beta  v_0 ] * [ r21  r22  t_y ] * [ Y ]
                 [  0      0     1  ]   [ r31  r32  t_z ]   [ 1 ]

                 [ (alpha*r11 + gamma*r21 + u_0*r31)  (alpha*r12 + gamma*r22 + u_0*r32)  (alpha*t_x + gamma*t_y + u_0*t_z) ]
               = [ (beta*r21 + v_0*r31)               (beta*r22 + v_0*r32)               (beta*t_y + v_0*t_z)              ]
                 [ (r31)                              (r32)                              (t_z)                             ]

               m_3 then gives us [r31, r32, t_z]
               
               m21 = (beta*r21 + v_0*r31) where v_0 can be presumed to be known, and r31 is m31
               m22 = (beta*r22 + v_0*r32) with similar knowns and solved
               m23 = (beta*t_y + v_0*t_z) with similar knowns and solved
               ==> beta = (m21 - v_0*r31)/r21  has unknowns r21 and beta
               ==> beta = (m22 - v_0*r32)/r22  has unknowns r22 and beta
               ==> beta = (m23 - v_0*t_z)/t_y  has unknowns t_y and beta
                   so have 4 unknowns but only 3 data
                   not a closed loop solution, need stages of setting variables to fixed best values and
                     minimizing for the others.
                   -- considering: first optimizing for r21, r22, and t_y which solves m_3 and m_2
               ==> m11 = (alpha*r11 + gamma*r21 + u_0*r31) would then have unknowns alpha and r11 and gamma
               ==> m12 = (alpha*r12 + gamma*r22 + u_0*r32)                          alpha and r12 and gamma
               ==> m13 = (alpha*t_x + gamma*t_y + u_0*t_z)                          alpha and t_x and gamma
                   -- considering: next optimizing for r11, r12, t_x, alpha, and gamma which minimizes m_1

               OR 
                   -- considering fixing intrinsic parameters and solve for extrinsic using pose algorithm
                      then use all parameters for projection to estimate ~m and store the total sum of squared sums of residuals.
                      And iterate: change intrinsic, solve extrinsic w/ pose, estimate ~m, store residual and params if min...
                      (note that pose is the algorithm above, solving L*m.)


---------------------------------------------------------------------
=====================================================================
