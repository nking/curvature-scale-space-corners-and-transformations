Lacking an app to keep these in, making a temporary file while
implementing...

=====================================================================
Story: Implementing a camera self-calibration algorithm by Ma, Chen, & Moore 2003
       "Camera Calibration: a USU Implementation"
 
    summary: given features from N images in image pixel coordinates and
      given the features in the world coordinate system, estimates
      the camera intrinsic and extrinsic parameters.

    time estimate: 
        minimum: 1 day
        maximum: 2 days for Tasks 1-5.  Task 6 is 5 days max.  Testing max ~2 days.
                 Total max should be ~9 days.

    amount of time used: 

    state of completion: at requirements stage

    comments regarding state:

    comments:

---------------------------------------------------------------------
Task 1: Write the stub for the main class CameraCalibration and the
        unit test for it.

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  (began near noon 4/7)

   state of completion: currently implementing

   comments:
              
   details:  
       Camera Calibration main method:
          input:  n, coordsI, coordsW
            n is the number of points in each image which is the
              same for all images.
            coordsI holds the image coordinates in pixels of 
               features present in all images ordered in the same
               manner and paired with features in coordsW.  
               It is a 2 dimensional double array of format
               3 X (N*n) where N is the number of images.
               the first row is the x coordinates, the second row
               is the y coordinates, and the third row is "1"'s.
            coordsW holds the world coordinates of features, ordered
               by the same features in the images.
               the first row is the X coordinates, the second row
               is the Y coordinates, and the third row is assumed to
               be zero as the scale factor is lost in the homography..
          output: camera intrinsic parameters, camera extrinsic parameters
          
          responsibilities:
             - asserts arguments: 
                N >= 3 where N is the number of images.
                input are consistent
             - for each image: invokes homography solution with a solver using SVD
             - given all homographies: estimates intrinsic parameters with a solver using SVD
             - for each image: estimates extrinsic parameters and then corrects the rotation
                matrix so that it is an orthogonal matrix (unitary matrix) using SVD products
             - for all coords and homographies: estimates radial distortion coefficients 
                with a solver using linear least squares (SVD...DLT)
             - improves the intrinsic and extrinsic estimates by optimization:
                  minimum residual of the given image points and the projection of the
                  world coordinates using current camera estimates.
                  -> LBFGS is in this project already and doesn't need the gradient or hessian,
                     can use finite difference.

       CameraCalibrationTest data 
          images are 3042 X 3504 (previously 3042x4032)
          errors likely > 8 pixels
          
                img2       img1         img3
          #1 560, 532     680, 720     744, 808
          #2 2428, 512    2208, 896    2284, 528
          #3 1484, 856    1508, 1100   1416, 1000
          #4 1488, 1708   1520, 1856   1428, 1784
          #5 1228, 1896   1296, 2012   1220, 1952
          #6 2136, 1908   2028, 2000   2012, 1972
          #7 620, 2800    704, 2940    788, 2718
          #8 2364, 2844   2208, 2780   2272, 2928

          features in WCS
          #1 -11, 14, 41.5
          #2  11, 14, 41.5
          #3   0, 9.7, 41.5
          #4   0, 0, 41.5
          #5 -3.7, -3, 41.5
          #6 -8, -3, 41.5
          #7 -11, -14, 41.5
          #8  11, -14, 41.5
          
          expecting 
             focalLength ~ 1604 pixels = 2.245 mm
             no skew
             xc=1521
             yc=1752
             little to no radial distortion (if was present, it is already removed)
             rotation between images = 23.4 degrees
             translation between images = 18 cm
          
          other information:
            pixel width = 1.4e-3mm
            FOV = 77 degrees = 1.344 radians

---------------------------------------------------------------------
Task 2: Implement the method that solves for the homography matrix H

   goal: given the image and world coordinates of features, calculates
       the homography matrix H.

   time estimate: minimum  30 min, maximum 0.5 day
  
   amount of time used:  30 min

   state of completion: implemented, not tested

   comments:
              H =   [ h11 h12 h13 ]
                    [ h21 h22 h23 ]
                    [ h31 h32 h33 ]
              H^T = [ h11 h21 h31 ]
                    [ h12 h22 h32 ]
                    [ h13 h23 h33 ]
          
              Let h_i be the ith row of H:
                  h_i = [h_i_1]^T = [h_i_1  h_i_2  h_i_3]
                        [h_i_2]
                        [h_i_3]
              
   details:  
          homography solver method:
            input: coordsI_i, coordsW_i 
               where coordsI_i holds the image coordinates in pixels of features present in image i
               and coordsW_i holds the world coordinates of features present in image 1 corresponding
               to the same features and order of coordsI_i
            output: double array of H, the projection matrix
            responsibilities:
              -creates matrix L (Sect 6.1 of Ma et al. 2003), 
               and finds the solution to x as orthogonal to L by using the SVD(L)
               to find the eigenvector belonging to the smallest eigenvalue.
              -reformats x into 3x3 H to return
              
---------------------------------------------------------------------
Task 3: implement the intrinsic parameters solver

   goal: 

   time estimate: minimum  hour, maximum 1/2 day
  
   amount of time used: 2 hours

   state of completion: implemented but not tested

   comments:
              
   details:  
            input: H as (3*NImages)x3 homography, projection matrices
              where each image homography is stacked row-wise
            output: camera intrinsic parameters
            responsibilities:
              - for each H:
                    form a matrix V_i_j out of the first 2 columns of each H matrix
                    and stack them by rows, into a matrix called V
              - perform SVD(V) to get right singular vector of V associated with the smallest singular value
                as the solution to b.
              - b holds the contents of the upper right triangle of B 
                where B = A^-T * A^-1 known as the absolute conic.
              - the intrinsic parameters are extracted from combinations of the solved 
                for B and other coefficients.
          
---------------------------------------------------------------------
Task 4: implement the extrinsic parameters solver

   goal: need a reasonably accurate and fast method to solve for
         camera extrinsic parameters (a.k.a. P-n-P)

   time estimate: minimum  hour, maximum  day
  
   amount of time used: 1 hour.

   state of completion: implemented but not tested

   comments:
      completed the version in Ma et al. 2003
              
   details:  
      Ma et al. 2003  extrinsic parameters solver method:
          input: intrinsicInverse_i, H_i
              where intrinsicInverse_i is the inverse matrix of the camera intrinsic parameters
              and H_i is the homography for the image.
          output: estimated camera extrinsic parameters
          responsibilities:
              - estimates extrinsic parameters from combinations of A^-1 and columns of H
              - for the rotation matrix, matrix is made into a unitary matrix by
                SVD(R).U * SVD(R)*V^T
---------------------------------------------------------------------
Task 5: implement the method to estimate the radial distortion coefficients

   goal: 

   time estimate: minimum 1 hour, maximum 1 day
  
   amount of time used:  5 hours

   state of completion: implemented, but test results are a little off

   comments:
              
   details:  
          radial parameters solver method:
            input: coordI, coordsW, Hs
            output: radial distortion parameters k1, k2
            responsibilities:
                eqn (17)
                       h11*X_w + h13*Y_W + h13
                   u = -----------------------
                       h31*X_w + h32*Y_W + h33
           
                       h21*X_w + h23*Y_W + h23
                   v = -----------------------
                       h31*X_w + h32*Y_W + h33

                (ud, vd) are Real observed distorted image points
                (u, v) Ideal projected undistorted image points
                [x,y,1] = A^-1 * [u, v, 1]
        
                eqn (5) of Ma, Chen, & Moore 2004, "Rational Radial Distortion..."
                   ud-u0 = (u-u0)*f(r)
                   vd-v0 = (v-v0)*f(r)
           
                eqn (8) of Ma, Chen & Moore 2003, "Camera Calibration..."
                   ud = u + (u−u0)*f_r
                   vd = v + (v−v0)*f_r

                eqn (11) of Zhang 1998, "Flexible Camera Calibration ..."
                   ud = u + (u−u0)*[k1*r + k2*r^2]
                   vd = v + (v−v0)*[k1*r + k2*r^2]
        
                (5) and (8) use equation #3 or #4 of Ma et al. 2004 Table 2
                    #3: f_r = 1 + k_1*r + k_2*r^2
                            = 1 + k_1*(x^2 + y^2)^-1/2 + k_2*(x^2 + y^2)
                    #4: f_r = 1 + k_1*r^2 + k_2*r^4
                            = 1 + k_1*(x^2 + y^2) + k_2*(x^2 + y^2)^2

                factored out eqn (5) of Ma, Chen, & Moore 2004:
                   ud-u0 = (u-u0)*(1 + k_1*r + k_2*r^2)
                         = (u-u0) + (u-u0)*(k_1*r + k_2*r^2)
                   ud = u + (u-u0)*(k_1*r + k_2*r^2)
                   ud-u = (u-u0)*(k_1*r + k_2*r^2)
                 is the same as eqn (11) of Zhang 1998.
        
                Given n points in nImages, we can stack all equations together
                to obtain totally 2Nn equations in matrix form as 
                Dk = d, where k = [k1, k2]^T . 

                The linear least-square solutions for k is k = (D^T*D)^−1*D^T*d = pseudoInv(D)*d.
        
                  if choose #3:
             
                       k1                        k2                    const
                      ----------------------------------------------------------
                D = [ (u-u0)*sqrt(x^2 + y^2)    (u-u0)*(x^2 + y^2) ]   d = [ ud - u ]
                    [ (v-v0)*sqrt(x^2 + y^2)    (v-v0)*(x^2 + y^2) ]       [ vd - v ]

                  if choose #4:
             
                       k1                   k2                       const
                      ----------------------------------------------------------
                D = [ (u-u0)*(x^2 + y^2)    (u-u0)*(x^2 + y^2)^2 ]   d = [ ud - u ]
                    [ (v-v0)*(x^2 + y^2)    (v-v0)*(x^2 + y^2)^2 ]       [ vd - v ]
        

---------------------------------------------------------------------
Task 6: implement the method for intrinsic and extrinsic camera parameters optimization

   goal: given all of the parameters estimated above as the initial estimates,
         use non-linear optimization to get the final estimated values.

         the objective function is the re-projection error composed of the observed
         points in the image plane and their predicted projections from the
         estimated parameters in the projection matrix and the measured points 
         in the world coordinate system.

         objective function L = summation_i=1_to_N( summation_j=1_to_n(
           || m_i_j - ~m(A, k1, k2, R_i, t_i, M_j) ||^2
           where N is the number of images, n is the number of features (a.k.a. points),
           m_i_j is the image i coordinate of feature j,
           ~m(...) is the projacetion given it's arguments,
           M_i = [Xw,Yw,1]T is the feature in world coordinate space with Zw=0

   time estimate: minimum 8 hours, maximum ?  day
  
   amount of time used: 1 month for reading + 5 days for design + ? days for implementation 

   state of completion: finished design stage. beginning implementation.

   comments:
      This task spawned Task 7 to solve for extrinsic parameters only as that is
      commonly used alone once the camera intrinsic parameters are known.
      
      This task has now become an implementation of the intrinsic and extrinsic
      parameters refinement using the sparse
      block structure of the Jacobian, and the Schur complement to solve for the
      gradient using a cholesky factorization of the reduced camera matrix.

      It could be overloaded to solve for extrinsic parameters only as another task.

      The task was blocked for awhile while I revised other camera code and added to
      it methods, some of which are needed by this task.  Then I spent about 1 month
      reading in and around the subject of bundle adjustment.
      So in the amount of time used I'll separate reading from design and implementation.

   details:  
      Requirements:
        input: intrinsic and extrinsic camera parameters as first estimate,
               features observed in images, features in world coordinate frame,
               boolean flag for radial distortion model
        output: refined intrinsic and extrinsic camera parameters and variables
               needed by Levenberg-Marquardt algorithm which are the
               parameter update steps solved for, the gradient vector of the parameters,
               and the evaluation of the objective.

      References:
     
        http://users.ics.forth.gr/~lourakis/sba/PRCV_colloq.pdf
        lecture by Lourakis  “Bundle adjustment gone public”

        Engels, Stewenius, Nister 2006, “Bundle Adjustment Rules”
      
        Bill Triggs, Philip Mclauchlan, Richard Hartley, Andrew Fitzgibbon. 
        Bundle Adjustment – A Modern Synthesis. 
        International Workshop on Vision Algorithms, 
        Sep 2000, Corfu, Greece. pp.298–372, 
        10.1007/3-540-44480-7_21 . inria-00548290 

        Zhongnan Qu's master thesis, "Efficient Optimization for Robust Bundle 
        Adjustment", 2018 Technical University of Munich
     
        Chen, Chen, & Wang 2019, "Bundle Adjustment Revisited"
     
        T. Barfoot, et al., "Pose estimation using linearized rotations and 
        quaternion algebra", Acta Astronautica (2010), doi:10.1016/j.actaastro.2010.06.049
            -- using the rotation and translation update details.

      Design:
        additional information is present in directory doc as "bundle_adjustment.pdf"
            and "Levenberg-Marquardt_notes.pdf"

        classes and methods needed in outline format:

           add to Camera.java:

             // world coords in camera reference (i.e. transformed by R*(xW + t)
             double[][] xWC = worldToCameraCoordinates
             double[] xWCI = worldToCameraCoordinates

           create class BlockMatrixIsometric.java
               // for block matrices composed of equal sized blocks
               // members:
               //   double[][] a
               //   int bSize0; // block size for rows
               //   int bSize1; // block size for columns
               // methods:
               //   BlockMatrixIsometric(double[][] a, int bSize0, int bSize1)
               //   setBlock(double[][] b, int blockNumber0, int blockNumber1) :: void
               //   getBlock(int blockNumber0, int blockNumber1) :: double[][]
               //   getBlock(double[][] b, int blockNumber0, int blockNumber1) 

           add to MatrixUtil.java
              transpose(BlockMatrixIsometric a) :: BlockMatrixIsometric
              reshapeToVector(double[][] a) :: double[] // [*, 0], [*, 1], i.e. all rows in col 0, all rows in col 1
              fill(double[][] a, double value) :: void
            
           add to CameraCalibration.java
             static double[] applyRadialDistortion(double[] xC, double k1, double k2,
               boolean useR2R4);

           NOTE: xWI = column i of coordsW
           NOTE: xWCNI = xWCI/xWCI[2]

           create class BundleAdjustment.java

           add to BundleAdjustment.java

             pdCpIJCIJ(double[] xWCNI, k1, k2)
                 cij = -xWCNI 
                 ...

             pdCIJXWIJ(double[] xWC)
  
             pdCpIJYJ(double[] xWCNI, k1, k2)
                 cij = -xWCNI
                 ...

             pdXWIJPhiJ(double[] xWI, double[] phi)
                // coordsW are in world coordinate frame
                // phi is the rotation angle vector of length 3
                // cosPhi is cosine of the magnitude of phi
                // sinPhi is sine of the magnitude of phi

             // for aIJ creates dF/dCameraParams which are the 9 parameters of extrinsic and intrinsic
             // where the 9 parameters are the Qu notation for the variables phi_j, t_j, y_j
             //    for each image = 9*nImages elements (j index is used for images)
             // for bIJ creates dF/dPointParams which are the 3 parameters of the world point position
             //    for each feature = 3 * mFeatures elements (i index is used for features)
             // returns a 2X9 array and a 2X3 array
             aIJBIJ(double[] xWCI, k1, k2, extrinsics, double[][] outAIJ, double[][] outBIJ)
                double[] xWCN = xWCI/xWCI[2]
                //aIJ:
                //dF/dPhi = pdCpIJCIJ * pdCIJXWIJ * pdXWIJPhiJ   [2X3]
                //dF/dT   = pdCpIJCIJ * pdCIJXWIJ                [2X3]
                //dF/dy   = pdCpIJYJ                             [2X3]
                //bIJ:
                //dF/dX = pdCpIJCIJ * pdCIJXWIJ                  [2X3]

             // there may be more than one camera and should be 6 of more features per image
             // (3 for rot, 3 for trans) and among those, need 3 per camera for the intrinsic parameters
             // and 2 or more vantage points for the point parameters (no reference for these
             // numbers, just a rough estimate from counting the number of unknowns).
             // the code needs initial parameter estimates in intr, extrRot, and extrTrans.
             // the code returns results in intr, extrRot, extrTrans, outGradP, outGradC, 
             //    outFSqSum, outDP, outDC for refined parameters
             //    and the gradient, objective and parameter update steps needed by
             //    code such as Levenberg-Marquardt.
             // runtime complexity is ~ O(nFeatures * mImages^2)
             // assumptions used in forming the partial deivatives of the intrinsic camera parameters
             //    are no skew, focal length along x is the same as focal length along y, square pixels.
             static public void solveSparse(double[][] coordsI, double[][] coordsW, 
                 double[][] intr, double[][] extrRot, double[][] extrTrans,
                 double[] kRadial, final int nMaxIter, boolean useR2R4,
                 double[] outDP, double[] outDC, double[] outGradP, double[] outGradC, double[] outFSqSum)

           create class LevenbergMarquardtForBA.java

           add to LevenbergMarquardtForBA.java
               static class BA with members
                   double[][] intr;
                   double[][] extrRot;
                   double[][] extrTrans;
                   double[] radial;
                   boolean useR2R4;
                   double err;

           add to LevenbergMarquardtForBA.java
             static public BA solveUsingSparse(double[][] coordsI, double[][] coordsW,
                final int nMaxIter, boolean useR2R4)

        details of the methods:

           coordinate information:
             -- create the projected coordinates needed in "eps" (Lourakis notation)
                eps is the righthand side vector of the augmented normal equation.
             -- tranform X_w into camera frame coords X_c
                X_feature_c = R_image*(X_feature_w + t_image)  <== using + translation in CameraCalib*java too
                X_feature_c in homogeneous = X_feature_c/X_feature_c[Z-coord]
                *The coordinates are now in the camera frame.
             -- apply radial distortion transformation f(r)=1+k2*r^2+k1*r^4, then multiply that by x_c
                (see the code I have for applying the distortion to x and y separately).
                Can call these x_d for distorted.
                *The coordinates are in the camera frame, but distorted.
             -- multiply by the intrinsic camera parameters matrix: K*x_d
                (which includes transforming origin from center to image corner)
                *The coordinates are now in the image frame.
                these are called x_hat.
             -- "eps_i_j" = x_i_j observed in images - x_hat_i_j projected to image frame
                NOTE: eps_a = A^T * eps, eps_b =  B^T * eps

           partial derivatives:
             -- implemented from Qu 2018
                Assumptions:  no skew, square pixels, f=f_x=f_y

           details of the BundleAdjustment.solveSparse(...) method:
             -- following the Engels pseudocode for solving for the parameter vector via the
                reduced camera matrix and cholesky factoring with forward and back substitution
                to avoid inverting the reduced camera matrix.

                this method should return the parameter vector updates (dP and dC), 
                   and the objective as the sum of squares of the observed - predicted, 
                   and the gradient vector components used by L-M in calculating the 
                   gain ratio and evaluating stopping criteria (outGradP and outGradC).
                the output variables will be arguments given to the method instead of
                   returned by it in order to allow garbage collection to reclaim the
                   class instance of BundleAdjustment (or similar name) and it's associated memory.

                consider adapting the code to use an associative map for features in images.
                   e.g. vij = 1 iff point i is visible in image j as used by Lourakis.


             // j for m images
             // i for n features

             // each row is for 1 image, so size for each is [mImages X 6]
             // inversion of matrix HPP (aka V*) for case (9^3)*mImages < (3^3)*nFeatures
             // consider branching here for inversion of matrix HCC (aka U*) for case (9^3)*mImages > (3^3)*nFeatures


             init mA ;// matrix A, aka reduced camera matrix; [9m X 9m]; [mXm] block matrix with  blocks [9x9]
             init vB ;// vector B, on the rhs of eqn; a matrix acting as a vector with m blocks of size [9X1]
             init hPPI; // aka V_i; a [3X3] block
             init bPI; // aka jPTf; row j of bP; [3X1]
             declare bIJT; //aka jP_I_J^T [3X2]
             declare aIJT;//aka jC_I_J^T  [9X2]
             declare bIJsq // aka jP^T*JP; [3X3]
             init aIJ; [2X9]
             init bIJ; [2X3]
             declare bIJTF; aka bP [3X1]
             declare aIJTF; aka bC [9X1]
             // outBC and outBP are the output gradient vectors -J^T*(x-x_hat) as -aIJTF and -bIJTF, respectively.
             double[] outGradC = new double[9]; // summation of aij^T*fij
             double[] outGradP = new double[3]; // summation of bij^T*fij
             declare fIJ;// [2X1]
             double outFSqSum = 0;
             declare hPCJ; // m rows of blocks of size [3X9]
             declare tP; // [3X1]
             declare tPC; // [9X3]
             declare tmp; // [9X1]
             declare hPCIJT; // [9X3]
             init tPs;// n blocks of [3X1] // i is nFeatures
             init tPCTs;// nXm blocks of [3X9] // i is nFeatures, j is mImages

             //runtime complexity for this loop is O(nFeatures * mImages^2)
             for (i=0; i<nFeatures; ++i) { // this is variable p in Engels pseudocode
                 reset hPPI to 0's;// [3x3]// a.k.a. V*_i. 
                 reset bPI to 0's; //[3X1]
                 // if the number of images that a feature is in is different for any feature,
                 //  need to reset hPCJ to m rows of blocks of size [3X9] for the feature

                 // extract the world feature.  size [1X3]
                 xWI = MatrixUtil.extractColumn(coordsW, col);
                 //transform to camera reference frame. size [1X3]
                 xWCI = Camera.worldToCameraCoordinates(xWI, extrinsics);
                 // normalize
                 xWCNI = xWCFI/xWCFI[2];
                 // distort
                 xWCNDI = CameraCalibration.applyRadialDistortion(xWCFNI, k1, k2, useR2R4);

                 for (j=0; j<mImages; ++j) {
                     aIJ, bIJ from aIJBIJ(xWCI, k1, k2, extrinsics, aIJ, bIJ)
                     // aij aka jC is [2X9] when including intrinsic, else [2X6]
                     // bij aka jP is [2X3].

                     //[3X2]  aka jP^T
                     bIJT = MatrixUtil.transpose(bIJ);

                     // bij^T * bij = [3X2]*[2X3] = [3X3]
                     bIJsq = MatrixUtil.multiply(bIJT, bIJ)

                     // add jP^T*JP to upper triangular part of hPP aka V
                     // sum bijsq over all images and set into diagonal of hPP_i which is V*_i
                     //     elementwise addition of 3X3 blocks:
                     hPPI = MatrixUtil.elementwiseAdd(hPPI, bijsq)

                     // the observed feature i in image j
                     xIJ = MatrixUtil.extractColumn(coordsI, col);

                     //the projected feature i into image j reference frame.  [1X3]
                     double[][] cameraIntr = if more than one camera, extract j
                     xIJHat = MatrixUtil.multiply(cameraIntr, xWCDI)

                     // f_i_j [2X1] 
                     fIJ = MatrixUtil.elementwiseSubtract(xIJ, xIJHat);
                     outFSqSum += MatrixUtil.lpSum(fIJ, 2);

                     // subtract jP^T*f (aka bP) from bP
                     bIJTF =  bIJT * fIJ;// [3X1]
                     bPI -= bIJTF; // element wise subtract
                     elementwisesubtract outGradP , bIJTF

                     // [9X2] aka jC^T
                     aIJT = MatrixUtil.transpose(aIJ);

                     // if camera c is free means this?
                     if (image j has feature i in it) {

                         // add jCT*JC aka U to upper triangular part of block (j,j) of lhs mA; // [9X9]
                         mA[j][j] = aIJT * aIJ;

                         // compute block (i,j) of hPC as hPC=jPTJC [3X9]
                         //    and store until end of image j loop.
                         hPCJ[j] = bIJT * aIJ;

                         // subtract aIJT*f (where bc = -aIJT*f aka -jCT*f) from block row j in vB. [9X1]
                         aIJTF = aIJT * fIJ

                         elementwisesubtract outGradC , aIJTF

                         vB[j] = MatrixUtil.elementwiseSubtract(vB[j], aIJTF)
                     }
                 } // end image j loop

                 //invert hPPI // hPP is V* // [3X3]
                 invHPPI = hppI; // invert the diagonal block for feature i;

                 // hPPI^-1 * bPI is V^-1*bPI // [3X3][3X1] = [3X1]
                 tP = invHPPI * bPI;
                 tPs[i] = tP; // system.arraycopy...

                 // outer product of feature i
                 // for each free camera means? in context of a use case of real-time acquisition?
                 for (j=0; j<mImages; ++j) {

                     // subtract hPC^T * tP = hPC^T * (hPP^-1) * bP from part j for image j of rhs vB;
                     //                     = W * V^-1 * bP
                     // [9X1] block

                     // calc hPC^T aka W for feature i, all j images;  [9X3] 
                     hPCIJT = MatrixUtil.transpose( hPCJ[j] );

                     // [9X3][3X1] = [9X1]
                     tmp = MatrixUtil.multiply(hPCIJT, tP);

                     // (at this point, have invHPPI (W_I_J for all j's of current feature i))
                     //   subtract hPC^T * tP from element j of rhs vB.
                     /* e.g.  (W*V^-1)*(bP) =
                                                                  vector element i=1 
                            element i=1,j=1                       (all j for this i already calculated)
                              \/                                    \/
                        -> | W11*V1   W21*V2  W31*V3  W41*V4 | * | B11T*F11+B12T*F12+B13T*F13 | <-
                           | W12*V1   W22*V2  W32*V3  W42*V4 |   | B21T*F21+B22T*F22+B23T*F23 |
                           | W13*V1   W23*V2  W33*V3  W43*V4 |   | B31T*F31+B32T*F32+B33T*F33 |
                                                                 | B41T*F41+B42T*F42+B43T*F43 |
                          *The V's are inverses in this matrix
                     */

                     vB[j] = MatrixUtil.elementwiseSubtractBlock(vB[j], tmp)

                     // calc hPC^T * invHPPI and store in variable TPC/  [9X3][3X3] = [9X3]
                     tPC = hPCIJT * invHPPI;

                     // tPC^T = invHPPI^T * hPCIJ = invHPPI * hPCIJ
                     set block i,j of tPCTs to MatrixUtil.multiply(invHPPI, hPCJ[j]);

                     for (j2=j+1; j2<mImages; ++j2) {
                         // calc tPC * hPCJ[j2] = hPC^T * invHPPI * hPCJ[j2] // [9X3][3X9]=[9X9]
                         //   subtract from block (j, j2) of lhs mA
                         mA[j][j2] -= (tPC * hPCJ[j2]);
                     }
                 } // end image j loop
             } // end feature i loop

             // (optional) Fix gauge by freezing coordinates and thereby reducing the linear system with a few dimensions.

             // cholesky decompostion to solve for dC in mA*dC=vB
             // (using the sparsity of upper and lower triangular matrices results in 
             //    half the computation time of LU decomposition in comparison)
             DenseCholesky chol = no.uib.cipr.matrix.DenseCholesky.factorize(new DenseMatrix(mA));
             LowerTriangDenseMatrix cholL = chol.getL()
             UpperTriangDenseMatrix cholU = chol.getU()
             double[] yM = MatrixUtil.forwardSubstitution(i
                 MatrixUtil.convertToRowMajor(cholL), MatrixUtil.vectorize(vB));
             // dC is [9m X 1]
             double[] dC = MatrixUtil.backwardSubstitution(
                 MatrixUtil.convertToRowMajor(cholU), yM);

             init dP // [3nX1]
             declare dCJ // [9X1]
             declare double[] tmp;
             for (i=0; i<nFeatures; ++i) {
                 // start with point update for feature i, dP = tP
                 dP[i] = tPs[i];
                 for (j=0; j<mImages; ++j) {
                     // subtract tPC^T*dCJ where dCJ is for image j (that is dCJ = subvector: dC[j*9:(j+1)*9) 
                     dCJ = // system.arraycopy dC[j*9:(j+1)*9)
                     tmp = tPCTs(i,j)*dCJ;
                     dP[i] = element wise subtract dP[i] - tmp;
                 }
                 // compute updated point
             }

             // dP and dC are the updating vectors for L-M
             -----------

           details of the LevenbergMarquardtForBA.solveSparse(...) method:

             static public BA solveUsingSparse(double[][] coordsI, double[][] coordsW,
                final int nMaxIter, boolean useR2R4)

                the structure is similar to LevenbergMarquardtForPNP.solveForPose(...)
                    excepting that the parameter update steps, gradients, and the objective
                    evaluation are calculated by
                    LevenbergMarquardtForBA.solveUsingSparse(...)

---------------------------------------------------------------------
Task 7: implement a method for extrinsic parameter optimization

   goal: given the extrinsic parameters estimated above as the initial estimates,
         use non-linear optimization to get the final estimated values.

         the objective function is the re-projection error composed of the observed
         points in the image plane and their predicted projections from the
         estimated parameters in the projection matrix and the measured points 
         in the world coordinate system.

         objective function L = summation_i=1_to_N( summation_j=1_to_n(
           || m_i_j - ~m(A, k1, k2, R_i, t_i, M_j) ||^2
           where N is the number of images, n is the number of features (a.k.a. points),
           m_i_j is the image i coordinate of feature j,
           ~m(...) is the projacetion given it's arguments,
           M_i = [Xw,Yw,1]T is the feature in world coordinate space with Zw=0

   time estimate: minimum 8 hours, maximum ?  day
  
   amount of time used:  3 days so far on background reading and algorithm details.

   state of completion: complete, but not tested

   comments:
      implemented as solveForPose() in class LevenbergMarquardtForPose.java.

      the notes below are following the Ma et al. 2003 algorithm, then other references
      in more detail.  3 methods are suggested.
      This task will implement algorithm (1)(a).

   details:  
          parameter optimization method:
            input: imageC, worldC, intrinsic, extrinsic, radial distortion
            output: intrinsic and exrinsic
            responsibilities:
              - given all of the parameters estimated above as the initial estimates,
                use non-linear optimization to get the final estimated values.

                the objective function is the re-projection error composed of the observed
                points in the image plane and their predicted projections from the
                estimated parameters in the projection matrix and the measured points 
                in the world coordinate system.

                objective function L = summation_i=1_to_N( summation_j=1_to_n(
                  || m_i_j - ~m(A, k1, k2, R_i, t_i, M_j) ||^2
                  where N is the number of images, n is the number of features (a.k.a. points),
                  m_i_j is the image i coordinate of feature j,
                  ~m(...) is the projetion given it's arguments,
                  M_i = [Xw,Yw,1]T is the feature in world coordinate space with Zw=0

                The authors use Matlab's fminunc which finds the minimum of an unconstrained
                multivariate function, it's using BFGS.

           for the objective function need to predict the image point [u,v,1]T to compare to
           the measured [u,v,1]T, so need to predict the distorted image coordinates
           [u_d,v_d,1]^T or undistort the observed image coordinates.

           to remove the distortion from the observed image points:
              // put x into camera coordinates reference frame:
              double[][] pix = MatrixUtil.multiply(cameraIntrInv, x);
              // remove radial distortion:
              pix = CameraCalibration.removeRadialDistortion(pix, rCoeffs[0], rCoeffs[1]);
              // transform back into image reference frame:
              pix = MatrixUtil.multiply(cameraIntr, cc);

           to minimize the objective function w/ non-linear optimization, could 
           (1) use Levenberg-Marquardt (L-M) over BFGS because it uses the Jacobian 
               and an estimated Hessian to inform step sizes and use L-M over 
               Gauss-Newton because it has a damping term which ensures 
               the positive definite of the coefficient matrix.
               
               A summary of the L-M algorithm  can be found in several sources.
               Insightful comments: a lecture by Danping Zou @Shanghai Jiao Tong University ,
               EE382-Visual localization & Perception, “Lecture 08- Nonlinear least square & RANSAC”
               http://drone.sjtu.edu.cn/dpzou/teaching/course/lecture07-08-nonlinear_least_square_ransac.pdf

               An L-M approach to minimizing the re-projection error of the perspective projection
               is outlined slightly differently from one another:

               (a) lecture notes of Gordon Wetzstein at Stanford University,
                   EE 267 Virtual Reality, "Course Notes: 6-DOF Pose Tracking with the VRduino",
                   https://stanford.edu/class/ee267/notes/ee267_notes_tracking.pdf
                   includes details for the partial derivatives to build the Jacobian:
                   J = J_f*J_g where p is the parameter vector [thetas, translations]
                      J_g = dh/dp
                         where h is the 2-D projection matrix of size 3x3 as
                         2 columns of rotation and last column is translation
                      J_f = df/dh where f is the world point transformed by the 
                         homography h.
                   J = df/dp
               (b) Barfoot et al. 2010 has details of perturbation of the rotation (euler and 
                   quaternion) and translation.
               (c) a summary of Levenberg-Marqurdt and perturbation of a vector and a Lie group
                   are in the Danping Zou lecture
                   http://drone.sjtu.edu.cn/dpzou/teaching/course/lecture07-08-nonlinear_least_square_ransac.pdf
               (d) Chapter 6 of Richard Szeliski 2010 "Computer Vision: Algorithms and Applications"
                   equations (6.44)-(6.47)

               One could reform the projection equations as separate steps in a chain of transformations
               and iteratively minimize the robustified linearized re-projection errors for each step
               (Section 6.2.2 of Szeliski 2010 on pose) simmilar to back-propogation in neural 
               networks (Bishop 2006)..
               "The advantage of this chained set of transformations is that each one has a 
               simple partial derivative with respect both to its parameters and to its input. 
               Thus, once the predicted value of x ̃i has been computed based on the 3D point 
               location pi and the current values of the pose parameters (cj,qj,k), we can 
               obtain all of the required partial derivatives using the chain rule."
               ..."The one special case in this formulation that can be considerably simplified 
               is the computation of the rotation update. Instead of directly computing the 
               derivatives of the 3X3 rotation matrix R(q) as a function of the unit 
               quaternion entries, you can prepend the incremental rotation matrix deltaR(omega) 
               given in Equation (2.35) to the current rotation matrix and compute the
               partial derivative of the transform with respect to these parameters, which 
               results in a simple cross product of the backward chaining partial derivative 
               and the outgoing 3D vector (2.36)"

                      write the projection equations as
                         x_i = f(X_i;R,t,K)
                      and iteratively minimize the robustified linearized re-projection errors
                         E_NLP = summation_i( rho( (d(f)/d(R))*deltaR + (d(f)/d(t))*deltat + (d(f)/d(K))*deltaK - resid_i)
                         where resid_i = x ̃i - xˆi is the current residual vector (2D error in predicted position) 
                         and the partial derivatives are with respect to the unknown pose parameters 
                         (rotation, translation, and optionally intrinsic calibration)
                      An easier to understand (and implement) version of the above non-linear regression 
                      problem can be constructed by re-writing the projection equations as a concatenation 
                      of simpler steps, each of which transforms a 4D homogeneous coordinate X_i by a 
                      simple transformation such as translation, rotation, or perspective division (Figure 6.5). 
                      The resulting projection equations can be written as
                         y_1 = f_T(X_i;c_j) = X_i - c_j,   where c_j is camera center
                         y_2 = f_R(y_1;q_j) = R(q_j)*y_1
                         y_3 = f_P(y_2) = y_2/z_2
                         x_i = f_C(y_3; k) = K(k) * y_3

                      Note that in these equations, we have indexed the camera centers cj and camera rotation 
                      quaternions q_j by an index j, in case more than one pose of the calibration object 
                      is being used (see also Section 7.4.) We are also using the camera center cj instead 
                      of the world translation t_j , since this is a more natural parameter to estimate.

                      The advantage of this chained set of transformations is that each one has a 
                      simple partial derivative with respect both to its parameters and to its input. 
                      Thus, once the predicted value of x ̃i has been computed based on the 3D point 
                      location X_i and the current values of the pose parameters (c_j,q_j,k), 
                      we can obtain all of the required partial derivatives using the chain rule
                          d(resid_i)/d(param_k) = (d(resid_i)/d(y_k))*(d(y_k)/d(param_k))
                          
                      The rotation partial derivative, espec. can be simplified:
                          eqn 2.35 d(R(ω)*v)/d(ω^T) = -[v]_x

                      see Section 4.2 of Gleicher & Witkin 1992 "Through-the-Lens Camera Control"

           (2) Lu et al. is recommended as a very accurate algorithm, 
               which is fast in comparison with other iterative ones but slow compared to 
               non-iterative methods.
               C. P. Lu, G. D. Hager, and E. Mjolsness 2000 PAMI, 22(6):610–622. 
               "Fast and globally convergent pose estimation from video images."

               comment from Moerno-Noguer:
               "Lu et al.’s approach relies on an initial estimation of the camera pose 
               based on a weak-perspective assumption (object far from the lens), 
               which can lead to instabilities when the assumption is not satisfied. 
               This happens when the points of the object are projected onto a small 
               region on the side of the image...our [Moreno-Noguer et al.'s] algorithm 
               can be used to initialize Lu et al.'s"
           (3) a distributed solution using ADAM to divide the problem:
               K. N. Ramamurthy, C. Lin, A. Y. Aravkin, S. Pankanti, and R. Viguier.
               Distributed bundle adjustment. In 2017 IEEE International Conference
               on Computer Vision Workshops, pages 2146–2154, 2017.
           (4) several suggestions in http://users.ics.forth.gr/~lourakis/sba/PRCV_colloq.pdf

---------------------------------------------------------------------
Task 8: implement the extrinsic parameters solver of Moreno-Noguer et al. 2007

   goal: implement a fast accurate P-n-P algorithm.
         The Moerno-Noguer et al. 2007 algorithm is a non-iterative solution 
         to the PnP problem — the estimation of the pose of a calibrated 
         camera from n 3D-to-2D point correspondences—whose computational 
         complexity grows linearly with n.

   time estimate: 
  
   amount of time used:

   state of completion: not scheduled.  consider as future implementation.

   priority: low as Task 4 has same goal and is already implemented.

   comments:
       implementing an algorithm recommended in Section 6.2.1 of Szeliski as
       a fast reasonably accurate P-n-P algorithm to be used before refinement
       with an iterative solution.  
       The P-n-P algorithm determines the position and orientation of a camera given its
       intrinsic parameters and a set of n correspondences between
       3D points and their 2D projections.

       https://github.com/cvlab-epfl/EPnP/tree/master/cpp
              
   details:  
      Moreno-Noguer et al. 2007  extrinsic parameters solver method:
          input: 
          output: estimated camera extrinsic parameters
          responsibilities:
---------------------------------------------------------------------
=====================================================================
