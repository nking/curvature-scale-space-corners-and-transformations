Lacking an app to keep these in, making a temporary file while
implementing...

=====================================================================
Story: Implementing a camera self-calibration algorithm by Ma, Chen, & Moore 2003
       "Camera Calibration: a USU Implementation"
 
    summary: given features from N images in image pixel coordinates and
      given the features in the world coordinate system, estimates
      the camera intrinsic and extrinsic parameters.

    time estimate: 
        minimum: 1 day
        maximum: 2 days for Tasks 1-5.  Task 6 is 5 days max.  Testing max ~2 days.
                 Total max should be ~9 days.

    amount of time used: 

    state of completion: at requirements stage

    comments regarding state:

    comments:

---------------------------------------------------------------------
Task 1: Write the stub for the main class CameraCalibration and the
        unit test for it.

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  (began near noon 4/7)

   state of completion: currently implementing

   comments:
              
   details:  
       Camera Calibration main method:
          input:  n, coordsI, coordsW
            n is the number of points in each image which is the
              same for all images.
            coordsI holds the image coordinates in pixels of 
               features present in all images ordered in the same
               manner and paired with features in coordsW.  
               It is a 2 dimensional double array of format
               3 X (N*n) where N is the number of images.
               the first row is the x coordinates, the second row
               is the y coordinates, and the third row is "1"'s.
            coordsW holds the world coordinates of features, ordered
               by the same features in the images.
               the first row is the X coordinates, the second row
               is the Y coordinates, and the third row is assumed to
               be zero as the scale factor is lost in the homography..
          output: camera intrinsic parameters, camera extrinsic parameters
          
          responsibilities:
             - asserts arguments: 
                N >= 3 where N is the number of images.
                input are consistent
             - for each image: invokes homography solution with a solver using SVD
             - given all homographies: estimates intrinsic parameters with a solver using SVD
             - for each image: estimates extrinsic parameters and then corrects the rotation
                matrix so that it is an orthogonal matrix (unitary matrix) using SVD products
             - for all coords and homographies: estimates radial distortion coefficients 
                with a solver using linear least squares (SVD...DLT)
             - improves the intrinsic and extrinsic estimates by optimization:
                  minimum residual of the given image points and the projection of the
                  world coordinates using current camera estimates.
                  -> LBFGS is in this project already and doesn't need the gradient or hessian,
                     can use finite difference.

       CameraCalibrationTest data 
          images are 3042 X 3504 (previously 3042x4032)
          errors likely > 8 pixels
          
                img2       img1         img3
          #1 560, 532     680, 720     744, 808
          #2 2428, 512    2208, 896    2284, 528
          #3 1484, 856    1508, 1100   1416, 1000
          #4 1488, 1708   1520, 1856   1428, 1784
          #5 1228, 1896   1296, 2012   1220, 1952
          #6 2136, 1908   2028, 2000   2012, 1972
          #7 620, 2800    704, 2940    788, 2718
          #8 2364, 2844   2208, 2780   2272, 2928

          features in WCS
          #1 -11, 14, 41.5
          #2  11, 14, 41.5
          #3   0, 9.7, 41.5
          #4   0, 0, 41.5
          #5 -3.7, -3, 41.5
          #6 -8, -3, 41.5
          #7 -11, -14, 41.5
          #8  11, -14, 41.5
          
          expecting 
             focalLength ~ 1604 pixels = 2.245 mm
             no skew
             xc=1521
             yc=1752
             little to no radial distortion (if was present, it is already removed)
             rotation between images = 23.4 degrees
             translation between images = 18 cm
          
          other information:
            pixel width = 1.4e-3mm
            FOV = 77 degrees = 1.344 radians

---------------------------------------------------------------------
Task 2: Implement the method that solves for the homography matrix H

   goal: given the image and world coordinates of features, calculates
       the homography matrix H.

   time estimate: minimum  30 min, maximum 0.5 day
  
   amount of time used:  30 min

   state of completion: implemented, not tested

   comments:
              H =   [ h11 h12 h13 ]
                    [ h21 h22 h23 ]
                    [ h31 h32 h33 ]
              H^T = [ h11 h21 h31 ]
                    [ h12 h22 h32 ]
                    [ h13 h23 h33 ]
          
              Let h_i be the ith row of H:
                  h_i = [h_i_1]^T = [h_i_1  h_i_2  h_i_3]
                        [h_i_2]
                        [h_i_3]
              
   details:  
          homography solver method:
            input: coordsI_i, coordsW_i 
               where coordsI_i holds the image coordinates in pixels of features present in image i
               and coordsW_i holds the world coordinates of features present in image 1 corresponding
               to the same features and order of coordsI_i
            output: double array of H, the projection matrix
            responsibilities:
              -creates matrix L (Sect 6.1 of Ma et al. 2003), 
               and finds the solution to x as orthogonal to L by using the SVD(L)
               to find the eigenvector belonging to the smallest eigenvalue.
              -reformats x into 3x3 H to return
              
---------------------------------------------------------------------
Task 3: implement the intrinsic parameters solver

   goal: 

   time estimate: minimum  hour, maximum 1/2 day
  
   amount of time used: 2 hours

   state of completion: implemented but not tested

   comments:
              
   details:  
            input: H as (3*NImages)x3 homography, projection matrices
              where each image homography is stacked row-wise
            output: camera intrinsic parameters
            responsibilities:
              - for each H:
                    form a matrix V_i_j out of the first 2 columns of each H matrix
                    and stack them by rows, into a matrix called V
              - perform SVD(V) to get right singular vector of V associated with the smallest singular value
                as the solution to b.
              - b holds the contents of the upper right triangle of B 
                where B = A^-T * A^-1 known as the absolute conic.
              - the intrinsic parameters are extracted from combinations of the solved 
                for B and other coefficients.
          
---------------------------------------------------------------------
Task 4: implement the extrinsic parameters solver

   goal: need a reasonably accurate and fast method to solve for
         camera extrinsic parameters (a.k.a. P-n-P)

   time estimate: minimum  hour, maximum  day
  
   amount of time used: 1 hour.

   state of completion: implemented but not tested

   comments:
      completed the version in Ma et al. 2003
              
   details:  
      Ma et al. 2003  extrinsic parameters solver method:
          input: intrinsicInverse_i, H_i
              where intrinsicInverse_i is the inverse matrix of the camera intrinsic parameters
              and H_i is the homography for the image.
          output: estimated camera extrinsic parameters
          responsibilities:
              - estimates extrinsic parameters from combinations of A^-1 and columns of H
              - for the rotation matrix, matrix is made into a unitary matrix by
                SVD(R).U * SVD(R)*V^T
---------------------------------------------------------------------
Task 5: implement the method to estimate the radial distortion coefficients

   goal: 

   time estimate: minimum  hour, maximum  day
  
   amount of time used:  1 hour so far

   state of completion: in the design stage

   comments:
              
   details:  
          radial parameters solver method:
            input: coordI, coordsW, Hs
            output: radial distortion parameters k1, k2
            responsibilities:
              - given 
                 the ideal projected undistorted image points (u, v) 
                   calculate from eqn (17) which is derived from eqn (15)
          
                then populate D and d below:
          
                then using linear least squares: k = (D^T*D)^-1 * d
          
                --------
                eqn (17)
                       h11*X_w + h13*Y_W + h13
                   u = -----------------------
                       h31*X_w + h32*Y_W + h33
           
                       h21*X_w + h23*Y_W + h23
                   v = -----------------------
                       h31*X_w + h32*Y_W + h33
           
                eqn(8) below starting with "u + ...":
                 NOTE:it uses equation #4 of Ma et al. 2004 Table 2
                      #4: f_r = 1 + k_1*r^2 + k_2*r^4
                          and ud−u0 = (u−u0)*f_r
                      can edit to use the lower order equation #3:
                      #3: f_r = 1 + k_1*r + k_2*r^2, but would use column 3 of Table 2 of Ma et al. too
                 NOTE: [x,y,1] = A^-1 * [u, v, 1]
                 NOTE: the center subtracted [x,y,1] is [x_c, y_c, 1]
                 NOTE: [x_c, y_c, 1] = -(u-u_0)*[s_x,s_y] where s is the pixel size in mm
                 NOTE: [x_c, y_c] = (f/Z_C)*[X_c,Y_c] if ignoring R and t
                     ==>presumably want to estimate [x,y] from [X_c,Y_c,Z_c] and f.
                        knowing that the radial distortion is applied to the centered
                        camera coordinates, one must assume the x^2 + y^2 are
                        the centered (x_c^2 + y_c^2), else would
                              add [x_0,y_0] determined from A^-1 * [u_0, v_0, 1]
             
                 u + (u-u_0)*[k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2] = u_d
                 v + (v-v_0)*[k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2] = v_d
                
                 [k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2]*(u-u_0) + u = u_d
                 [k_1*(x^2 + y^2) + k_2*(x^2+y^2)^2]*(v-v_0) + v = v_d
                
                 [ [(x^2 + y^2)   (x^2+y^2)^2]*(u-u_0)   u ] * [k1] = u_d
                 [ [(x^2 + y^2)   (x^2+y^2)^2]*(v-v_0)   v ]   [k2] = v_d
                                                               [1 ]
                
                  ==> D = [ [(x^2 + y^2)   (x^2+y^2)^2]*(u-u_0)   u ]
                          [ [(x^2 + y^2)   (x^2+y^2)^2]*(v-v_0)   v ]
                          [ next point on 2 rows                    ]
                          [...                                      ]
          
                  ==> d =[u_d]
                         [v_d]
                         [u_d for next point]
                         [v_d for next point]
                         [...]

---------------------------------------------------------------------
Task 6: implement the method for parameter optimization

   goal: given all of the parameters estimated above as the initial estimates,
         use non-linear optimization to get the final estimated values.

         the objective function is the reproduction error composed of the observed
         points in the image plane and their predicted projections from the
         estimated parameters in the projection matrix and the measured points 
         in the world coordinate system.

         objective function L = summation_i=1_to_N( summation_j=1_to_n(
           || m_i_j - ~m(A, k1, k2, R_i, t_i, M_j) ||^2
           where N is the number of images, n is the number of features (a.k.a. points),
           m_i_j is the image i coordinate of feature j,
           ~m(...) is the projacetion given it's arguments,
           M_i = [Xw,Yw,1]T is the feature in world coordinate space with Zw=0

   time estimate: minimum 8 hours, maximum ?  day
  
   amount of time used:  3 days so far on background reading and algorithm details.

   state of completion: in the design stage. 

   comments:
      the notes below are following the Ma et al. 2003 algorithm, then other references
      in more detail.  5 methods are suggested.
      This task will implement algorithm (1)(a).

   details:  
          parameter optimization method:
            input: imageC, worldC, intrinsic, extrinsic, radial distortion
            output: intrinsic and exrinsic
            responsibilities:
              - given all of the parameters estimated above as the initial estimates,
                use non-linear optimization to get the final estimated values.

                the objective function is the reproduction error composed of the observed
                points in the image plane and their predicted projections from the
                estimated parameters in the projection matrix and the measured points 
                in the world coordinate system.

                objective function L = summation_i=1_to_N( summation_j=1_to_n(
                  || m_i_j - ~m(A, k1, k2, R_i, t_i, M_j) ||^2
                  where N is the number of images, n is the number of features (a.k.a. points),
                  m_i_j is the image i coordinate of feature j,
                  ~m(...) is the projetion given it's arguments,
                  M_i = [Xw,Yw,1]T is the feature in world coordinate space with Zw=0

                The authors use Matlab's fminunc which finds the minimum of an unconstrained
                multivariate function, it's using BFGS.

           for the objective function need to predict the image point [u,v,1]T to compare to
           the measured [u,v,1]T, so need to predict the distorted image coordinates
           [u_d,v_d,1]^T or undistort the observed image coordinates.

           to remove the distortion from the observed image points:
              // put x into camera coordinates reference frame:
              double[][] pix = MatrixUtil.multiply(cameraIntrInv, x);
              // remove radial distortion:
              pix = CameraCalibration.removeRadialDistortion(pix, rCoeffs[0], rCoeffs[1]);
              // transform back into image reference frame:
              pix = MatrixUtil.multiply(cameraIntr, cc);

           to minimize the objective function w/ non-linear optimization, could 
           (1) use Levenberg-Marquardt (L-M) over BFGS because it uses the Jacobian 
               and an estimated Hessian to inform step sizes and use L-M over 
               Gauss-Newton because it has a damping term which ensures 
               the positive definite of the coefficient matrix.
               
               A summary of the L-M algorithm  can be found in several sources.
               Insightful comments: a lecture by Danping Zou @Shanghai Jiao Tong University ,
               EE382-Visual localization & Perception, “Lecture 08- Nonlinear least square & RANSAC”
               http://drone.sjtu.edu.cn/dpzou/teaching/course/lecture07-08-nonlinear_least_square_ransac.pdf

               An L-M approach to minimizing the reprojection error of the perspective projection
               is outlined slightly differently from one another:

               (a) lecture notes of Gordon Wetzstein at Stanford University,
                   EE 267 Virtual Reality, "Course Notes: 6-DOF Pose Tracking with the VRduino"
               (b) Chapter 6 of Richard Szeliski 2010 "Computer Vision: Algorithms and Applications"

           (2) One could reform the projection equations as separate steps in a chain of transformations
               and iteratively minimize the robustified linearized reprojection errors for each step
               (Section 6.2.2 of Szeliski 2010 on pose) simmilar to back-propogation in neural 
               networks (Bishop 2006)..
               "The advantage of this chained set of transformations is that each one has a 
               simple partial derivative with respect both to its parameters and to its input. 
               Thus, once the predicted value of x ̃i has been computed based on the 3D point 
               location pi and the current values of the pose parameters (cj,qj,k), we can 
               obtain all of the required partial derivatives using the chain rule."
               ..."The one special case in this formulation that can be considerably simplified 
               is the computation of the rotation update. Instead of directly computing the 
               derivatives of the 3⇥3 rotation matrix R(q) as a function of the unit 
               quaternion entries, you can prepend the incremental rotation matrix deltaR(omega) 
               given in Equation (2.35) to the current rotation matrix and compute the
               partial derivative of the transform with respect to these parameters, which 
               results in a simple cross product of the backward chaining partial derivative 
               and the outgoing 3D vector (2.36)"
                  NOTE: perturbations are summarized in the Danping Zou lecture
                  http://drone.sjtu.edu.cn/dpzou/teaching/course/lecture07-08-nonlinear_least_square_ransac.pdf

                      write the projection equations as
                         x_i = f(X_i;R,t,K)
                      and iteratively minimize the robustified linearized reprojection errors
                         E_NLP = summation_i( rho( (d(f)/d(R))*deltaR + (d(f)/d(t))*deltat + (d(f)/d(K))*deltaK - resid_i)
                         where resid_i = x ̃i - xˆi is the current residual vector (2D error in predicted position) 
                         and the partial derivatives are with respect to the unknown pose parameters 
                         (rotation, translation, and optionally intrinsic calibration)
                      An easier to understand (and implement) version of the above non-linear regression 
                      problem can be constructed by re-writing the projection equations as a concatenation 
                      of simpler steps, each of which transforms a 4D homogeneous coordinate X_i by a 
                      simple transformation such as translation, rotation, or perspective division (Figure 6.5). 
                      The resulting projection equations can be written as
                         y_1 = f_T(X_i;c_j) = X_i - c_j,   where c_j is camera center
                         y_2 = f_R(y_1;q_j) = R(q_j)*y_1
                         y_3 = f_P(y_2) = y_2/z_2
                         x_i = f_C(y_3; k) = K(k) * y_3

                      Note that in these equations, we have indexed the camera centers cj and camera rotation 
                      quaternions q_j by an index j, in case more than one pose of the calibration object 
                      is being used (see also Section 7.4.) We are also using the camera center cj instead 
                      of the world translation t_j , since this is a more natural parameter to estimate.

                      The advantage of this chained set of transformations is that each one has a 
                      simple partial derivative with respect both to its parameters and to its input. 
                      Thus, once the predicted value of x ̃i has been computed based on the 3D point 
                      location X_i and the current values of the pose parameters (c_j,q_j,k), 
                      we can obtain all of the required partial derivatives using the chain rule
                          d(resid_i)/d(param_k) = (d(resid_i)/d(y_k))*(d(y_k)/d(param_k))
                          
                      The rotation partial derivative, espec. can be simplified:
                          eqn 2.35 d(R(ω)*v)/d(ω^T) = -[v]_x

                      see Section 4.2 of Gleicher & Witkin 1992 "Through-the-Lens Camera Control"


           (3) Alternatively, iteratively solving for parameters in an order suggested by the depndencies, essentially.

              looking at M contents to consider which parameters in which order in the optimization iterations:
              NOTE that instead of derivatives would use gradients estimated from finite differences for each iteration.

              M as m_1 is K * [R | t ] * X where t is actually -R*translation
                   m_2
                   m_3

                 [alpha  gamma  u_0 ]   [ r11  r12  r13  t_x ]   [ X ]
             M = [  0     beta  v_0 ] * [ r21  r22  r23  t_y ] * [ Y ]
                 [  0      0     1  ]   [ r31  r32  r33  t_z ]   [ 0 ]
                                                                 [ 1 ]
                 [alpha  gamma  u_0 ]   [ r11  r12  t_x ]   [ X ]
               = [  0     beta  v_0 ] * [ r21  r22  t_y ] * [ Y ]
                 [  0      0     1  ]   [ r31  r32  t_z ]   [ 1 ]

                 [ (alpha*r11 + gamma*r21 + u_0*r31)  (alpha*r12 + gamma*r22 + u_0*r32)  (alpha*t_x + gamma*t_y + u_0*t_z) ]
               = [ (beta*r21 + v_0*r31)               (beta*r22 + v_0*r32)               (beta*t_y + v_0*t_z)              ]
                 [ (r31)                              (r32)                              (t_z)                             ]

               m_3 then gives us [r31, r32, t_z]
               
               m21 = (beta*r21 + v_0*r31) where v_0 can be presumed to be known, and r31 is m31
               m22 = (beta*r22 + v_0*r32) with similar knowns and solved
               m23 = (beta*t_y + v_0*t_z) with similar knowns and solved
               ==> beta = (m21 - v_0*r31)/r21  has unknowns r21 and beta
               ==> beta = (m22 - v_0*r32)/r22  has unknowns r22 and beta
               ==> beta = (m23 - v_0*t_z)/t_y  has unknowns t_y and beta
                   so have 4 unknowns but only 3 data
                   not a closed loop solution, need stages of setting variables to fixed best values and
                     minimizing for the others.
                   -- considering: first optimizing for r21, r22, and t_y which solves m_3 and m_2
               ==> m11 = (alpha*r11 + gamma*r21 + u_0*r31) would then have unknowns alpha and r11 and gamma
               ==> m12 = (alpha*r12 + gamma*r22 + u_0*r32)                          alpha and r12 and gamma
               ==> m13 = (alpha*t_x + gamma*t_y + u_0*t_z)                          alpha and t_x and gamma
                   -- considering: next optimizing for r11, r12, t_x, alpha, and gamma which minimizes m_1


---------------------------------------------------------------------
Task 7: implement the extrinsic parameters solver of Moreno-Noguer et al. 2007

   goal: implement a fast accurate P-n-P algorithm.
         The Moerno-Noguer et al. 2007 algorithm is a non-iterative solution 
         to the PnP problem — the estimation of the pose of a calibrated 
         camera from n 3D-to-2D point correspondences—whose computational 
         complexity grows linearly with n.

   time estimate: minimum  hour, maximum  day
  
   amount of time used:

   state of completion: in design stage

   priority: low as Task 4 has same goal and is already implemented.

   comments:
       implementing an algorithm recommended in Section 6.2.1 of Szeliski as
       a fast reasonably accurate P-n-P algorithm to be used before refinement
       with an iterative solution.  
       The P-n-P algorithm determines the position and orientation of a camera given its
       intrinsic parameters and a set of n correspondences between
       3D points and their 2D projections.

       How does this compare to the method already implemented above from Ma et al. 2003?
       And both compared to the DLT method in the project as CameraPose.calculatePose()?

       haven't read the paper yet...
              
   details:  
      Moreno-Noguer et al. 2007  extrinsic parameters solver method:
          input: 
          output: estimated camera extrinsic parameters
          responsibilities:
---------------------------------------------------------------------
=====================================================================
