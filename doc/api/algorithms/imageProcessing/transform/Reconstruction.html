<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (21) on Tue Jul 29 11:59:29 PDT 2025 -->
<title>Reconstruction</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2025-07-29">
<meta name="description" content="declaration: package: algorithms.imageProcessing.transform, class: Reconstruction">
<meta name="generator" content="javadoc/ClassWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../script-dir/jquery-ui.min.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-ui.min.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">var pathtoroot = "../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Toggle navigation links"><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span></button>
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../index.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="nav-bar-cell1-rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html#class">Help</a></li>
</ul>
<ul class="sub-nav-list-small">
<li>
<p>Summary:</p>
<ul>
<li><a href="#nested-class-summary">Nested</a></li>
<li>Field</li>
<li><a href="#constructor-summary">Constr</a></li>
<li><a href="#method-summary">Method</a></li>
</ul>
</li>
<li>
<p>Detail:</p>
<ul>
<li>Field</li>
<li><a href="#constructor-detail">Constr</a></li>
<li><a href="#method-detail">Method</a></li>
</ul>
</li>
</ul>
</div>
<div class="sub-nav">
<div id="navbar-sub-list">
<ul class="sub-nav-list">
<li>Summary:&nbsp;</li>
<li><a href="#nested-class-summary">Nested</a>&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor-summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method-summary">Method</a></li>
</ul>
<ul class="sub-nav-list">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor-detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</div>
<div class="nav-list-search"><a href="../../../search.html">SEARCH</a>
<input type="text" id="search-input" disabled placeholder="Search">
<input type="reset" id="reset-button" disabled value="reset">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="sub-title"><span class="package-label-in-type">Package</span>&nbsp;<a href="package-summary.html">algorithms.imageProcessing.transform</a></div>
<h1 title="Class Reconstruction" class="title">Class Reconstruction</h1>
</div>
<div class="inheritance" title="Inheritance Tree"><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">java.lang.Object</a>
<div class="inheritance">algorithms.imageProcessing.transform.Reconstruction</div>
</div>
<section class="class-description" id="class-description">
<hr>
<div class="type-signature"><span class="modifiers">public class </span><span class="element-name type-name-label">Reconstruction</span>
<span class="extends-implements">extends <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">Object</a></span></div>
<div class="block">This class has/will have methods for Structure from Motion and 3-D point reconstruction.
 given correspondence between two images calculate the camera
 parameters as intrinsic and extrinsic parameters,
 and the real world position.

 TODO: implement affine reconstruction for the case of pure translation,
     see Example 6.6 of Ma, Soatto, Kosecká, and Sastry 2012, "An Invitation to 3-D Vision"".
     For the case of pure rotation, see Example 6.10.

 Euler rotations:

        about z-axis (yaw):           about x-axis (roll):       about the y-axis (pitch):
            | cos φ   -sin φ    0 |    |    1       0       0 |  |  cos ψ    0  sin ψ |
            | sin φ    cos φ    0 |    |    0   cos θ   sin θ |  |      0    1      0 |
            |     0        0    1 |    |    0  -sin θ   cos θ |  | -sin ψ    0  cos ψ |        

 useful reading:
  http://www.cs.cmu.edu/~16385/s17/Slides/12.5_Reconstruction.pdf
  Fig 1.8 of "Computing Intrinsic Images" by Aloimonos 1986 for a snapshot in time of c.v. algorithms

   Motion - the rotation and translation of an object in front of a camera.
            sometimes represented as the 3X4 projection matria P = |R | t|
   Shape - the local surface orientation where surface orientation is 
           usually the surface normal vector.
           sometimes represented by X, the 3-D coordinates w.r.t. a world reference system.
           From "Computing Intrinsic Images" by Aloimonos 1986
             perspective projection is pinhole camera.
             Under orthographic projection, the image coordinates of a point 
             are equal to the corresponding 3-D coordinates, i.e. (x.y ) =(X,Y)
             and we do not know its depth.
   Depth - the Z-coordinate of a 3-D object in the world coordinate system.

   test datasets:
      https://www.cs.cmu.edu/afs/cs/project/vision/vasc/idb/www/html_permanent/index.html
      http://www.cs.cmu.edu/afs/cs/project/cil/www/v-images.html


 TODO: implement more of Chap 11 of MASKS (Ma, Soatto, Kosecká, and Sastry 2012, 
 "An Invitation to 3-D Vision")
 
 </pre></div>
<dl class="notes">
<dt>Author:</dt>
<dd>nichole</dd>
</dl>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ======== NESTED CLASS SUMMARY ======== -->
<li>
<section class="nested-class-summary" id="nested-class-summary">
<h2>Nested Class Summary</h2>
<div class="caption"><span>Nested Classes</span></div>
<div class="summary-table three-column-summary">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Class</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color"><code>static class&nbsp;</code></div>
<div class="col-second even-row-color"><code><a href="Reconstruction.MotionAndStructure.html" class="type-name-link" title="class in algorithms.imageProcessing.transform">Reconstruction.MotionAndStructure</a></code></div>
<div class="col-last even-row-color">&nbsp;</div>
<div class="col-first odd-row-color"><code>static class&nbsp;</code></div>
<div class="col-second odd-row-color"><code><a href="Reconstruction.OrthographicProjectionResults.html" class="type-name-link" title="class in algorithms.imageProcessing.transform">Reconstruction.OrthographicProjectionResults</a></code></div>
<div class="col-last odd-row-color">&nbsp;</div>
<div class="col-first even-row-color"><code>static class&nbsp;</code></div>
<div class="col-second even-row-color"><code><a href="Reconstruction.ParaperspectiveProjectionResults.html" class="type-name-link" title="class in algorithms.imageProcessing.transform">Reconstruction.ParaperspectiveProjectionResults</a></code></div>
<div class="col-last even-row-color">&nbsp;</div>
<div class="col-first odd-row-color"><code>static class&nbsp;</code></div>
<div class="col-second odd-row-color"><code><a href="Reconstruction.ProjectionResults.html" class="type-name-link" title="class in algorithms.imageProcessing.transform">Reconstruction.ProjectionResults</a></code></div>
<div class="col-last odd-row-color">&nbsp;</div>
<div class="col-first even-row-color"><code>static class&nbsp;</code></div>
<div class="col-second even-row-color"><code><a href="Reconstruction.ReconstructionResults.html" class="type-name-link" title="class in algorithms.imageProcessing.transform">Reconstruction.ReconstructionResults</a></code></div>
<div class="col-last even-row-color">&nbsp;</div>
</div>
</section>
</li>
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<li>
<section class="constructor-summary" id="constructor-summary">
<h2>Constructor Summary</h2>
<div class="caption"><span>Constructors</span></div>
<div class="summary-table two-column-summary">
<div class="table-header col-first">Constructor</div>
<div class="table-header col-last">Description</div>
<div class="col-constructor-name even-row-color"><code><a href="#%3Cinit%3E()" class="member-name-link">Reconstruction</a>()</code></div>
<div class="col-last even-row-color">&nbsp;</div>
</div>
</section>
</li>
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Method Summary</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">All Methods</button><button id="method-summary-table-tab1" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab1', 3)" class="table-tab">Static Methods</button><button id="method-summary-table-tab4" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab4', 3)" class="table-tab">Concrete Methods</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel">
<div class="summary-table three-column-summary" aria-labelledby="method-summary-table-tab0">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Method</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>(package private) static <a href="Reconstruction.OrthographicProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.OrthographicProjectionResults</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#_DoNotUseThisCalculateAffineReconstruction(double%5B%5D%5B%5D,int)" class="member-name-link">_DoNotUseThisCalculateAffineReconstruction</a><wbr>(double[][]&nbsp;x,
 int&nbsp;mImages)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOT READY FOR USE.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.OrthographicProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.OrthographicProjectionResults</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateAffineReconstruction(double%5B%5D%5B%5D,int)" class="member-name-link">calculateAffineReconstruction</a><wbr>(double[][]&nbsp;x,
 int&nbsp;mImages)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOTE: not ready for use yet.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.ParaperspectiveProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ParaperspectiveProjectionResults</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateParaperspectiveReconstruction(double%5B%5D%5B%5D,int)" class="member-name-link">calculateParaperspectiveReconstruction</a><wbr>(double[][]&nbsp;x,
 int&nbsp;mImages)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOT READY FOR USE.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static double[][]</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateProjectiveHomographyWithLeastSquares(double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D)" class="member-name-link">calculateProjectiveHomographyWithLeastSquares</a><wbr>(double[][]&nbsp;x1P,
 double[][]&nbsp;x2P,
 double[][]&nbsp;fm,
 double[]&nbsp;e2)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOT READY FOR USE YET.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Camera.CameraExtrinsicParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraExtrinsicParameters</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateProjectiveMotion(double%5B%5D%5B%5D,double%5B%5D%5B%5D)" class="member-name-link">calculateProjectiveMotion</a><wbr>(double[][]&nbsp;x1,
 double[][]&nbsp;x2)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">estimate the extrinsic camera matrix P2 (notion) assuming P1 is[I|0]
 using epipolar geometry given correspondence points between 2 images (structure).</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.ProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ProjectionResults</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateProjectiveReconstruction(double%5B%5D%5B%5D,int)" class="member-name-link">calculateProjectiveReconstruction</a><wbr>(double[][]&nbsp;x,
 int&nbsp;mImages)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOT READY FOR USE
 TODO: proof read the algorithm and write test for this.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.ReconstructionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ReconstructionResults</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateReconstruction(algorithms.imageProcessing.transform.Camera.CameraParameters,algorithms.imageProcessing.transform.Camera.CameraParameters,double%5B%5D%5B%5D,double%5B%5D%5B%5D)" class="member-name-link">calculateReconstruction</a><wbr>(<a href="Camera.CameraParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraParameters</a>&nbsp;camera1,
 <a href="Camera.CameraParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraParameters</a>&nbsp;camera2,
 double[][]&nbsp;x1,
 double[][]&nbsp;x2)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">given 2 sets of correspondence from 2 different images taken from
 2 cameras whose intrinsic and extrinsic parameters are known,
 determine the world scene coordinates of the correspondence points.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.ReconstructionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ReconstructionResults</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#calculateUsingEssentialMatrix(double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D%5B%5D)" class="member-name-link">calculateUsingEssentialMatrix</a><wbr>(double[][]&nbsp;intr1,
 double[][]&nbsp;intr2,
 double[][]&nbsp;x1,
 double[][]&nbsp;x2)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">NOTE: the method needs improvement to choose the best 2 solutions, meanwhile prefer to use
 calculateProjectiveReconstruction(double[][] x1c, double[][] x2c).</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#isInFrontOfCamera(double%5B%5D,double%5B%5D%5B%5D,double%5B%5D)" class="member-name-link">isInFrontOfCamera</a><wbr>(double[]&nbsp;x,
 double[][]&nbsp;p,
 double[]&nbsp;XW)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">&nbsp;</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>(package private) static void</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#populateWithDet1Rs(double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D%5B%5D,double%5B%5D%5B%5D)" class="member-name-link">populateWithDet1Rs</a><wbr>(double[][]&nbsp;u,
 double[][]&nbsp;vT,
 double[][]&nbsp;r1Out,
 double[][]&nbsp;r2Out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">&nbsp;</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code>static <a href="Reconstruction.MotionAndStructure.html" title="class in algorithms.imageProcessing.transform">Reconstruction.MotionAndStructure</a>[]</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4"><code><a href="#solve4RotationAndDirection(double%5B%5D%5B%5D,double%5B%5D%5B%5D)" class="member-name-link">solve4RotationAndDirection</a><wbr>(double[][]&nbsp;x1c,
 double[][]&nbsp;x2c)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1 method-summary-table-tab4">
<div class="block">estimate the rotation and direction of motion using planar homography.</div>
</div>
</div>
</div>
</div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-java.lang.Object">Methods inherited from class&nbsp;java.lang.<a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">Object</a></h3>
<code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#clone()" title="class or interface in java.lang" class="external-link">clone</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#equals(java.lang.Object)" title="class or interface in java.lang" class="external-link">equals</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#finalize()" title="class or interface in java.lang" class="external-link">finalize</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#getClass()" title="class or interface in java.lang" class="external-link">getClass</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#hashCode()" title="class or interface in java.lang" class="external-link">hashCode</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#notify()" title="class or interface in java.lang" class="external-link">notify</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#notifyAll()" title="class or interface in java.lang" class="external-link">notifyAll</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#toString()" title="class or interface in java.lang" class="external-link">toString</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait()" title="class or interface in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait(long)" title="class or interface in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Object.html#wait(long,int)" title="class or interface in java.lang" class="external-link">wait</a></code></div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<li>
<section class="constructor-details" id="constructor-detail">
<h2>Constructor Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="&lt;init&gt;()">
<h3>Reconstruction</h3>
<div class="member-signature"><span class="modifiers">public</span>&nbsp;<span class="element-name">Reconstruction</span>()</div>
</section>
</li>
</ul>
</section>
</li>
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Method Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="calculateReconstruction(algorithms.imageProcessing.transform.Camera.CameraParameters,algorithms.imageProcessing.transform.Camera.CameraParameters,double[][],double[][])">
<h3>calculateReconstruction</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.ReconstructionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ReconstructionResults</a></span>&nbsp;<span class="element-name">calculateReconstruction</span><wbr><span class="parameters">(<a href="Camera.CameraParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraParameters</a>&nbsp;camera1,
 <a href="Camera.CameraParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraParameters</a>&nbsp;camera2,
 double[][]&nbsp;x1,
 double[][]&nbsp;x2)</span>
                                                                    throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">given 2 sets of correspondence from 2 different images taken from
 2 cameras whose intrinsic and extrinsic parameters are known,
 determine the world scene coordinates of the correspondence points.
 This method simply uses triangulation on each correspondence pair.
 <pre>
 following CMU lectures of Kris Kitani at 
 http://www.cs.cmu.edu/~16385/s17/Slides/12.5_Reconstruction.pdf
 
 </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>camera1</code> - image 1 camera matrices of intrinsic and extrinsic parameters.
 the size is 3 x 4.</dd>
<dd><code>camera2</code> - image 2 camera matrices of intrinsic and extrinsic parameters.
 the size is 3 x 4.</dd>
<dd><code>x1</code> - the image 1 set of correspondence points in image coordinates.  format is 3 x N where
 N is the number of points.</dd>
<dd><code>x2</code> - the image 2 set of correspondence points in image coordinates.  format is 3 x N where
 N is the number of points.</dd>
<dt>Returns:</dt>
<dd>the world scene coordinates and the intrinsic and extrinsic
 camera matrices (the later were given to the code, but are convenient to return in results).</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateProjectiveMotion(double[][],double[][])">
<h3>calculateProjectiveMotion</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Camera.CameraExtrinsicParameters.html" title="class in algorithms.imageProcessing.transform">Camera.CameraExtrinsicParameters</a></span>&nbsp;<span class="element-name">calculateProjectiveMotion</span><wbr><span class="parameters">(double[][]&nbsp;x1,
 double[][]&nbsp;x2)</span>
                                                                  throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">estimate the extrinsic camera matrix P2 (notion) assuming P1 is[I|0]
 using epipolar geometry given correspondence points between 2 images (structure).
 The essential matrix contains information about the relative position
 T and orientation R between 2 cameras (the camera pose).
 The best of 4 solutions constructed from the essential matrix is returned
 where the best is defined using the volume of each point's epipolar plane
 with respect to the signs of the scales (scale in x = scale * P * X).

 The projective calibration can be upgraded to
     affine (parallelism preserved) and Euclidean (parallelism and orthogonality preserved) 
     reconstructions.
     To upgrade to an affine projection, need 3 vanishing points
     (see Section 9.2.2 of Belongie lec 9).
     To directly upgrade from projective to euclidean projection, need
     5 ground truth points in general position, that is, no 4 points
     are coplanar (see Section 9.3 of Belongie lec 9).
 NOTE: this solution is fine for cases with no noise, otherwise, the
 results should be the initial values for a non-linear optimization method.

 <pre>
 MASKS chap. 5 and their code essentialDiscrete.m
 </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x1</code> - the camera 1 set of image correspondence points in the reference frame of the camera.
 format is 3 x N where N is the number of points.
 NOTE: one can guess at the camera intrinsic matrix if needed, in order to calibrate the camera.
 MASKS gives advice in Algorithm 11.6 step 1.</dd>
<dd><code>x2</code> - the camera 2 set of image correspondence points in the reference frame of the camera.
 format is 3 x N where N is the number of points.
 NOTE: one can guess at the camera intrinsic matrix if needed, in order to calibrate the camera.
 MASKS gives advice in Algorithm 11.6 step 1.</dd>
<dt>Returns:</dt>
<dd>the estimated projections P1 and P2 and the objects locations as 3-D points.</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="isInFrontOfCamera(double[],double[][],double[])">
<h3>isInFrontOfCamera</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">boolean</span>&nbsp;<span class="element-name">isInFrontOfCamera</span><wbr><span class="parameters">(double[]&nbsp;x,
 double[][]&nbsp;p,
 double[]&nbsp;XW)</span></div>
</section>
</li>
<li>
<section class="detail" id="solve4RotationAndDirection(double[][],double[][])">
<h3>solve4RotationAndDirection</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.MotionAndStructure.html" title="class in algorithms.imageProcessing.transform">Reconstruction.MotionAndStructure</a>[]</span>&nbsp;<span class="element-name">solve4RotationAndDirection</span><wbr><span class="parameters">(double[][]&nbsp;x1c,
 double[][]&nbsp;x2c)</span>
                                                                      throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">estimate the rotation and direction of motion using planar homography.
 This is also called Projective Structure From Motion for the
 Two-camera case.   it's a distorted version of euclidean 3d.
 4 solutions are returned and the user should decide between them by which produces
 the larger number of points in front of the camera via triangulation of each pair of
 correspondence.

 The results can be used to form a homography and then the essential matrix.
 H2 = (R + ((1/d)*outerproduct(T, N^T))
             and then
             EM = essential matrix = [T]_x * H2

     To upgrade to an affine projection, need 3 vanishing points
     (see Example 6.5 of MASKS).
     To directly upgrade from projective to euclidean projection, need
     5 ground truth points in general position (a.k.a. control points, points in WCS), that is, no 4 points
     are coplanar (see Section 9.3 of Belongie lec 9. and MASKS chap 5 "Historical Notes" and Algorithm 11.7).

 <pre>
 following "An Invitation to 3-D Vision" by Ma,  Soatto,  Kosecká, and Sastry
 noted as MASKS.
 Algorithm 5.2 in Chapter 5 and their code homography2Motion.m from
 https://cs.gmu.edu/~kosecka/bookcode.html
 which is free for non-commercial use.
 </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x1c</code> - the image 1 set of correspondence points in camera coordinates.  format is 3 x N where
 N is the number of points.
 NOTE: since intrinsic parameters are not known, users of this method should
 presumably center the coordinates in some manner
 (e.g. unit standard normalization) since internally
 an identity matrix is used for K.</dd>
<dd><code>x2c</code> - the image 1 set of correspondence points in camera coordinates.  format is 3 x N where
      * N is the number of points.
      * NOTE: since intrinsic parameters are not known, users of this method should
      * presumably center the coordinates in some manner
      * (e.g. unit standard normalization) since internally
      * an identity matrix is used for K.</dd>
<dt>Returns:</dt>
<dd>the 2 solutions for the set of {estimated projections P1 and P2, and the triangulation of x1 and x2 in WCS}</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateProjectiveReconstruction(double[][],int)">
<h3>calculateProjectiveReconstruction</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.ProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ProjectionResults</a></span>&nbsp;<span class="element-name">calculateProjectiveReconstruction</span><wbr><span class="parameters">(double[][]&nbsp;x,
 int&nbsp;mImages)</span>
                                                                          throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOT READY FOR USE
 TODO: proof read the algorithm and write test for this.
 for the case of un-calibrated cameras viewing the same scene features,
 recover the 3-D coordinates in WCS and the projection matrices 
 from pairs of corresponding
 un-calibrated image points, that is, points in the image reference frame in pixels.
 
 The method implements the Sturm <span class="invalid-tag">invalid input: '&amp;'</span> Triggs 1996 algorithm: 
     "a method for the recovery of projective shape and motion from multiple 
     images of a scene by the factorization of a matrix containing the images 
     of all points in all views. This factorization is only possible when the
     image points are correctly scaled. The major technical contribution of 
     this paper is a practical method for the recovery of these scalings, 
     using only fundamental matrices and epipoles estimated from the image data."
     "[it is a] closed form solutions, not iterative bundle-adjustment..."
 <pre>
 references:
 
 Sturm and Triggs 1996, 
    "A Factorization Based Algorithm for Multi-Image Projective Structure and Motion"
     https://link.springer.com/content/pdf/10.1007/3-540-61123-1_183.pdf

    see also proj_recons_fsvd.m from http://lear.inrialpes.fr/people/triggs/src/
    which has a very liberal copyright in the file COPYRIGHT
    Copyright Bill Triggs (http://www.inrialpes.fr/movi/people/Triggs),
    INRIA (http://www.inria.fr) and CNRS (http://www.cnrs.fr),
    1995-2002. All rights reserved.

    You may use and distribute [*] this work with or without modification,
    for any purpose and without fee or royalty, subject to the following
    conditions:
       (see file COPYRIGHT)

 </pre>
 
 NOTE: Sturm <span class="invalid-tag">invalid input: '&amp;'</span> Triggs 1996 state in their code, "% The projective output 
     frame is numerically well-conditioned, but otherwise *completely* 
     arbitrary. It has *no* relation to any Euclidean frame.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x</code> - the image coordinates of feature correspondences in 2 or more
 images.  format is 2 X (nImages * nFeatures) where row 0 holds the x-coordinates
 and row 1 holds the y-coordinates and each image's features are given
 before the next and the features are ordered in the same manner within
 all images.
 for example: row 0 = [img_0_feature_0, ... img_0_feature_n-1, ... img_m-1_feature_0,...
     img_m-1_feature_n-1].</dd>
<dd><code>mImages</code> - the number of images in x.</dd>
<dt>Returns:</dt>
<dd>the estimated projections P1 and P2 and the objects locations as 3-D points;</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateUsingEssentialMatrix(double[][],double[][],double[][],double[][])">
<h3>calculateUsingEssentialMatrix</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.ReconstructionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ReconstructionResults</a></span>&nbsp;<span class="element-name">calculateUsingEssentialMatrix</span><wbr><span class="parameters">(double[][]&nbsp;intr1,
 double[][]&nbsp;intr2,
 double[][]&nbsp;x1,
 double[][]&nbsp;x2)</span>
                                                                          throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOTE: the method needs improvement to choose the best 2 solutions, meanwhile prefer to use
 calculateProjectiveReconstruction(double[][] x1c, double[][] x2c).
 given correspondence between two images in image coordinates calculate 
 the extrinsic camera parameters and the 3-D points.
 
 This method calculates the essential matrix and uses the SVD of it to
 extract the translation and possible rotation matrices which are
 filtered to find the best while calculating triangulation for each point.
 
 Note that the absolute translation between the two cameras can never be 
 recovered from pure image measurements alone, regardless of how many 
 cameras or points are used as ground control points are
 needed.
 <pre>
 following CMU lectures of Kris Kitani at 
     http://www.cs.cmu.edu/~16385/s17/Slides/12.5_Reconstruction.pdf
     Szeliski 2010, Chapter 7, and eqn (7.25).
     Ma, Soatto, Kosecká, and Sastry 2012, "An Invitation to 3-D Vision", pg 121 
 </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>intr1</code> - intrinsic camera matrix for image 1 in units of pixels.</dd>
<dd><code>intr2</code> - intrinsic camera matrix for image 2 in units of pixels.</dd>
<dd><code>x1</code> - the image 1 set of correspondence points in image reference frame.
 format is 3 x N where N is the number of points.</dd>
<dd><code>x2</code> - the image 2 set of correspondence points in image reference frame.
 format is 3 x N where N is the number of points.</dd>
<dt>Returns:</dt>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateAffineReconstruction(double[][],int)">
<h3>calculateAffineReconstruction</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.OrthographicProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.OrthographicProjectionResults</a></span>&nbsp;<span class="element-name">calculateAffineReconstruction</span><wbr><span class="parameters">(double[][]&nbsp;x,
 int&nbsp;mImages)</span>
                                                                                  throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOTE: not ready for use yet.
 
 TODO: proof read the algorithm and write test for this.
 for the case where the cameras are viewing small, distant scenes,
 recover the 3-D coordinates in WCS and the rotation matrices 
 from pairs of corresponding
 un-calibrated image points, that is, points in the image reference frame in pixels.
 assumes an orthographic camera model.
 can use the orthographic camera model when
    (the average distance of an object from the camera) 
     .geq. 10*(the average width of the object (measured along the optical axis of the camera).
 <pre>
 references:
 
 lecture 16 notes from Serge Belongie lectures from Computer Vision II, CSE 252B, USSD
 http://www-cse.ucsd.edu/classes/sp04/cse252b/notes/lec16/lec16.pdf
 
 lectures of Deva Ramanan at http://16720.courses.cs.cmu.edu/lec/sfm.pdf
 .:w
 
 Tomasi <span class="invalid-tag">invalid input: '&amp;'</span> Kanade 1991, "Shape and motion from image streams under 
 orthography: a factorization method", International journal of computer vision 
 
  Morita and Kanade 1997 for solving Q.
         T. Morita and T. Kanade, A Sequential Factorization Method for Recovering Shape and Motion
         from Image Streams, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 19,
         no.8, pp.858-867, Aug 1997  (1994?)

 Higham, 1988, “Computing a Nearest Symmetric Positive Semidefinite Matrix,” 
    Linear Algebra and Appl., 103:103-118, 1988
 
 a great summary of the above:
 http://note.sonots.com/SciSoftware/Factorization.html#cse252b
 http://note.sonots.com/?plugin=attach<span class="invalid-tag">invalid input: '&amp;refer'</span>=SciSoftware%2FFactorization<span class="invalid-tag">invalid input: '&amp;openfile'</span>=Factorization.pdf
 
 and a derivation of the geometry of the tracking equation:
 Birchfield 1997, "Derivation of Kanade-Lucas-Tomasi Tracking Equation"
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.413<span class="invalid-tag">invalid input: '&amp;rep'</span>=rep1<span class="invalid-tag">invalid input: '&amp;type'</span>=pdf
 </pre>
 NOTE: could overload this method to enable handling of occlusion 
 following Section 5 of Tomasi <span class="invalid-tag">invalid input: '&amp;'</span> Kanade 1991, but might want to alter the
 algorithm to use geometric median in place of centroid so that the
 "centers" are not as affected by removing or adding a point.
 NOTE: comments from Poelman <span class="invalid-tag">invalid input: '&amp;'</span> Kanade 1992:
 Orthographic projection does not account for the apparent change in size 
 of an object as it moves toward or away from the camera, nor the different 
 angle from which an object is viewed as it moves parallel to the image plane.
 NOTE: consider implementing Section 3.3 Sequential Factorization Algorithm
 from the Morita <span class="invalid-tag">invalid input: '&amp;'</span> Kanade 1997 paper (1994?)</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x</code> - the image coordinates of feature correspondences in 2 or more
 images.  format is 2 X (nImages * nFeatures) where row 0 holds the x-coordinates
 and row 1 holds the y-coordinates and each image's features are given
 before the next and the features are ordered in the same manner within
 all images.
 for example: row 0 = [img_0_feature_0, ... img_0_feature_n-1, ... img_m-1_feature_0,...
     img_m-1_feature_n-1].</dd>
<dd><code>mImages</code> - the number of images in x.</dd>
<dt>Returns:</dt>
<dd>the estimated projections P1 and P2 and the objects locations as 3-D points;</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="_DoNotUseThisCalculateAffineReconstruction(double[][],int)">
<h3>_DoNotUseThisCalculateAffineReconstruction</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Reconstruction.OrthographicProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.OrthographicProjectionResults</a></span>&nbsp;<span class="element-name">_DoNotUseThisCalculateAffineReconstruction</span><wbr><span class="parameters">(double[][]&nbsp;x,
 int&nbsp;mImages)</span>
                                                                                        throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOT READY FOR USE.
 a look at enforcing orthonormal rotation</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x</code> - </dd>
<dd><code>mImages</code> - </dd>
<dt>Returns:</dt>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateParaperspectiveReconstruction(double[][],int)">
<h3>calculateParaperspectiveReconstruction</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type"><a href="Reconstruction.ParaperspectiveProjectionResults.html" title="class in algorithms.imageProcessing.transform">Reconstruction.ParaperspectiveProjectionResults</a></span>&nbsp;<span class="element-name">calculateParaperspectiveReconstruction</span><wbr><span class="parameters">(double[][]&nbsp;x,
 int&nbsp;mImages)</span>
                                                                                              throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOT READY FOR USE.
 for the case where the cameras are viewing small, distant scenes,
 recover the 3-D coordinates in WCS and the projection matrices 
 from pairs of corresponding
 un-calibrated image points, that is, points in the image reference frame in pixels.
 assumes a para-perspective camera model.

 Input set of P feature point coordinates (x_f_p,y_f_p) , for each of 
 the F frames of the image sequence. From this information, our goal is 
 to recover the estimated shape of the object, given by the position 
 s_P, of every point, and the estimated motion of the
     camera, given by iHat_f, jHat_f, kHat_f for each frame in the sequence. 
     Rather than recover iHat_f in world coordinates, we generally recover 
     the three.eparate components tHat_f dot iHat_f, tHat_f dot jHat_f,
     tHat_f dot kHat_f.


     <pre>
      references:

     Poelman <span class="invalid-tag">invalid input: '&amp;'</span> Kanade 1997 (1994), "A Paraperspective Factorization Method for Shape 
     and Motion Recovery" 

     Description from Poelman <span class="invalid-tag">invalid input: '&amp;'</span> Kanade:

     Each feature point p that we track corresponds to a single world point, 
      located at position s. in some fixed world coordinate system.

      Each image f was taken at some specific camera orientation, which we 
      describe by the orthonormal unit vectors i_f, j_f and k_f 
      where kf_ points along the camera's line of sight, 
      i_f corresponds to the camera image plane's x-axis, 
      and j_f corresponds to the camera image's y-axis.

      t_f is a vector pointing from the origin of the fixed world coordinate system
      to the camera's focal plane.  it's the position of the camera in each fram f.

     </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x</code> - the image coordinates of feature correspondences in 2 or more
 images.  format is 2 X (nImages * nFeatures) where row 0 holds the x-coordinates
 and row 1 holds the y-coordinates and each image's features are given
 before the next and the features are ordered in the same manner within
 all images.
 for example: row 0 = [img_0_feature_0, ... img_0_feature_n-1, ... img_m-1_feature_0,...
     img_m-1_feature_n-1].</dd>
<dd><code>mImages</code> - the number of images in x.</dd>
<dt>Returns:</dt>
<dd>the estimated projections P1 and P2 and the objects locations as 3-D points;</dd>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="calculateProjectiveHomographyWithLeastSquares(double[][],double[][],double[][],double[])">
<h3>calculateProjectiveHomographyWithLeastSquares</h3>
<div class="member-signature"><span class="modifiers">public static</span>&nbsp;<span class="return-type">double[][]</span>&nbsp;<span class="element-name">calculateProjectiveHomographyWithLeastSquares</span><wbr><span class="parameters">(double[][]&nbsp;x1P,
 double[][]&nbsp;x2P,
 double[][]&nbsp;fm,
 double[]&nbsp;e2)</span>
                                                                throws <span class="exceptions">no.uib.cipr.matrix.NotConvergedException</span></div>
<div class="block">NOT READY FOR USE YET.  Looks like there's an error in the last column of the result as
 the numbers are too large.

 calculates the homography as the canonical pose for the un-calibrated camera
 (the projective projection as the 2nd image's projection
 in the canonical decomposition, pg 189 of MASKS).
 <pre>
 The homography H in [x2]_x*H*x1 = [x2]_x*( ([e2]_x)^T * F + e2*v^T)*x1 ~ 0
 where [b]_x is the skew-symmetric matrix of vector b.

 the skew symmetric matrix multiplication replaces the cross product.
 x2 cross H*x1 ~ 0.

 Details from Chapter 6 of Ma, Soatto, Kosecka,<span class="invalid-tag">invalid input: '&amp;'</span> Sastry (MASKS)
 "An Invitation to Computer Vision, From Images to Geometric Models"

 let X' = K*X and T'=K*T where K is the intrinsic camera parameters matrix.

 from euclidean transformation, we can derive the
 epipolar constraint:  x2'^T * [T']_x * K *R * K^-1 * x1' = 0

 x2^T*[T]_x*R*x1=0 <span class="invalid-tag">invalid input: '&lt;'</span>==&gt; x2'^T * [T']_x * K *R * K^-1 * x1' = 0

     F = K^-T * [T]_x * R * K^-1 (when K=I, F=E)
            = [T']_x * K * R * K^-1 if det(K)=1, else it's approx (up to a scale factor)

    epipoles e2^T*F = 0, F*e1 = 0.
         e2 = K*T
         e1 = K*R^T*T

    epipolar constraint for uncalibrated cameras:
        x2'^T * [T']_x * K *R * K^-1 * x1' = x2'^T * [T']_x * (K*R*K^-1 + T'*v^T)*x1'
        = x2'^T * [T']_x * R' * x1'
           where v is an arbitrary vector

    since F = [T']_x * K *R * K^-1,
       fitting for the projection |(K*R*K^-1 + T'*v^T), v_4*T'|
    one can then approximate the uncalibrated camera pose.
     choosing solution  this is a 4-parameter family of ambiguous decompositions.
    pg 187 of MASKS.
    This method implements point 4 in algorithm 11.9 on pg 405, Section 11.5 of MASKS.    
 </pre></div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>x1P</code> - the image 1 (a.k.a. left) half of correspondence points. format is 3 x N
 where N is the number of points. NOTE: since intrinsic parameters are not
 known, users of this method should presumably center the coordinates in
 some manner (e.g. subtract the image center or centroid of points) since
 internally an identity matrix is used for K.</dd>
<dd><code>x2P</code> - the image 2 (a.k.a. right) half of correspondence points. format is 3 x N where
 N is the number of points. NOTE: since intrinsic parameters are not
 known, users of this method should presumably center the coordinates in
 some manner (e.g. subtract the image center or centroid of points).</dd>
<dd><code>fm</code> - the fundamental matrix.  size is 3X3.</dd>
<dd><code>e2</code> - the left null space in the left singular vector of F.
 it's the last column of svd(fm).u and represents the location of
 the image 1 optical center (a.k.a. camera center).
 The epipole is the point where the baseline (the line joining the two
 camera centers ol, O2) intersects the image plane in each view,
 e2^T*F=0.  e2 = K*T where T is translation vector between cameras
 (a.k.a. the extrinsic camera parameter called translation).
 (NOTE: e1=K*R^T*T where R and T are extrinsic camera rotation and translation).</dd>
<dt>Returns:</dt>
<dt>Throws:</dt>
<dd><code>no.uib.cipr.matrix.NotConvergedException</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="populateWithDet1Rs(double[][],double[][],double[][],double[][])">
<h3>populateWithDet1Rs</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type">void</span>&nbsp;<span class="element-name">populateWithDet1Rs</span><wbr><span class="parameters">(double[][]&nbsp;u,
 double[][]&nbsp;vT,
 double[][]&nbsp;r1Out,
 double[][]&nbsp;r2Out)</span></div>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
</main>
</div>
</div>
</body>
</html>
