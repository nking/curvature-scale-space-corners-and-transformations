terms
=============
row major format:       a[row][col]; this is the default used in c.s.

identity matrix:        all items are zero except along diagonal upper left to
                        lower right, which are ones.
diagonal matrix:        all items are zero except the diagonal from upper left
                        to lower right
singular equations:     have no soln or inf many solutions.  
                        there are fewer equations than unknowns when reduced
non-singular equations: have exactly one solution

"inner" product is a row times a column, a.k.a. dot product
"outer" product is a column times a row (produces a large matrix of dim n_row x n_col

inverse matrix: may or may not exist.  has to be a square matrix.
                A^-1 x A = I, where I is the identity matrix.
                -- an inverse matrix has n pivots remaining after elimination,
                where pivot is the leftmost non-zero variable.
                -- after elimination, next test for possible invertibility is 
                that the determinant is not zero

                if A is invertible, then A * x = b can be solved as x = A^-1 * b

                (A * B)^-1 = B^-1 * A^-1

rank : the number of pivots in the reduced echelon matrix, that is the number
       of solved variables (the free variables do not have a number in column
       and so can be any value while the pivot are solvable by back substitution)

       for a matrix of size m x n:
         for rank r == m  and  r == n  square an invertible   A*x=b   has 1 soln
                  r == m  and  r < n                                  has inf soln
                       m is numRows which is num eqns
                       n is numCols which is num vars 
                       so (m < n) is over determined 
                       and columns are dependent
                  r < m  and  r == n                                  has 0 or 1 soln
                       so (m > n) so under determined
                       and columns might or might not be dependent
                       but echelon format reveals pivots as independent
                  r < m  and  r < n                                   has 0 or inf soln

       when r==n, columns are independent

basis:  vectors are linearly independent and vectors span the space.

function spaces:
               y^n = 0   is solved by linear function y = c*x + d
               y"  = -y  is solved by                 y = c*sin(x) + d*cos(x)
               y"  =  y  is solved by                 y = c*exp(x) + d*exp(-x)

orthogonal:  dot product is zero

orthonormal: orthogonal and normalized so that Q^T * Q = I

nullspace: where Ax=0 in reduced echelon, that is, the free variable rows.  there are n-r such rows.
           m is numRows which is num eqns and n is numCols which is num vars

eigenvector: Almost all vectors change direction when they are multiplied by A. 
             Certain exceptional vectors x are in the same direction as Ax. 
             Those are the “eigenvectors”. 
             Multiply an eigenvector by A, and the vector Ax is a number λ times the original x.
             (G. Strang)
             The basic equation is Ax = λx. The number λ is an eigenvalue of A.

eigenvalue is the lambda in A * x = lambda * b (or, A * x = lambda * x)

steady state: for first eigenvalue lambda = 1 , P*x=x.  e.g. Markov matrices

nullspace entries: for eigenvalue lambda = 0 , P*x=0*x.  e.g. 

decaying mode: for eigenvalue lambda < 1 and >  0, P*x=lambda*x.

positive definite:  a symmetric n X n real matrix M is said to be positive definite 
   if the scalar z^TMz is strictly positive for every non-zero column vector z of 
   n real numbers

positive semi-definite: same as positive definite, except that z^TMz must be positive or zero 

---------------------------------------------------
quick notes from Strang about solving eigenvectors:
  start by solving Ax = λx
    move λx to the left side: (A − λI)x = 0 
    *then, eigenvectors make up the nullspace of A − λI. 
     When we know an eigenvalue λ, we find an eigenvector by solving (A − λI)x = 0.
       - if has a nonzero solution, then A isn't invertible, so determinant of A − λI must be zero.
         det(A − λI) is “characteristic polynomial” 
         degree of det(A − λI) is the dimension of A (degree n for A being nxn).
    *factor det(A − λI) to find the λ's, that is, find the roots
    *use back-substitution For each eigenvalue λ, solve (A − λI)x = 0
    
    NOTE: 
     For symmetric matrices:
       The product of the n eigenvalues equals the determinant.
       The sum of the n eigenvalues equals the sum of the n diagonal entries (i.e. the trace of A).

    The eigenvalues of A2 and A^−1 are λ^2 and λ^−1 , with the same eigenvectors
    Projections P, reflections R, 90◦ rotations Q have special eigenvalues 1, 0, −1, i, −i.
    Singular matrices have λ = 0. Triangular matrices have λ’s on their diagonal.

    NOTE:
     For any A, can estimate the eigenvalue for a row, based upon the sum of the row excluding the diagonal
     and that the eigenvalue is actually "near" that sum.
        for i : |aii − λ| ≤ (sum of absolute values of entries in that row, excluding the the diagonal)

        Example:  A = | a b |  First circle:  | lambda - a | <= |b| 
                      | c d |  Second circle: | lambda - d | <= |c| 

---------------------------------------------------
quick notes from Strang 
  6 Great Theorems of Linear Algebra
    Dimension Theorem: All bases for a vector space have the same number of vectors.
    Counting Theorem: Dimension of column space + dimension of nullspace = number of columns.
    Rank Theorem: Dimension of column space = dimension of row space. This is the rank.
    Fundamental Theorem: The row space and nullspace of A are orthogonal complements in R^n
    SVD: There are orthonormal bases (v’s and u’s for the row and column spaces) so that A*v_i = σ_i*u_i.
    Spectral Theorem: If A^T = A there are orthonormal q’s so that 
      A*q_i = λ_i*q_i and A = QΛQ^T

=============
multiplication:

    A*B = C takes O(n^2.37) operations

Randomized algorithm:
  -- choose random vector:  r_vec = (r1, r2, ..., rn) from the set {0,1}^n
  -- compute B*r_vec
  -- compute A*(B*r_vec)
  -- compute C*r_vec
  -- if A*(B*r_vec) != C*r_vec return A*B != C, else return A*B = C
  Theorem:  if A*B != C and r_vec is chosen uniformly from {0, 1}^n
            then Probability(A*B*r_vec == C*r_vec) <= 0.5

rotation:
   apply to any d-by-d matrix D whose transpose is its inverse (== it is orthogonal)
   and the determinant is 1
             ( cos theta   sin theta )
         R = ( -sin theta  cos theta )
scale:
         s = ( s1   0 )
             (  0  s2 )
translate:
         subtract mu from each row

can use them for one form of data normalization as follows:
   X = n x d matrix of n data points in d-dimensions
   -- create mean vector by averaging each column.
   -- change center to zero by using translation.
   -- then rotate the data so that the variance is aligned with the coordinate axes.
      --  the later is part of principal component analysis
   -- then scale the data to unit variance along each coordinate axis

--------------------------------------------------
from ""?

tensor: 
  in matrix terms, a tensor is an array of numbers with more than 2 dimensions.
A tensor with m distinct dimensions or modes is called an m-way tensor or a tensor of order m.

tensor fiber: 
  A generalization of matrix rows and columns to a higher order case
is called a fiber. Fiber represents a sequence of elements along a fixed mode when all
but one indices are fixed.
  A mode-1 fiber of a tensor is equivalent to a matrix column, 
  a mode-2 fiber of a tensor corresponds to a matrix row. 
  a mode-3 fiber in a tensor is also called a tube.

Tensor slices: 
  Another important concept is a tensor slice. Slices can be obtained
by fixing all but two indices in a tensor, thus forming a two-dimensional array, i.e.
matrix. In a third order tensor there could be 3 types of slices: horizontal, lateral, and
frontal, which are denoted as A_i::, A_:j:, A_::k
respectively

Matricization: 
  Matricization is a key term in tensor factorization techniques. This
is a procedure of reshaping a tensor into a matrix. Sometimes it is also called unfolding
or flattening.

Diagonal tensors: 
  Another helpful concept is a diagonal tensor. Tensor A ∈ R^I1×I2×···×Im 
is called diagonal when a_i1i2...im != 0 only if i1 = i2 = ... = im. 
This concept helps to build a connection between different kinds of tensor decompositions.


