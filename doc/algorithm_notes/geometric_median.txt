One method in cluster determination uses Bayesian Inference.
   One version of Bayesian Inference uses the median posterior.
      the median posterior uses the geometric median.

calculating the geometric median:
   the weighted geometric-median is a.k.a. L1-median (though it uses the euclidean distance).
   a.k.a. spatial median.  a.k.a. L1-estimator.
   definition: the point which minimizes the sum of the euclidean distance of that 
     point to all other points in the set.
     the sum is a convex function (i.e. local search will work).
     Unfortunately, no algorithms are closed form, that is no algorithms have a
     finite number of computational operations.
     The geometric median is a rotation and translation invariant estimator that 
     achieves the optimal breakdown point of 0.5, i.e. it is a good estimator
     even when up to half of the input data is arbitrarily corrupted.
     (https://dl.acm.org/doi/pdf/10.1145/2897518.2897647)

   NOTE: for nDimensions == nPoints, is the centroid always a geometric-median?

   simple unit tests,
     these from https://www.geeksforgeeks.org/geometric-median/
     -- Input: (1, 1), (3, 3)
        Output: Geometric Median = (1,1) or (2, 2) or (3,3) with minimum distance = 2.82843
     -- Input: (0, 0), (0, 0), (0, 12)
        Output: Geometric Median = (0, 0) with minimum distance = 12
     this from "Noniterative Solution of Some Fermat-Weber Location Problems"
        https://www.researchgate.net/publication/220418355_Noniterative_Solution_of_Some_Fermat-Weber_Location_Problems
     -- Input: (-20, 48), (-20, -48), (20, 0), (59, 0)
        Output: Geometric Median = (20, 0) with minimum distance = 163.964
           NOTE: start with (44, 0) to test for a Weiszfeld problem w/ demand points
           and use the same test for the weighted implementation to show that the
           min is found (the weights can be each 1./nPoints).

   and example evaluations for f and the derivative w.r.t. each dimension:
   for nDimensions = 2:
   f = summation_i=1_n( || X - obs_i || )/n
       where || X - obs_i ||_2 is ( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2)
   df/dX_0 = d/dx( (1/n)*summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2) ))
           = (1/n)*(1/2)*2*(X_0-obs_i_0)*(1) 
              / summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2) )
           = (1./n)*(X_0-obs_i_0) / summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2) )
   df/dX_1 = (1./n)*(X_1-obs_i_1) / summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2) )

   d/dX_0 of df/dX_0 = (1./n) * (1) * (-1/2)*summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(-3/2) )
                       *2*(X_0-obs_i_0)*(1)
                     = (-1./n)*(X_0-obs_i_0) / summation_i=1_n( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(3/2) )
   d/dX_1 of df/dX_0 = 0
   Hessian d/dX of d/dX where p is nDimensions and I is the identity matrix of size pxp:
                            ( (    I_p      )   ( (X - obs_i)*(X - obs_i)^T )
           = summation_i=1_n( (-------------) - ( --------------------------)
                            ( (||X - obs_i||)   (      ||X - obs_i||^3      )

     ...
   For the weighted geometric-median, there are 2 papers will use (combined with an improved Newton's method):
     The Vardi-Zhang 2000 algorithm:
       "The multivariate L1-median and associated data depth", Yehuda Vardi† and Cun-Hui Zhang
       https://www.pnas.org/content/pnas/97/4/1423.full.pdf
     This review which compares several algorithms, including Vardi-Zhang 2000:
       "A comparison of algorithms for the multivariate L1-median",
       Heinrich Fritz, Peter Filzmoser, Christophe Croux
       https://feb.kuleuven.be/public/u0017833/PDF-FILES/l1medianR2.pdf
       (eqns (7)-(11) are Vardi-Zhang)
     
     let X be the geometric-median
     let n be the number of observations obs_i
     let M be the L1-median:
        M is a function of observed data obs_i and multiplicites eta_i
           where eta_i are used to make weights.  
        M = argmin X of C(X)
           where C(X) = summation_i_1_to_n( eta_i*d_i(X) )
           where d_i is the euclidean distance between a point obs_i and X in all dimensions.
     ==> X=M iff T(X)=X iff r(X)<=eta(X)􏰌􏰅 
           where eta(X) = eta_i if X=obs_i, i.e. geometric-median is a point in the set obs
                        else eta(X) = 0;
           where r(X) = ||R̃(X)||
           where R̃(X) = summation_over_obs_except_X( eta_i*(obs_i-X)/||obs_i-X|| )
           where T(X) = (1-(eta(X)/r(X))) * T̃(X) + X*math.min(1, (eta(X)/r(X)))
             where 0/0 = 0 in the computation of eta(X)/r(X),
           where T̃(X) = summation_over_obs_except_X( eta_i*(obs_i)/||obs_i-X|| )
                         / summation_over_obs_except_X( eta_i/||obs_i-X|| )
                        (and was derived by setting derivatives to zero)
         NOTE: after iterative algorithm solves M, the test
               X=M iff T(X)=X iff r(X)<=eta(X) is performed.
-----
   can calculate geometric median (a.k.a. euclidean median) many different ways:
      0) for a nearly linear time implementation, looking at this and it's thesis:
         https://dl.acm.org/doi/pdf/10.1145/2897518.2897647
         ** https://www.cs.cmu.edu/~glmiller/Publications/Papers/PachockiPHD.pdf
         This is an "interior point method" so belongs in section 3) below as a
         convex optimization algorithm.  
         (see page 3, Sect 1.2)
      1) least squares
         minimize  ||Ax - b||^2_2
         for A being size kXn, runtime complexity is k*n^2
              SVD on x = arg min||Ax - b||^2  
                   where A is n × k and || · || is the standard vector 2-norm (Euclidian length).
              Let A = USV^T denote the SVD of A
              optimal solution is ⃗x = (V RU^T )⃗b where R is the pseudo-inverse of S 
                 (The the jth entry on the diagonal of R is rj=1/sj if sj ̸=0,and rj=0 if sj=0)
              Note that for large matrices, ⃗x = V (R(U^T⃗b)) is much 
                 more efficient to compute. 
              Note the solution matrix used above, namely VRU^T, equals the pseudo-inverse A.
           NOTE: to solve for b, need an iterative method for retrying values of b.
           frequently referenced is Weiszfeld and the Fermat-Weber problem. see 3) below.
      2) linear programming... similar runtime for this
      3) any convex optimization algorithm.
         computation time (roughly) proportional to max{n^3, m*n^2, F}, 
         where F is cost of evaluating fi’s and their first and second derivatives
         -- LBFGS is good for high-dimensional data. starting point being the centroid?
            The linear time algorithm in 0) uses the Hessian rather than the gradient.
            The appeal to implementing it is not only its speed for high dimensions,
            but that the authors prove convergence for each method in the pseudocode.
             
         -- iteratively reweighted least squares (IRLS)
            given observed points Z_I=(x_i, y_i) also noted as obs_{dimension_pointNumber} below.
            and wanting to solve for X=(x_median, y_median)
            let weight w_i = 1/( || X-z_i || )
            note that the geometric median usually uses equal weights w_i=1/n

            f = summation_i=1_n( w_i * || X - obs_i || )/n
                where || X - obs_i ||_2 is ( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 )^(1/2)
            df/dX_0 = summation_i=1_n( w_i * (X_0-obs_i_0) / ( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 +...)^(1/2)
            df/dX_1 = summation_i=1_n( w_i * (X_1-obs_i_1) / ( (X_0-obs_i_0)^2 + (X_1-obs_i_1)^2 +...)^(1/2)
                 find where df/dX_0 = df/dX_1 = 0
                    X_0 = summation_i=1_n( w_i*obs_i_0 / || X - obs_i ||_2) /
                             summation_i=1_n( w_i/|| X - obs_i ||_2)
                    X_1 = summation_i=1_n( w_i*obs_i_1 / || X - obs_i ||_2) /
                             summation_i=1_n( w_i/|| X - obs_i ||_2)
                 but that is not closed form

                 so Weiszfeld suggested an iterative closed form, similar to steepest descent method.
                 the right hand side are populated using X_{t}
                    X_0_(t+1) = (summation_i=1_n( w_i*obs_i_0 / || X - obs_i ||_2) ) /
                                 (summation_i=1_n( w_i / || X - obs_i ||_2) ) 
                    X_1_(t+1) = (summation_i=1_n( w_i*obs_i_1 / || X - obs_i ||_2) ) /
                                 (summation_i=1_n( w_i / || X - obs_i ||_2) ) 

            iterated updates of estimates of X in time steps t:
              X_(t+1) = summation_i=1_n( w_t*z_i ) / summation_i=1_n( w_t^2 )

            for L1 version given by Ostresh, 1978, he proves converge for 0 <= alpha <= 2 and::
               X_0_(t+1) = X_0_t - alpha * (summation_i=1_n( w_i*obs_i_0 / || X - obs_i ||_2) ) /
                                              (summation_i=1_n( w_i / || X - obs_i ||_2) )
               X_1_(t+1) = ...

            the gradient of the Riemannian sum-of-distances function is given by
               grad( f(X) ) = -summation_i=1_n( w_i*Log(X_i) / (X-z_i))

               using a steepest descent iteration with step size alpha:
                   X_(t+1) = Exp_(X_t)( alpha * v_t)
               where 
                   v_t = summation_i=1_n( w_i*Log(X_i) / (X-z_i)) /
                           summation_i=1_n( w_i / (X-z_i))
               authors found alpha=1 leads to convergence

          *IRLS, can use the Weiszfeld algorithm for geometric median:
             Two steps:
               (1) w_i = 1/[max(|| X-z_i ||_2, delta)]
               (2) X = (summation_i=1_n(w_i*z_i))/(summation_i=1_n(w_i))
                           where (2) is derived from setting the deriv to zero for:
                                     X = minimum X in (summation_i=1_n(w_i*(z_i-X_i)^2))
             And a strategy for optimization:
                fix X{y} and minimize f with respect to X{x}, 
                fix X{x} and minimize f with respect to X{y}, 
                and repeat:
                  for i ← 1, 2, . . .
                     X{y}_i+1 = min_(X{y}) f(X{x}_i+1, X{y}) . Optimize X{y} with X{x} fixed
                     X{x}_i+1 = min_(X{x}) f(X, X{y}_i) . Optimize X{x} with X{y} fixed

         *ADMM is an augmented Lagrangian method which are used in constrained optimization problems.
            they add a penalty term to the objective and another term to mimic the Lagrange multiplier.
            alternating direction method of multipliers (ADMM) is a variant of the augmented Lagrangian 
            scheme that uses partial updates for the dual variables.
               minimize summation_i=1_n( ||X-z_i||_2 )
               see page 218 of book "Numerical Algorithms" by Solomon 

-------
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2735114/

These algorithms do not generalize beyond Euclidean spaces. A more general iterative algorithm due to Weiszfeld (1937) and later improved by Kuhn and Kuenne (1962) and Ostresh (1978) converges to the optimal solution in Euclidean spaces (Kuhn, 1973), and was subsequently generalized to Banach spaces by Eckhardt (1980).

-----
For real numbers, the arithmetic mean is a Fréchet mean, using the usual Euclidean distance as the distance function. The median is also a Fréchet mean, using the square root of the Euclidean distance, i.e. the taxicab distance.

