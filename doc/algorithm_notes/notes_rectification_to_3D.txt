These are notes for making 3D point clouds from stereo images
summarized from browsing papers in the literature and reviews
and tutorials.
   Rectification is outlined briefly, then the making of disparity maps,
   followed by the visualization of the 3-D point clouds.
========================================================================

--------------
Rectification:
--------------
Once a set of point correspondances between images have been derived,
and the stereo projection transformation solved from those, the 
transformation parameters are used to create rectified left and right 
images where the epipolar lines are parallel to the horizontal axis.
This places both images in the same reference frame.

   There are 3 algorithms for rectification:
      -- planar (Fusiello et al. 2000;  Trucco and Verri 1998):
      -- cylindrical rectification (Oram 2001):
      -- polar rectification (Pollefeys et al. 1999):
   But there are also methods which use the epipolar geometry to rectify
   the images and these can be used if there are enough correspondence

   If the transformation matrix is still normalized, one can use the
   extrinsic parameters rotation and translation:

       Hartley 1991, "Theory and practice of projective rectification"
       Hartley 1991 offers a method of rectifying pairs of images by making
       matched epipolar projections, that is projections in which the epipolar
       lines run parallel with the x-axis and the epipoles are mapped to infinity.
       The chosen projective transform is as close to rigid as possible 
       (that is, rotation and translation only).
           The transformation       | 1   0   0 |
                                G = | 0   1   0 |
                                    |-1/f 0   1 |
            transforms the epipole (f, 0, 1)^T  to the point at infinity (f, 0, 0)^T.
            A point (u, v, 1)^T is mapped by G to the point (u, v, 1 − u/f)^T. 
            If |u/f| < 1 then we may write (u, v, 1 − u/f) = (u(1 + u/f + ...), v(1 + u/f + ...),1)^T

            The Jacobian is
                ∂(u_transformed, v_transformed)     ( 1 + 2*u/f      0   )
                -------------------------------  =  (   v/f      1 + u/f )
                            ∂(u, v)

            One wants to avoid a non-quasi-affine projectivity.

            The transformation H' = G*R*T where T is a translation taking a point u0 to
            the origin and R is a rotation about the origin which puts the epipole
            point p at (f, 0, 1)^T on the x-axis.  G takes that point to infinity.
            The composite mapping is to first order a rigid transformation in the 
            neighbourhood of u0.

            Then the matching projective transformation H that minimizes the least-squares
            distance  summation_i of d(H*u_i, H'*u'_i)
            knowing H' and M, the matched points u'_i <--> u_i
            from the first image, vector u_i = (u_i, v_i, 1), so need to minimize
                summation_i of (a*u_i + b*v_i + c-u'_i^2)^2
                can find a, b, and c through linear least squares

                      then          | a   b   c |
                                A = | 0   1   0 |
                                    | 0   0   1 |

		      then H = (I + H'*p'*a^T)*H'*M

            Then resample the first image according to the projective 
            transformation H and the second image according to the 
            projective transformation H'.

       Other methods for rectifying images are in:
           Gluckman and Nayar 2001, "Rectifying transformations that minimize resampling effects"
           Nozick 2011, "Multiple view image rectification"

      Note that the rectified images now are aligned by y coordinate too, that is a matching
      point in one image has the same y coordinate in the other image, though the
      x will differ by disparity.
  
-------------------------
Making of Disparity Maps:
-------------------------

Given rectified stereo images, one can learn the relative distance of
objects in the left or right image within distance limits dependening 
on camera details.

When viewing a 3D point through 2 cameras, if the cameras are not
pointing towards one another, a point in the image of the camera on the right
will be lower horizontally than the image of the point in the left camera.
This disparity is also inversely proportional to the depth of the point.

stereo vision of the rectified images:

                                   #P(x,y,z)
                                 . | .
                               .   |   .
                             .     |     .
                           .       |       .
               <-x_L->   .         |         .  <-x_R->
  image plane |-------# P_L        |       P_R #-------| image plane
                   .               |              .    .
                .                  |                 . f=focal length
              @----------------------------------------@
          opt center          b=Baseline           opt center
            left                                     right
            camera                                   camera

    For each camera, left and right, the focal length f is the perpendicular
    distance from the optical center to the image plane,

    b is the distance between the right and left cameras.

    P is a point in the (x, y, z) reference frame seen as P_L and P_R in the left
    and right images, respectively.
    x_L and x_R are the x coordinates of P_L and P_R.
    Using geometry (similar triangles and parallax), and camera details, one can 
    derive the depth of the point P and know that value relative to the
    camera.  d is that distance for P and is called the disparity for that point.
    
    If the cameras are pinhole cameras, can use similar triangles to
    write
              b * f       b * f
       z = ----------- = -------
            x_L - x_R       d

    Note that the camera should be calibrated so that the objects of interest are
    within a range of distance from the focal plane.
    -------------------------
    calibration of the camera
    -------------------------
       From "3D Video: From Capture to Diffusion" by 
           Laurent Lucas; Celine Loscos; Yannick Remion

       If no camera calibration, self-calibration:
           (Pollefeys et al. 1999; Pollefeys and Van Gool 2002)

       For camera calibration, calculate the projection matrix, then split it
       into sub-matrices to determine focal, position, and direction parameters.

       intrinsic parameters, representing a camera's lens:
               | f_x   s   u_x |
           K = |  0   f_y  u_y |
               |  0    0    1  |

           where f_x and f_y are the focal distance of the camera expressed in 
               pixel units
           s is the non-orthogonality of the sensor's photosite grid, so this
               is usually 0 because the photosite are square. 
           u_x, u_y are coordinates in pixels of the principal point
               (where the optical axis intersects the image).  this is usually
               the image center, but may be offset, especially if zoom is used

       extrinsic parameters represent the position and direction of the camera.

       P_3x4 = K_3x3 * [R_3x3 | -R_3x3*c_3x1]

       where [A | B] is the matrix concatenation of A and B
       and R is the rotation matrix for the camera's direction in the scene's 
           reference frame,
       and c is the camera's position of the camera's center of projection in
           the scene's reference frame.  c is the vector [c_x, c_y, c_z, c_w]^T

       P = K * [R | -R*c]
       P = [K*R | -K*R*c] = [p1 p2 p3 p4]
       let M = [p1 p2 p3]
       then [K*R | -K*R*c] = [M | p4]

           once K and R are solved by QR decomposition of M, 
           the position of the camera is obtained as:
           c_x = det([p2 p3 p4])
           c_y = det([p1 p3 p4])
           c_z = det([p1 p2 p4])
           c_w = det([p1 p2 p3])

           and c = [c_x/c_w, c_y/c_w, c_z/c_w)^T

       Calibration requires multiple pairs of images of a test pattern viewed 
       from different angles.  Need the entire pattern visible in each image 
       without occlusion.   A chess or checker board pattern is common as rulers
       projected to 2 dimensions from several angles and covering vanishing points.

       The test pattern should be at approx the distance of the objects of interest
       to photograph after calibration.   The measurements of the the test pattern,
       such as a checkerboard, should be measured very precisely.

       correspondences in the images can be used to solve for the fundamental
       matrix.   (left image points)^T * (fundamental matrix) * (right image points) = 0
 
       then the essential matrix can be derived from the fundamental matrix or 
       essential solved for from beginning?

-------------------------------------
Making of Disparity Maps (continued):
-------------------------------------

    Note that energy functions are also known as objective functions, loss functions,
    cost functions, and utility functions.  the goal is always to find the minimum
    of the energy function.   current minimization methods are 
    sets of partitions that are higher order and irregular,
    linear programming relaxation (LPR) inference,
    variations of message passing,
    move-making algorithms in graph cuts.

    -- The distance in pixels between corresponding points in the rectified images
       is called disparity. The disparity is used for 3-D reconstruction, because
       it is proportional to the distance between the cameras and the 3-D world point.
       That is, objects w/ larger disparity are closer to the camera.

       from http://www.cs.cornell.edu/~hema/rgbd-workshop-2014/papers/Alhwarin.pdf

    Matching homologous pixels between left and right images is called stereoscopic
    matching.  The otherwise similar pixels present in both images may vary due to
    the lighting, or occlusion, or color variations at the edges of objects due to
    a difference in background objects due to a change in perspective, or due to
    a difference in the size of a patch of pixels in one image and the other due to
    being further or closer to the optical center, or ambiguity due to a pixel
    being in a largely featureless region, or ambiguity due to repetitive patterns.


    There are many algorithms for determining sparse or dense disparity maps.
    Here, will summarize the simplest of methods, the fastest, and then the most 
    accurate but slower algorithms.
        Useful resources for reading were:
           http://en.wikipedia.org/wiki/Binocular_disparity
           http://chrisjmccormick.wordpress.com/2014/01/10/stereo-vision-tutorial-part-i/
           http://vision.middlebury.edu/mview/seitz_mview_cvpr06.pdf
           http://vision.middlebury.edu/stereo/taxonomy-IJCV.pdf
           http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.7321&rep=rep1&type=pdf
           http://www.cvlibs.net/publications/Geiger2010ACCV.pdf
           http://cmp.felk.cvut.cz/~cechj/GCS/
           http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf 

    -- The SIMPLEST algorithm for making a disparity map would be to compare patches
       of a block size in the left image to patches in the right image using a brute force
       comparison of each pixel centered patch in the left image and a range search against
       each pixel centered patch in the right image for the scanned row.
          -- One can improve upon that by retaining the pixel centered patch calculations
             from the right image and reusing them for each left patch comparison.
          -- One can improve upon that by limiting the comparisons to only those 
             in the forward scan directions once one establishes the first best 
             matching patches.
          -- One can limit the number of rows needed in a comparison presumably in a few ways
             including possibly during camera system set up
             (in other words, creating a rectilinear stereo rig).
          -- One could improve upon that by storing the previous left pixel patch calculations
             by columns and rows in a manner to update the patch of comparison
             (more memoization)
          -- One could imagine re-using the points of correspondences used to derive the 
             transformations in the first place as anchors of smaller error disparities
             that be used to define boundaries from which one could further match 
             left and right within the bounds.  Note, one of the fastest current 
             algorithms further written about in the next section, uses different 
             points in such a manner to create a Delanauy Triangulation of 
             bounds to search within to populate the remaining disparity map points.

          The patch block comparisons tend to use a greyscale intensity for the pixel (0-255)
          or color space indexing such as CIE x and y. (The CIE xy color space are independent of
          intensity, but most objects have a color dependent reflection (see index of refraction 
          for materials or albedo for more information).  But if the illumination
          is largely grey scattering of light, that is Mie scattering, CIE xy could be very useful
          for segmentation of related pixels.)
          (Note, have implemented a color segmentation based upon CIE XY space now.)

          The 3 different patterns used for comparing the blocks of pixels are for the simplest:
              -- Sum of absolute differences:
                 \sum{\sum{ | L(r,c) - R(r,c-d) | }}
              -- Sum of squared differences:
                 \sum{\sum{ (L(r,c) - R(r,c-d))^2 }}
              -- Normalized correlation:
                 \sum{\sum{ L(r,c) dot R(r,c-d) }}
                 --------------------------------------------------------------
                 \sqrt{(\sum{\sum{ L(r,c)^2 }}) dot (\sum{\sum{ R(r,c-d)^2 }})} 
              
              where r and c are row and column of the  pixel within the left image (L)
              and right image (R).
              d is the disparity.

          The disparity for a pixel is then where those comparison functions are minimized
          and that means that the number of scanned pixels, that is the offset from left and
          right, has to be large enough to find the real disparity for a left pixel.

          Instead of using the minimum, one can interpolate between the smallest values 
          to find the peak of those as a parabola if only using a 1-D scan comparison:
            https://chrisjmccormick.wordpress.com/2014/01/10/stereo-vision-tutorial-part-i/
            d_estimated = d_2 - 0.5*(C_3 - C_1)/(C_1 - 2*C_2 + c_3) 
               where _2 is for the closest matching block and _1 and _3 are those to the left
               and right of it, respectively.

          Note also, that some algorithms use segmentation of either disparity or of color in
          the first step to interpret the disparities as surfaces.  Segmentation, combined
          with growing connected similar regions are sometimes used.

          Algorithms similar to the above tend to have the longest runtimes unless steps
          are taken to make the comparisons smaller in number and size.

          Note also, there is dynamic programming to find the global minimum for independent 
          scanlines in polynomial time, but the solution has streaks in it.
              see http://vision.middlebury.edu/stereo/taxonomy-IJCV.pdf
              the basic outline for sparse dynamic solution (edges only) is in
              Ohta and Kanade 1983 "Stereo by Intra- and Inter- scanline Search Using
              Dynamic Programming"
                  from a rectified pair of stereo images, a search is made for corresponding
                  points in the left and right for the same scane lines and using information
                  from the already found correspondences as progress is made.
                  two simultaneous searches proceed dynamically:
                    -- the sum of the cost of matching surface is the intra-line search 
                       for correspondences between left and right for all scan lines
                    -- the contraint of the searches across the 2D plane is given
                       by connected edges.
                  some characteristics:  
                    - a match of left pixel to right pixel on one scanline should match on
                      other scanlines too and the same for a "not match".
                    - when searching for the correspondence of 2 edges, the left image edges
                      for each scanline must already be processed.
                      edges left-to-right on each scanline:  
                           left is [0:N] and right is [0:M]
                      and scanning by block bounds, starting at smaller indices:
                         The path is a consecutive set of straight line segments from 
                         node (0,0) to node (M,N) on a 2D array [0:M,0:N]. 
                      then the order of matched edges is preserved.
                      a path has a vertex at node=(m,n) when right edge m and left edge n 
                      are matched.
                      D(m,k) is the minimal cost of the partial path from node k to node m. 
                      D(m) is the cost of the optimal path from the origin (0,0) to node m.

    -- The FASTEST algorithms for making a disparity map want to be usable in realtime systems.
        The algorithm with the fastest runtime with a high accuracy seems to be:
            Geiger, Roser, and Urtasun 2010, "Efficient Large-Scale Stereo Matching"
            http://www.cvlibs.net/publications/Geiger2010ACCV.pdf
            The authors offer the code under a GPL license, but only for non-commercial systems.
                http://www.cvlibs.net/download.php?file=libelas.zip
            The algorithm uses a Delanay triangulation of well determined disparities called
            "support points" as boundary points to more efficiently search the disparity space.
            Image boundary "support points" are added with values assigned from their neighbors.
            It assumes the left and right images are already aligned vertically so that
            scaning is only needed along a row.
            They discard ambiguous matches if the 2nd best match is within a threshold 0.9 
            (factor?  difference?) of the first.  They discard matches which look spurious
            by being very different from all of their surrounding support points.
            For each pixel, the support points within a 20x20 region are used to calculate
            a mean which is used to limit the disparity range of the pixel search to less
            than 3 * sigma from the mean (where sigma is presumably the standard deviation
            of the mean).   For those pixels, feature vectors are created using Sobel operators
            in blocks of size 5x5.  the feature vectors are used to generate probabilities of
            matches between the compared left and right image pixels.
            Their generative model: (1) Given support points and left image pixels, draw
            a disparity from the prior probability function which is eqn 2 in the paper.  
            (2) Given the generated disparity and left pixel, draw the right pixel from the 
            constrained Laplacian distribution which uses the feature vectors.  This is eqn
            4 in the paper.  The "draws" are repeated 100 times for each pixel.
            For each left pixel set of generative disparities and right pixels, the estimated
            final disparity for that pixel is found by minimizing an eqn that combines use of
            the log of the prior and constrained laplacian probabilities.
            (Note: the 100 throws might be purely for the paper figure and the impl in code
            might be different?) 
            Note also that the minimization for each pixel's matching r sets of data can be
            completed faster on processors that can process SIMD instructions provided by
            the authors.

            Are there some images in which the calculated "support points" are not 
            a small enough mesh to handle image characteristics such as shadows?
            just noticing the disparity maps for this and SGM and a proposed local plane 
            sweep algorithm in other online slides...

        -- Note that in their paper, the next best performing for the runtime is from the paper
               Jan Cech 2007, "Growing Correspondence Seeds.  A Fast Stereo Matching of 
               Large Images" 
               Their code is offered w/o commercial restrictions, but is in matlab.
               http://cmp.felk.cvut.cz/~cechj/GCS/

    -- Among the most ACCURATE algorithms considering a compromise w/ runtime are the 
            Graph-Cut algorithms.
        One of the latest implementations:
            Boykov and Kolmogorov 2004, 
                "An Experimental Comparison of Min-Cut/Max-Flow Algorithms for
                Energy Minimization in Computer Vision"
                http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf
                http://pub.ist.ac.at/~vnk/software/maxflow-v3.03.src.zip
        and before that was V. Kolmogorov and R. Zabih, 2001
            "Computing visual correspondence with occlusions using graph cuts."
            http://pub.ist.ac.at/~vnk/software.html, match-v3.4.src.tar.gz

        Compare where possible to Semi-Global Matching.
           There is a CUDA only Semi-Global Matching algorithm: 
               Michael et al. 2013., "Real-time Stereo Vision: Optimizing Semi-Global Matching" 
               http://www.ini.rub.de/data/documents/michaeletal_improvesgm_iv2013.pdf
           There is Hirschmuller 2005
               but there are mentions of using it in textureless patches or where dense 
               disparity maps are needed.

        "fast plane sweeping stereo"?
            http://research.microsoft.com/pubs/232423/sinhaLPS_CVPR2014.pdf

    In navigation systems with very high resolution images, wouldn't an early 
         down sampled image pair solution much faster than 1 second be useful 
         before a more accurate dense disparty map that might take 10 seconds?
         (Meaning, calculate both, but the fast runtime less accurate is always
         available and followed by the more accurate later.)
         (see the Disney mansion runtimes in 
         http://www.imgfsr.com/CVPR2014IRW/I1.pdf
         http://research.microsoft.com/pubs/232423/sinhaLPS_CVPR2014.pdf)
         There would be large errors per down sized image pixel that could be
         used to estimate a possible range of true distances around the quick 
         determination for an early look around.


  Test data:
      -- Middlebury stereo vision datasets and/or Merton College I
      -- 2 all black images?  2 all white images?
      -- 2 identical images
      -- simple image with a single area of texture
      -- a stereo vision image pair containing a repeated pattern

-----------------
3-D point clouds:
-----------------
    The reprojection from the disparity map to a 3D point cloud is
        Q * [x y d 1]^T = [X Y Z W]^T
        that is the 2D point is reprojected to 3D space as [X/W Y/W Z/W]^T

    in progress...

    from Pollefeys et al. 2001, "Structure and motion from image sequences"
        http://www.inf.ethz.ch/personal/pomarc/pubs/PollefeysCO3DM01.pdf
        overlay a 2D triangular mesh on one of the images, then build 
        a corresponding 3D mesh by placing the vertices of the triangles 
        in 3D space by their depth map or disparity map.
        then use the image as a texture map.

    or:
        grow connected disparity regions that are the same within a tolerance
        and assign that region as a polygon?  

        consider reduced polygon resolution for further disparities or depths?
        (see LDOS below)
    
    or:
        triangular mesh is obtained from a voxel representation via marching 
        cubes algorithm, after silhouette-based reconstruction

    Polygonal modeling: 
        pros: flexibility and speed
        3D points connected by line segments to create a polygonal mesh.

    Curve modeling:
        surfaces created using weighted points w/ curves changed by control points.
        splines, patches, geometric primitives, nonuniform rational B-splines (NURBS).

    Digital sculpting:
        Displacement, a.k.a. deformable polygon meshes, is the type used more often.  
            Each vertex stores its modifiable position.
        Volumetric digital sculpting is the type based on Voxels and is useful
            to avoid deformation for regions w/ few polygons. 
        Dynamic tesselation is the type which also uses Voxels, but it uses
            triangulation for surfaces for finer detail capability.
            (mesh algorithms here?)

    http://people.csail.mit.edu/sparis/talks/Paris_06_3D_Reconstruction.pdf

    Thrun et al. 200
        "A Real-Time Algorithm for Mobile Robot Mapping With Applications to
        Multi-Robot and 3D Mapping"
        http://robots.stanford.edu/papers/thrun.map3d.pdf
        and references therein

    surfels:
        http://graphics.ucsd.edu/~matthias/Papers/Surfels.pdf

    terrain:
        mipmapping and tiling
        mesh simplification:  Luebke et al. 2001
            http://www.cs.virginia.edu/~luebke/publications/pdf/cg+a.2001.pdf
            improve runtime performance:
                levels of detail (LOD): distant objects have lower LOD and
                    nearer have higher LOD
                SE (Simplification Envelopes) for manifold models:
                    Rossignac et al. 1999,
                    "Edgebreaker: Connectivity Compression for Triangle
                    Meshes"
                QEM (Quadratic Error Metrics):
                    Taubin and Rossignac 1999,
                    "3D Geometry Compression"
                Progressive Meshes:
                    Deering 1995
                    "Geometry Compression"

    Simultaneous Localization and Mapping:


    Dense 3D reconstruction:
        Ummenhofer and Brox 2012
        http://lmb.informatik.uni-freiburg.de/Publications/2012/UB12/ummenhofer2012Dense3D.pdf

    sfm w/ video:

    WebGL / Three.js:

    Kinect fusion reconstructs surfaces from dense 3D point clouds from
       depth maps:
       http://research.microsoft.com/pubs/155416/kinectfusion-uist-comp.pdf

    Reconstructing geometry using public images from different cameras:
        Frahm et al. 2010, "Building Rome on a Cloudless Day"
        http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.174.8828&rep=rep1&type=pdf
        (see also, the sparse 3D mapping section below).

    http://cronos.rutgers.edu/~meer/TEACHTOO/PAPERS/Szeliskistereodraft.pdf
    surface mesh reconstruction techniques such as the one of Fua and Leclerc (1995) 

    silhouette-based reconstruction:
        Laurentini 1994, "The Visual Hull Concept for Silhouette-Based 
        Image Understanding"
        http://dl.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=628563&preflayout=flat
        The visual hull of a 3-D object is computed using silhouette based algorithms.

    Image based modeling:
      * Furukawa and Ponce 2008 http://www.cse.wustl.edu/~furukawa/papers/pami08a.pdf
        Vogiatzis et al. 2007
        Pons et al 2006

-------------------------------------------------------------
For non-stereo images: sparse 3D reconstruction
-------------------------------------------------------------
    sparse 3D reconstruction (w/ absolute location)
    (Szeliski and Kang 1994)

    Kaminsky et al 2009, "Alignment of 3D Point Clouds to Overhead Images"
    http://grail.cs.washington.edu/pub/papers/kaminsky2009ao.pdf

    Snavely, Seitz, and Szeliski 2007
    "Modeling the World from Internet Photo Collections"
    http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf

    Snavely, Seitz and Szeliski 2007:
       To find features, they use SIFT (Lowe 1999.  To make correspondences 
       between images, they use SIFT and RANSAC and additional steps to 
       remove outliers, discard ambiguities, and discard images with less 
       than 20 matches.

       image connectivity graph: the images are placed into a graph with 
       an edge representing that the 2 images (vertexes) have point 
       correspondences.

       "Structure from Motion" is used to get relative positions of cameras:
           a position of the same point in correspondences in all images is 
           called a track.
           non-linear least squares and bundle adjustment is used to refine
           the solution, incrementally for each camera.
           The pair of images w/ the largest number of matches and both images
           having different homographies is selected.
           RANSAC is again used to retain the best matches and discard outliers
           using a threshold of 0.4% of max(image width, image height).
           The image pair w/ smallest % inliers for the (total?) recovered 
           homography but with >= 100 matches is then selected and the camera 
           parameters are estimated using Nister's 5-point algorithm.   
           The tracks between the two images are triangulated.
           From those, a 2-frame bundle adjustment is performed.
           Then the camera w/ largest number of tracks w/ 3D estimations 
           is selected and it's camera extrinsic parameters are found using 
           DLT (Hartley and Zisserman 2004) inside a RANSAC procedure 
           (w/ same threshold as prev defined).
           DLT returns the upper triangular matrix K (which is the intrinsics)
           and the rotation and translation (part of the extrinsics).   
           K combined with the focal length from the EXIF tags refines the 
           new camera parameter for focal length.
           Then each new camera's individual points are added if the location 
           errors are small and if the maximum separation between the point 
           considered and all possible points too create another triangle 
           which is larger than 2 degrees and also has a small location error.
           After each camera's points are added, bundle adjustment is used again.
           The largest runtime phases are for the pairwise matching and the 
           incremental bundle adjustment.
      
           Then, this consistent model can be aligned with a map, a sattelite
           image, floor plan, etc having a precise geo-reference frame.
           orthographic projections of the 3D points, lines, and camera 
           locations are rendered superimposed on the alignment image.


-------------------------------------------------------------
tangent related subject, object recognition
-------------------------------------------------------------
in progress...

http://www.di.ens.fr/~ponce/mundy.pdf

many papers...
